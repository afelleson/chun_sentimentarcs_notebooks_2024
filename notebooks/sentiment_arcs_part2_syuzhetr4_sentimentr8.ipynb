{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtxyCnlCcAi1"
      },
      "source": [
        "# **Compute Sentiment Using 4 SyuzhetR and 7 SentimentR Models**\n",
        "\n",
        "* https://www.youtube.com/watch?v=U3ByGh8RmSc\n",
        "\n",
        "* https://github.com/ttimbers/intro-to-reticulate\n",
        "\n",
        "[Use R on Google Colab!](https://colab.research.google.com/notebook#create=true&language=r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgvTrI7bevn2"
      },
      "source": [
        "# **[STEP 1] Manual Configuration/Setup**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_-wb8jb6M0h"
      },
      "source": [
        "## (Popups) Connect Google gDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDerblMS6M0h"
      },
      "outputs": [],
      "source": [
        "# [INPUT REQUIRED]: Authorize access to Google gDrive\n",
        "\n",
        "# Connect this Notebook to your permanent Google Drive\n",
        "#   so all generated output is saved to permanent storage there\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "else:\n",
        "  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKEdKhZ_6M0h"
      },
      "source": [
        "## (3 Inputs) Define Directory Tree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n",
        "#              to math the full path to your gDrive subdirectory which should be the \n",
        "#              root directory cloned from the SentimentArcs github repo.\n",
        "\n",
        "# NOTE: Make sure this subdirectory already exists and there are \n",
        "#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n",
        "\n",
        "# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n",
        "#         letter, number and underscores ('_') characters should appear afterwards.\n",
        "#         Make sure your full path after %cd obeys this constraint or errors may appear.\n",
        "\n",
        "# #@markdown **Instructions**\n",
        "\n",
        "# #@markdown Set Directory and Corpus names:\n",
        "# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n",
        "# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n",
        "# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n",
        "# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n",
        "\n",
        "#@markdown <hr>\n",
        "\n",
        "# Step #1: Get full path to SentimentArcs subdir on gDrive\n",
        "# =======\n",
        "#@markdown **Accept default path on gDrive or Enter new one:**\n",
        "\n",
        "Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#@markdown Set this to the project root in your <b>GDrive folder</b>\n",
        "#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n",
        "\n",
        "#@markdown <hr>\n",
        "\n",
        "#@markdown **Which type of texts are you cleaning?** \\\n",
        "\n",
        "Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n",
        "\n",
        "# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n",
        "Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n",
        "\n",
        "\n",
        "Corpus_Number = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "\n",
        "#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n",
        "#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n",
        "#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n",
        "\n",
        "#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n",
        "\n",
        "print('Current Working Directory:')\n",
        "%cd $Path_to_SentimentArcs\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "if Corpus_Type == 'reference':\n",
        "  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n",
        "  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n",
        "else:\n",
        "  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "\n",
        "# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n",
        "# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n",
        "PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n",
        "PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n",
        "\n",
        "# TODO: Clean up\n",
        "# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n",
        "\n",
        "print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n",
        "print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n",
        "print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"
      ],
      "metadata": {
        "id": "9z5ZRX_s6a34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[STEP 2] Automatic Configuration/Setup**"
      ],
      "metadata": {
        "id": "HM4ePEaq6nrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPdbnOjw6ycy"
      },
      "outputs": [],
      "source": [
        "# Add PATH for ./utils subdirectory\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "!python --version\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n",
        "PATH_UTILS\n",
        "\n",
        "sys.path.append(PATH_UTILS)\n",
        "\n",
        "print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n",
        "!ls $PATH_UTILS\n",
        "\n",
        "# More Specific than PATH for searching libraries\n",
        "# !echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvMuohQZ6ycz"
      },
      "outputs": [],
      "source": [
        "# Review Global Variables and set the first few\n",
        "\n",
        "import global_vars as global_vars\n",
        "\n",
        "global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n",
        "global_vars.Corpus_Genre = Corpus_Genre\n",
        "global_vars.Corpus_Type = Corpus_Type\n",
        "global_vars.Corpus_Number = Corpus_Number\n",
        "\n",
        "global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n",
        "global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n",
        "\n",
        "global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n",
        "global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n",
        "\n",
        "dir(global_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeLft8mw7moD"
      },
      "source": [
        "## (each time) Custom Libraries & Define Globals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and clean for each iteration of notebook\n",
        "\n",
        "# dir(global_vars)\n",
        "\n",
        "global_vars.corpus_texts_dt = {}\n",
        "global_vars.corpus_titles_dt = {}"
      ],
      "metadata": {
        "id": "Y2IRi-3z7moE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IU1IHzA7moE"
      },
      "outputs": [],
      "source": [
        "# Import SentimentArcs Utilities to define Directory Structure\n",
        "#   based the Selected Corpus Genre, Type and Number\n",
        "\n",
        "!pwd \n",
        "print('\\n')\n",
        "\n",
        "# from utils import sa_config # .sentiment_arcs_utils\n",
        "from utils import sa_config\n",
        "\n",
        "print('Objects in sa_config()')\n",
        "print(dir(sa_config))\n",
        "print('\\n')\n",
        "\n",
        "# Directory Structure for the Selected Corpus Genre, Type and Number\n",
        "sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWZWepaO7moE"
      },
      "outputs": [],
      "source": [
        "# Call SentimentArcs Utility to define Global Variables\n",
        "\n",
        "sa_config.set_globals()\n",
        "\n",
        "# Verify sample global var set\n",
        "print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n",
        "print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n",
        "print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n",
        "print(f'SLANG_DT: {global_vars.SLANG_DT}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRb36wyH7moE"
      },
      "source": [
        "## Configure Jupyter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjoIK5U_7moE"
      },
      "outputs": [],
      "source": [
        "# Configure Jupyter\n",
        "\n",
        "# To reload modules under development\n",
        "\n",
        "# Option (a)\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Option (b)\n",
        "# import importlib\n",
        "# importlib.reload(functions.readfunctions)\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enable multiple outputs from one code cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from ipywidgets import widgets, interactive\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW6YDyHT7moF"
      },
      "source": [
        "## (each time) Read YAML Configuration for Corpus and Models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUveIcUOzYav"
      },
      "outputs": [],
      "source": [
        "# from utils import sa_config # .sentiment_arcs_utils\n",
        "\n",
        "import yaml\n",
        "\n",
        "from utils import read_yaml\n",
        "\n",
        "print('Objects in read_yaml()')\n",
        "print(dir(read_yaml))\n",
        "print('\\n')\n",
        "\n",
        "# Directory Structure for the Selected Corpus Genre, Type and Number\n",
        "read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n",
        "\n",
        "print('SentimentArcs Model Ensemble ------------------------------\\n')\n",
        "model_titles_ls = global_vars.models_titles_dt.keys()\n",
        "print('\\n'.join(model_titles_ls))\n",
        "\n",
        "\n",
        "print('\\n\\nCorpus Texts ------------------------------\\n')\n",
        "corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n",
        "print('\\n'.join(corpus_titles_ls))\n",
        "\n",
        "\n",
        "print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n",
        "print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n",
        "print('\\n')\n",
        "\n",
        "global_vars.corpus_titles_dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNwIAUc6uDdo"
      },
      "source": [
        "## Install Libraries: R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p05_XBc6uBrK"
      },
      "outputs": [],
      "source": [
        "# !pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83BAG8HPS0-f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U rpy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4zeGVURuJ9r"
      },
      "outputs": [],
      "source": [
        "# Load Jupyter rpy2 Extension  \n",
        "#   enables the %%R magic commands\n",
        "\n",
        "%load_ext rpy2.ipython\n",
        "\n",
        "# %reload_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgF1pBDQuVKY"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "%%capture \n",
        "%%R\n",
        "\n",
        "# Install Syuzhet.R, Sentiment.R and Utility Libraries\n",
        "\n",
        "# NOTE: 1m12s \n",
        "#       1m05s\n",
        "\n",
        "#       1m13s 00:47 @20220406Wed Novels Corpus1 2 Novels\n",
        "\n",
        "install.packages(c('syuzhet', 'sentimentr', 'tidyverse', 'lexicon'))\n",
        "\n",
        "library(syuzhet)\n",
        "library(sentimentr)\n",
        "library(tidyverse)\n",
        "library(lexicon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUmxULxnKqQb"
      },
      "outputs": [],
      "source": [
        "# Load Python libraries to exchange data with R Program Space and read R Datafiles\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQTZyrZzH-XZ"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "# Verify R in Kernel Version\n",
        "# R.version.string\n",
        "\n",
        "# Verfiy R Kernel Environment\n",
        "# Sys.getenv\n",
        "\n",
        "# Verify R Kernel Session Info\n",
        "sessionInfo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If55aLQYsk_K"
      },
      "source": [
        "## Install Libraries: Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNT7dGQsmw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from glob import glob\n",
        "import copy\n",
        "import json # Installed above in YAML Configuration Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1yTaY_9Qod"
      },
      "source": [
        "## Setup Matplotlib Style\n",
        "\n",
        "* https://matplotlib.org/stable/tutorials/introductory/customizing.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1s24O-S9JJX"
      },
      "outputs": [],
      "source": [
        "# Configure Matplotlib\n",
        "\n",
        "# View available styles\n",
        "# plt.style.available\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/config_matplotlib.py'\n",
        "\n",
        "config_matplotlib()\n",
        "\n",
        "print('Matplotlib Configuration ------------------------------')\n",
        "print('\\n  (Uncomment to view)')\n",
        "# plt.rcParams.keys()\n",
        "print('\\n  Edit ./utils/config_matplotlib.py to change')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from cycler import cycler\n",
        "\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']   \n",
        "linestyles = ['-', '--', ':', '-.','-', '--', ':', '-.','-', '--']\n",
        "\n",
        "cycle = plt.cycler(\"color\", colors) + plt.cycler(\"linestyle\", linestyles)\n",
        "\n",
        "# View previous matplotlib configuration\n",
        "print('\\n Old Matplotlib Configurtion Settings:\\n')\n",
        "# plt.rc.show\n",
        "print('\\n\\n')\n",
        "\n",
        "# Update and view new matplotlib configuration\n",
        "print('\\n New Matplotlib Configurtion Settings:\\n')\n",
        "myparams = {'axes.prop_cycle': cycle}\n",
        "plt.rcParams.update(myparams)\n",
        "\n",
        "plt.rcParams[\"axes.titlesize\"] = 16\n",
        "plt.rcParams['figure.figsize'] = 20,10\n",
        "plt.rcParams[\"legend.fontsize\"] = 10\n",
        "plt.rcParams[\"xtick.labelsize\"] = 12\n",
        "plt.rcParams[\"ytick.labelsize\"] = 12\n",
        "plt.rcParams[\"axes.labelsize\"] = 12\n",
        "\n",
        "# Set matplotlib plot figure.figsize\n",
        "\n",
        "new_plt_size = plt.rcParams[\"figure.figsize\"]=(20,10)\n",
        "\n",
        "print(\" New figure size: \",new_plt_size)\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "Oym8UfAa_Gta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPPrZwyIIze"
      },
      "source": [
        "## Setup Seaborn Style"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Seaborn\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/config_seaborn.py'\n",
        "\n",
        "config_seaborn()\n",
        "\n",
        "print('Seaborn Configuration ------------------------------\\n')\n",
        "# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n",
        "\n",
        "\"\"\"\n",
        "# Seaborn: Set Context\n",
        "# sns.set_context(\"notebook\")\n",
        "\n",
        "# Seaborn: Set Theme (Scale of Font)\n",
        "sns.set_theme('paper')  # paper, notebook, talk, poster\n",
        "\n",
        "# Seaborn: Set Style\n",
        "# sns.set_style('ticks') # darkgrid, whitegrid, dark, white, and ticks\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "# sns.set_palette('tab10')\n",
        "# sns.color_palette()\n",
        "\n",
        "# sns.set_palette('tab10')\n",
        "# sns.color_palette()\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "qMECX12r_CNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM3oRY-UOmzX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Configure Seaborn\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/config_seaborn.py'\n",
        "\n",
        "config_seaborn()\n",
        "\n",
        "print('Seaborn Configuration ------------------------------\\n')\n",
        "# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n",
        "# View previous seaborn configuration\n",
        "print('\\n Old Seaborn Configurtion Settings:\\n')\n",
        "sns.axes_style()\n",
        "print('\\n\\n')\n",
        "\n",
        "# Update and View new seaborn configuration\n",
        "print('\\n New Seaborn Configurtion Settings:\\n')\n",
        "# sns.set_style('white')\n",
        "sns.set_context('paper')\n",
        "sns.set_style('white')\n",
        "sns.set_palette('tab10')\n",
        "\n",
        "# Change defaults\n",
        "# sns.set(style='white', context='talk', palette='tab10')\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X229IbToHwa2"
      },
      "source": [
        "## Python Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (each time) Generate Convenient Data Lists"
      ],
      "metadata": {
        "id": "gjmkC1FbAEpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO08GFoGlP3y"
      },
      "outputs": [],
      "source": [
        "# Derive List of Texts in Corpus a)keys and b)full author and titles\n",
        "\n",
        "print('Dictionary: corpus_titles_dt')\n",
        "global_vars.corpus_titles_dt\n",
        "print('\\n')\n",
        "\n",
        "corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n",
        "print(f'\\nCorpus Texts:')\n",
        "for akey in corpus_texts_ls:\n",
        "  print(f'  {akey}')\n",
        "print('\\n')\n",
        "\n",
        "print(f'\\nNatural Corpus Titles:')\n",
        "corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n",
        "for akey in corpus_titles_ls:\n",
        "  print(f'  {akey}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQNlQr4_Ckb1"
      },
      "outputs": [],
      "source": [
        "# Get Model Families of Ensemble\n",
        "\n",
        "from utils.get_model_families import get_ensemble_model_famalies\n",
        "\n",
        "global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n",
        "\n",
        "print('\\nTest: Lexicon Family of Models:')\n",
        "global_vars.models_ensemble_dt['lexicon']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File Functions"
      ],
      "metadata": {
        "id": "B3sXwZOq_-0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "%run -i './utils/file_utils.py'\n",
        "# from utils.file_utils import *\n",
        "\n",
        "# %run -i './utils/file_utils.py'\n",
        "\n",
        "# TODO: Not used? Delete?\n",
        "# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"
      ],
      "metadata": {
        "id": "3JQBWKKcN2Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fys3dkJSB656"
      },
      "source": [
        "# **[STEP 3] Read all Preprocessed Novels**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {Path_to_SentimentArcs}{PATH_TEXT_CLEAN}"
      ],
      "metadata": {
        "id": "esrv4EUjA9Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify cwd and subdir of Cleaned Corpus Texts\n",
        "\n",
        "print('Current Working Directory:')\n",
        "!pwd\n",
        "\n",
        "print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_TEXT_CLEAN}')\n",
        "\n",
        "print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n",
        "!ls -1 {Path_to_SentimentArcs}{PATH_TEXT_CLEAN}"
      ],
      "metadata": {
        "id": "Roq-2Ol8yH5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ClL4-1Gqe7g"
      },
      "outputs": [],
      "source": [
        "# Create a List (preprocessed_ls) of all preprocessed text files\n",
        "\n",
        "# Verify in SentimentArcs Root Directory\n",
        "os.chdir(Path_to_SentimentArcs)\n",
        "\n",
        "try:\n",
        "    preprocessed_ls = glob(f'{PATH_TEXT_CLEAN}/*.csv')\n",
        "    preprocessed_ls = [x.split('/')[-1] for x in preprocessed_ls]\n",
        "    preprocessed_ls = [x.split('.')[0] for x in preprocessed_ls]\n",
        "except IndexError:\n",
        "    raise RuntimeError('No csv file found')\n",
        "\n",
        "print('\\n'.join(preprocessed_ls))\n",
        "print('\\n')\n",
        "print(f'Found {len(preprocessed_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4CqJQY9rNRw"
      },
      "outputs": [],
      "source": [
        "# Read all preprocessed text files into master DataFrame (corpus_dt)\n",
        "\n",
        "corpus_texts_dt = {}\n",
        "\n",
        "for i,atext in enumerate(preprocessed_ls):\n",
        "  print(f'Processing #{i}: {atext}...')\n",
        "  afile_fullpath = f'{PATH_TEXT_CLEAN}/{atext}.csv'\n",
        "  print(f'               {afile_fullpath}')\n",
        "  atext_df = pd.read_csv(afile_fullpath, index_col=[0])\n",
        "  corpus_texts_dt[atext] = atext_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHLt5o78tJyR"
      },
      "outputs": [],
      "source": [
        "# Verify the Text read into master Dictionary of DataFrames\n",
        "\n",
        "corpus_texts_dt.keys()\n",
        "print('\\n')\n",
        "print(f'There were {len(corpus_texts_dt)} preprocessed Text read into the Dict corpus_texts_dt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JI2z5wCz8zz"
      },
      "outputs": [],
      "source": [
        "# Check if there are any Null strings in the text_clean columns\n",
        "\n",
        "for i, atext in enumerate(list(corpus_texts_dt.keys())):\n",
        "  print(f'\\nNovel #{i}: {atext}')\n",
        "  nan_ct = corpus_texts_dt[atext].text_clean.isna().sum()\n",
        "  if nan_ct > 0:\n",
        "    print(f'      {nan_ct} Null strings in the text_clean column')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgLyDNrYzTuF"
      },
      "outputs": [],
      "source": [
        "# Fill in all the Null value of text_clean with placeholder 'empty_string'\n",
        "\n",
        "for i, atext in enumerate(list(corpus_texts_dt.keys())):\n",
        "  # print(f'Novel #{i}: {atext}')\n",
        "  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n",
        "  corpus_texts_dt[atext][corpus_texts_dt[atext].text_clean.isna()] = 'empty_string'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7gE08b_rNPH"
      },
      "outputs": [],
      "source": [
        "# Verify DataFrame of first Text in Corpus Dictionary\n",
        "\n",
        "corpus_texts_dt[corpus_texts_ls[0]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuoJERbI0wEJ"
      },
      "source": [
        "# **[STEP 4] Get Sentiments with SyuzhetR (4 Models)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMS53E-CorWs"
      },
      "source": [
        "## Compute New SyuzhetR Values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_ls"
      ],
      "metadata": {
        "id": "PlMNpymZAFpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, acorpus in enumerate(corpus_texts_ls):\n",
        "  corpus_texts_dt[acorpus].head()"
      ],
      "metadata": {
        "id": "qjhQjSkfAAw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h94o_8qOAINH"
      },
      "outputs": [],
      "source": [
        "# Verify text_clean of sample text\n",
        "\n",
        "corpus_texts_dt[corpus_texts_ls[0]]['text_clean'].to_list()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDAb6bHqya1-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Compute Sentiments from all 4 Syuzhet Models applied to all 32 Novels (4 x 32 = 128 runs)\n",
        "\n",
        "# NOTE:  9m45s 23:30 on 20220114 Colab Pro (33 Novels)\n",
        "#       28:32s 21:06 on 20220226 Colab Pro (33 Novels)\n",
        "#        3m20s 19:11 on 20220217 Colab Pro (2 Novels)\n",
        "#        3m05s 19:17 on 20220217 Colab Pro (2 Novels)\n",
        "\n",
        "#        2m21s 00:57 @20220406Wed Colab Pro (2 Novels)\n",
        "\n",
        "#        1h29m 09:24 @20220406Wed Colab Pro (2 Financial Ref: Speeches FedGov & EU CenBank)\n",
        "\n",
        "#        3m05s 21:13 on 20220415 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "#        3m05s 21:48 on 20220415 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "#        6m03s 07:39 on 20220416 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "\n",
        "# base = importr('base')\n",
        "syuzhet = importr('syuzhet')\n",
        "\n",
        "# corpus_syuzhetr_dt = {}\n",
        "\n",
        "# base.rank(0, na_last = True)\n",
        "texts_titles_ls = list(corpus_texts_dt.keys())\n",
        "texts_titles_ls.sort()\n",
        "for i, anovel in enumerate(texts_titles_ls):\n",
        "  print(f'Processing Novel #{i}: {anovel}...')\n",
        "  corpus_texts_dt[anovel]['syuzhetr_syuzhet'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='syuzhet')\n",
        "  corpus_texts_dt[anovel]['syuzhetr_bing'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='bing')\n",
        "  corpus_texts_dt[anovel]['syuzhetr_afinn'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='afinn')\n",
        "  corpus_texts_dt[anovel]['syuzhetr_nrc'] = syuzhet.get_sentiment(corpus_texts_dt[anovel]['text_clean'].to_list(), method='nrc')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%whos list"
      ],
      "metadata": {
        "id": "H_UiKtnZUNTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_titles_ls"
      ],
      "metadata": {
        "id": "B-ARGJnTUQhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_dt[texts_titles_ls[0]].head()"
      ],
      "metadata": {
        "id": "ijpZGnR9UF3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify First Text in Corpus has New SyuzhetR Columns with Plausible Values\n",
        "\n",
        "# corpus_texts_dt[next(iter(corpus_texts_dt))].head()\n",
        "\n",
        "corpus_texts_dt[texts_titles_ls[0]].head()\n",
        "corpus_texts_dt[texts_titles_ls[0]].info()\n",
        "\n",
        "corpus_texts_dt[texts_titles_ls[1]].head()\n",
        "corpus_texts_dt[texts_titles_ls[1]].info()"
      ],
      "metadata": {
        "id": "U9fjy5dzrgg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR9X7JGEqkMV"
      },
      "source": [
        "## Checkpoint: Save SyuzhetR Values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%whos str"
      ],
      "metadata": {
        "id": "nKDOsR-5PHoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "#   and destination Subdir for Raw Sentiment Values\n",
        "\n",
        "!pwd\n",
        "print('\\n')\n",
        "\n",
        "print(f'PATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\\n\\n')\n",
        "\n",
        "print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n",
        "\n",
        "!ls $PATH_SENTIMENT_RAW"
      ],
      "metadata": {
        "id": "Durdojkm7zl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Saving Corpus\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "corpus_texts_dt.keys()\n",
        "\n",
        "print(f'\\nTo This Subdirectory:\\n')\n",
        "PATH_SENTIMENT_RAW"
      ],
      "metadata": {
        "id": "EbiBAtrg9lJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder and filter out cols/models to save\n",
        "\n",
        "syuzhetr_only_dt = {}\n",
        "cols_syuzhetr_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'\\n\\nText #{i}: {atext}')\n",
        "  # print(f'      {corpus_texts_dt[atext].info()}')\n",
        "  cols_syuzhetr_ls = [x for x in corpus_texts_dt[atext].columns if 'syuzhetr' in x]\n",
        "  cols_syuzhetr_ls = ['text_raw', 'text_clean'] + cols_syuzhetr_ls\n",
        "  # print(f'      {cols_syuzhetr_ls}')\n",
        "  syuzhetr_only_dt[atext] = corpus_texts_dt[atext][cols_syuzhetr_ls]\n",
        "  syuzhetr_only_dt[atext].columns"
      ],
      "metadata": {
        "id": "85_RTTBcGAWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus_Type"
      ],
      "metadata": {
        "id": "-EzV4iKGM_B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-Kzyc3FEZ71"
      },
      "outputs": [],
      "source": [
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_4syuzhetr.json'\n",
        "elif Corpus_Type == 'reference':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_4syuzhetr.json'\n",
        "else:\n",
        "  print(f'ERROR: Illegal Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "write_dict_dfs(syuzhetr_only_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify json file created\n",
        "\n",
        "!ls -altr $PATH_SENTIMENT_RAW"
      ],
      "metadata": {
        "id": "4Z64a7JPYNKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_dt[corpus_texts_ls[0]].head()"
      ],
      "metadata": {
        "id": "luZgoOmBYZWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Dictionary was saved correctly by reading back the *.json datafile\n",
        "\n",
        "test_dt = read_dict_dfs(in_file=save_filename, in_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "test_dt.keys()"
      ],
      "metadata": {
        "id": "WTjSK5KX-BQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbQbIO9iqHUo"
      },
      "source": [
        "## Plot SyuzhetR 4 Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxfVwdKvudRa"
      },
      "outputs": [],
      "source": [
        "#@markdown Select option to save plots:\n",
        "Save_Raw_Plots = True #@param {type:\"boolean\"}\n",
        "\n",
        "Save_Smooth_Plots = True #@param {type:\"boolean\"}\n",
        "Resolution_in_dpi = \"300\" #@param [\"100\", \"300\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5TXvTTAsPur"
      },
      "outputs": [],
      "source": [
        "# Get Col Names for all 4 SyuzhetR Models\n",
        "\n",
        "cols_all_ls = corpus_texts_dt[texts_titles_ls[0]].columns\n",
        "\n",
        "cols_syuzhetr_ls = [x for x in cols_all_ls if 'syuzhetr_' in x]\n",
        "cols_syuzhetr_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMQ3Sl8trUw"
      },
      "outputs": [],
      "source": [
        "corpus_texts_dt[corpus_texts_ls[0]].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_vars.corpus_titles_dt.keys()"
      ],
      "metadata": {
        "id": "KXUfQYy5XWQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  SUBDIR_GRAPHS = f'graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "  # save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_4syuzhetr.json'\n",
        "elif Corpus_Type == 'reference':\n",
        "  SUBDIR_GRAPHS = f'graphs_{Corpus_Genre}_{Corpus_Type}/'\n",
        "  # save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_4syuzhetr.json'\n",
        "else:\n",
        "  print(f'ERROR: Illegal Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "SUBDIR_GRAPHS = f'{global_vars.SUBDIR_GRAPHS}graphs_{Corpus_Genre}/{SUBDIR_GRAPHS}'\n",
        "print(f'Saving to SUBDIR_GRAPHS: {SUBDIR_GRAPHS}')\n",
        "# print(f'save_filename: {save_filename}')\n",
        "# write_dict_dfs(syuzhetr_only_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}')"
      ],
      "metadata": {
        "id": "Km17AyGENqdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lkWMeKU2BnK"
      },
      "outputs": [],
      "source": [
        "# Verify 4 SyuzhetR Models with Plots\n",
        "\n",
        "for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n",
        "\n",
        "  print(f'Novel #{i}: {global_vars.corpus_titles_dt[anovel][0]}')\n",
        "\n",
        "  # Raw Sentiments \n",
        "  fig = corpus_texts_dt[anovel][cols_syuzhetr_ls].plot(title=f'{global_vars.corpus_titles_dt[anovel][0]}\\n SyuzhetR 4 Models: Raw Sentiments', alpha=0.3)\n",
        "  # plt.show();\n",
        "\n",
        "  if Save_Raw_Plots:\n",
        "    # save_filename = f'{global_vars.SUBDIR_GRAPHS}plot_{anovel}_syuzhetr_raw_dpi{Resolution_in_dpi}.png'\n",
        "    save_filename = f'{Path_to_SentimentArcs}{SUBDIR_GRAPHS}plot_{anovel}_syuzhetr_raw_dpi{Resolution_in_dpi}.png'\n",
        "    print(f'\\n\\nSaving to: {save_filename}')\n",
        "    plt.savefig(save_filename, dpi=int(Resolution_in_dpi))\n",
        "\n",
        "  \n",
        "  # Smoothed Sentiments (SMA 10%)\n",
        "  # novel_sample = 'cdickens_achristmascarol'\n",
        "  win_10per = int(corpus_texts_dt[anovel].shape[0] * 0.1)\n",
        "  corpus_texts_dt[anovel][cols_syuzhetr_ls].rolling(win_10per, center=True, min_periods=0).mean().plot(title=f'{global_vars.corpus_titles_dt[anovel][0]}\\n SyuzhetR 4 Models: Smoothed Sentiments (SMA 10%)', alpha=0.3)\n",
        "  # plt.show();\n",
        "\n",
        "  if Save_Smooth_Plots:\n",
        "    # save_filename = f'{global_vars.SUBDIR_GRAPHS}plot_{anovel}_syuzhetr_smooth10sma_dpi{Resolution_in_dpi}.png'\n",
        "    save_filename = f'{Path_to_SentimentArcs}{SUBDIR_GRAPHS}plot_{anovel}_syuzhetr_smooth10sma_dpi{Resolution_in_dpi}.png'\n",
        "    print(f'\\n\\nSaving to: {save_filename}')\n",
        "    plt.savefig(save_filename, dpi=int(Resolution_in_dpi))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl8og94l09WX"
      },
      "source": [
        "# **[STEP 5] Get Sentiments with SentimentR (7 Models)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQeJ5VtmkX3I"
      },
      "source": [
        "## Compute New SentimentR Values\n",
        "\n",
        "Call function in external get_sentimentr.R from within Python Loop\n",
        "\n",
        "* https://medium.com/analytics-vidhya/calling-r-from-python-magic-of-rpy2-d8cbbf991571\n",
        "\n",
        "* https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjytKCXPjZ_o"
      },
      "outputs": [],
      "source": [
        "%%file get_sentimentr.R\n",
        "\n",
        "library(sentimentr)\n",
        "library(lexicon)\n",
        "\n",
        "get_sentimentr_values <- function(s_v) {\n",
        "  \n",
        "  print('Processing sentimentr_jockersrinker')\n",
        "  sentimentr_jockersrinker <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers_rinker, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_jockers')\n",
        "  sentimentr_jockers <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_jockers, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_huliu')\n",
        "  sentimentr_huliu <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_huliu, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_nrc')\n",
        "  sentimentr_nrc <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_nrc, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_senticnet')\n",
        "  sentimentr_senticnet <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_senticnet, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_sentiword')\n",
        "  sentimentr_sentiword <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_sentiword, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_loughran_mcdonald')\n",
        "  sentimentr_loughran_mcdonald <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_loughran_mcdonald, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  print('Processing sentimentr_socal_google')\n",
        "  sentimentr_socal_google <- sentiment(s_v, polarity_dt=lexicon::hash_sentiment_socal_google, \n",
        "                                        hypen=\"\", amplifier.weight=0.8, n.before=5, n.after=2,\n",
        "                                        adversative.weight=0.25, neutral.nonverb.like = FALSE, missing_value = 0)\n",
        "\n",
        "  anovel_sentimentr_df <- data.frame('text_clean' = s_v,\n",
        "                                'sentimentr_jockersrinker' = sentimentr_jockersrinker$sentiment,\n",
        "                                'sentimentr_jockers' = sentimentr_jockers$sentiment,\n",
        "                                'sentimentr_huliu' = sentimentr_huliu$sentiment,\n",
        "                                'sentimentr_nrc' = sentimentr_nrc$sentiment,\n",
        "                                'sentimentr_senticnet' = sentimentr_senticnet$sentiment,\n",
        "                                'sentimentr_sentiword' = sentimentr_sentiword$sentiment,\n",
        "                                'sentimentr_loughran_mcdonald' = sentimentr_loughran_mcdonald$sentiment,\n",
        "                                'sentimentr_socal_google' = sentimentr_socal_google$sentiment\n",
        "                                )\n",
        "  return(anovel_sentimentr_df)\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baykTTfbkROx"
      },
      "outputs": [],
      "source": [
        "# Verify the *.R file above was written correctly\n",
        "\n",
        "!cat get_sentimentr.R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGpOb1CNkRLJ"
      },
      "outputs": [],
      "source": [
        "# Setup python robject with external library::function()\n",
        "# https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n",
        "\n",
        "# import rpy2.robjects as robjects\n",
        "\n",
        "# Defining the R script and loading the instance in Python\n",
        "# from rpy2.robjects import pandas2ri \n",
        "r = robjects.r\n",
        "\n",
        "# Loading the function we have defined in R.\n",
        "r['source']('get_sentimentr.R')\n",
        "\n",
        "# Reading and processing data\n",
        "get_sentimentr_function_r = robjects.globalenv['get_sentimentr_values']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_dt.keys()"
      ],
      "metadata": {
        "id": "K4JdlG3KBEmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bYu1QKMmMBR"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "# Convert Python List of Strings to a R vector of characters\n",
        "# test_ls = corpus_texts_dt[next(iter(corpus_texts_dt))]['text_clean'].to_list()\n",
        "test_ls = corpus_texts_dt[corpus_texts_ls[0]]['text_clean'].to_list()\n",
        "s_v = robjects.StrVector(test_ls)\n",
        "type(s_v)\n",
        "\n",
        "get_sentimentr_function_r(s_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJOt0G-kyhJu"
      },
      "outputs": [],
      "source": [
        "text_clean_ct = corpus_texts_dt[corpus_texts_ls[0]].text_clean.isna().sum()\n",
        "text_clean_ct\n",
        "# len(text_clean_ls.isnull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi-unkmAJLJl"
      },
      "source": [
        "**[RE-EXECUTE] May have to re-execute following code cell several times**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%whos dict"
      ],
      "metadata": {
        "id": "Fsvo0h0Aq2LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_dt.keys()"
      ],
      "metadata": {
        "id": "_ht4lNv9y-HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3EW-6zVlGxW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  8m19s 13 Novels \n",
        "#       16m39s 19 Novels\n",
        "#      -----------------\n",
        "#       24m58s 32 Novels\n",
        "#        5m00s  @19:44 on 20220227 Colab Pro (2 Novels)\n",
        "\n",
        "#        3m18s 21:24 on 20220415 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "#        3m09s 21:45 on 20220415 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "\n",
        "#        3m17s 08:17 on 20220416 Colab Pro (3 Novels, 628k, 662k, 897k)\n",
        "\n",
        "\n",
        "# Call external get_sentimentr::get_sentimentr_values with Python loop over all novels\n",
        "\n",
        "# novels_sentimentr_dt = {}\n",
        "\n",
        "# Reset corpus_texts_dt\n",
        "# corpus_texts_dt = {}\n",
        "\n",
        "anovel_df = pd.DataFrame()\n",
        "\n",
        "novels_titles_ls = list(corpus_texts_dt.keys())\n",
        "novels_titles_ls.sort()\n",
        "# for i, anovel in enumerate(novels_titles_ls[:19]):\n",
        "for i, anovel in enumerate(novels_titles_ls):  \n",
        "  print(f'\\nProcessing Novel #{i}: {anovel}')\n",
        "  \n",
        "  # Delete contents of anovel_df DataFrame\n",
        "  anovel_df = anovel_df[0:0]\n",
        "\n",
        "  print(f'     {corpus_texts_dt[anovel].shape}')\n",
        "  # Get text_clean as list of strings\n",
        "  text_clean_ls = corpus_texts_dt[anovel]['text_clean'].to_list()\n",
        "\n",
        "  # Convert Python List of Strings to a R vector of characters\n",
        "  # https://rpy2.github.io/doc/v3.0.x/html/generated_rst/pandas.html\n",
        "  s_v = robjects.StrVector(text_clean_ls)\n",
        "  anovel_df_r = get_sentimentr_function_r(s_v)\n",
        "\n",
        "  # Convert rpy2.robjects.vectors.DataFrame to pandas.core.frame.DataFrame\n",
        "  # https://stackoverflow.com/questions/20630121/pandas-how-to-convert-r-dataframe-back-to-pandas \n",
        "  print(f'type(anovel_df_r): {type(anovel_df_r)}')\n",
        "  anovel_df = pd.DataFrame.from_dict({ key : np.asarray(anovel_df_r.rx2(key)) for key in anovel_df_r.names })\n",
        "  print(f'type(anovel_df): {type(anovel_df)}')\n",
        "\n",
        "  # Save Results\n",
        "  # novels_dt[anovel] = anovel_df.copy(deep=True)\n",
        "\n",
        "  # This works for Novels New Corpus Texts\n",
        "  corpus_texts_dt[anovel]['sentimentr_jockersrinker'] = anovel_df['sentimentr_jockersrinker']\n",
        "  corpus_texts_dt[anovel]['sentimentr_jockers'] = anovel_df['sentimentr_jockers']\n",
        "  corpus_texts_dt[anovel]['sentimentr_huliu'] = anovel_df['sentimentr_huliu']\n",
        "  corpus_texts_dt[anovel]['sentimentr_nrc'] = anovel_df['sentimentr_nrc']\n",
        "  corpus_texts_dt[anovel]['sentimentr_senticnet'] = anovel_df['sentimentr_senticnet']\n",
        "  corpus_texts_dt[anovel]['sentimentr_sentiword'] = anovel_df['sentimentr_sentiword']\n",
        "  corpus_texts_dt[anovel]['sentimentr_loughran_mcdonald'] = anovel_df['sentimentr_loughran_mcdonald']\n",
        "  corpus_texts_dt[anovel]['sentimentr_socal_google'] = anovel_df['sentimentr_socal_google'] \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  # This works for Novels Reference Corpus Texts\n",
        "  corpus_texts_dt[anovel]['sentimentr_jockersrinker'] = anovel_df[anovel]['sentimentr_jockersrinker']\n",
        "  corpus_texts_dt[anovel]['sentimentr_jockers'] = anovel_df[anovel]['sentimentr_jockers']\n",
        "  corpus_texts_dt[anovel]['sentimentr_huliu'] = anovel_df[anovel]['sentimentr_huliu']\n",
        "  corpus_texts_dt[anovel]['sentimentr_nrc'] = anovel_df[anovel]['sentimentr_nrc']\n",
        "  corpus_texts_dt[anovel]['sentimentr_senticnet'] = anovel_df[anovel]['sentimentr_senticnet']\n",
        "  corpus_texts_dt[anovel]['sentimentr_sentiword'] = anovel_df[anovel]['sentimentr_sentiword']\n",
        "  corpus_texts_dt[anovel]['sentimentr_loughran_mcdonald'] = anovel_df[anovel]['sentimentr_loughran_mcdonald']\n",
        "  corpus_texts_dt[anovel]['sentimentr_socal_google'] = anovel_df[anovel]['sentimentr_socal_google'] \n",
        "\"\"\" "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anovel_df.head()"
      ],
      "metadata": {
        "id": "XZBP1FUb0art"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_dt[texts_titles_ls[0]].head()"
      ],
      "metadata": {
        "id": "QajVHfJv1zqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_sentimentr_ls = [x for x in corpus_texts_dt[corpus_texts_ls[0]].columns if 'sentimentr_' in x]\n",
        "cols_sentimentr_ls"
      ],
      "metadata": {
        "id": "gE2SroN2z1iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjtPRa6oD0Iz"
      },
      "outputs": [],
      "source": [
        "# Verify DataFrame shape of first Text in Corpus\n",
        "\n",
        "corpus_texts_dt[corpus_texts_ls[0]].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmvRotzK2Ot-"
      },
      "source": [
        "## Checkpoint: Save SentimentR Values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify in SentimentArcs Root Directory\n",
        "#   and destination Subdir for Raw Sentiment Values\n",
        "\n",
        "!pwd\n",
        "print('\\n')\n",
        "\n",
        "print(f'PATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\\n\\n')\n",
        "\n",
        "print('Existing Sentiment Datafiles in Destination Subdir:\\n')\n",
        "\n",
        "!ls $PATH_SENTIMENT_RAW"
      ],
      "metadata": {
        "id": "LY9Qo3MC2n--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Saving Corpus\n",
        "\n",
        "print(f'Saving Text_Type: {Corpus_Genre}')\n",
        "print(f'     Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "print(f'\\nThese Text Titles:\\n')\n",
        "corpus_texts_dt.keys()"
      ],
      "metadata": {
        "id": "cBL20HOu2n_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder and filter out cols/models to save\n",
        "\n",
        "sentimentr_only_dt = {}\n",
        "cols_sentimentr_ls = []\n",
        "\n",
        "for i, atext in enumerate(corpus_texts_ls):\n",
        "  print(f'\\n\\nModel #{i}: {atext}')\n",
        "  # print(f'      {corpus_texts_dt[atext].info()}')\n",
        "  cols_sentimentr_ls = [x for x in corpus_texts_dt[atext].columns if 'sentimentr' in x]\n",
        "  cols_sentimentr_ls = ['text_raw', 'text_clean'] + cols_sentimentr_ls\n",
        "  # print(f'      {cols_syuzhetr_ls}')\n",
        "  sentimentr_only_dt[atext] = corpus_texts_dt[atext][cols_sentimentr_ls]\n",
        "  sentimentr_only_dt[atext].columns"
      ],
      "metadata": {
        "id": "WZ0M5gxNFbEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_8sentimentr.json'\n",
        "elif Corpus_Type == 'reference':\n",
        "  save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_8sentimentr.json'\n",
        "else:\n",
        "  print(f'ERROR: Illegal Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "# write_dict_dfs(sentimentr_only_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}')"
      ],
      "metadata": {
        "id": "5KIxHMDmU9FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sentiment values to subdir_sentiments\n",
        "\"\"\"\n",
        "save_filename = f'all_{Corpus_Genre}_{Corpus_Type}_8sentimentr.json'\n",
        "\n",
        "write_dict_dfs(sentimentr_only_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}')\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "wmhksvludOU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Dictionary was saved correctly by reading back the *.json datafile\n",
        "\n",
        "test_dt = read_dict_dfs(in_file=save_filename, in_dir=f'{PATH_SENTIMENT_RAW}/')\n",
        "test_dt.keys()"
      ],
      "metadata": {
        "id": "cWaVnCEs2n_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g41AtTk2OuD"
      },
      "source": [
        "## Plot SentimentR 7 Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crebkDiI2OuE"
      },
      "outputs": [],
      "source": [
        "#@markdown Select option to save plots:\n",
        "Save_Raw_Plots = True #@param {type:\"boolean\"}\n",
        "\n",
        "Save_Smooth_Plots = True #@param {type:\"boolean\"}\n",
        "Resolution_in_dpi = \"300\" #@param [\"100\", \"300\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Col Names for all SentimentR Models\n",
        "cols_all_ls = corpus_texts_dt[corpus_texts_ls[0]].columns\n",
        "cols_sentimentr_ls = [x for x in cols_all_ls if 'sentimentr_' in x]\n",
        "cols_sentimentr_ls"
      ],
      "metadata": {
        "id": "5Y8YMcussjxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW2i22s6tCaH"
      },
      "outputs": [],
      "source": [
        "corpus_texts_dt[corpus_texts_ls[0]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save sentiment values to subdir_sentiments\n",
        "\n",
        "if Corpus_Type == 'new':\n",
        "  SUBDIR_GRAPHS = f'graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n",
        "  # save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all_4syuzhetr.json'\n",
        "elif Corpus_Type == 'reference':\n",
        "  SUBDIR_GRAPHS = f'graphs_{Corpus_Genre}_{Corpus_Type}/'\n",
        "  # save_filename = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_all_4syuzhetr.json'\n",
        "else:\n",
        "  print(f'ERROR: Illegal Corpus_Type: {Corpus_Type}')\n",
        "\n",
        "SUBDIR_GRAPHS = f'{global_vars.SUBDIR_GRAPHS}graphs_{Corpus_Genre}/{SUBDIR_GRAPHS}'\n",
        "print(f'Saving to SUBDIR_GRAPHS: {SUBDIR_GRAPHS}')\n",
        "# print(f'save_filename: {save_filename}')\n",
        "# write_dict_dfs(syuzhetr_only_dt, out_file=save_filename, out_dir=f'{PATH_SENTIMENT_RAW}')"
      ],
      "metadata": {
        "id": "gkMqQFgYedRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq3isNKvtCaI"
      },
      "outputs": [],
      "source": [
        "# Verify 8 SentimentR Models with Plots\n",
        "\n",
        "for i, anovel in enumerate(list(corpus_texts_dt.keys())):\n",
        "\n",
        "  print(f'Novel #{i}: {global_vars.corpus_titles_dt[anovel][0]}')\n",
        "\n",
        "  # Raw Sentiments \n",
        "  fig = corpus_texts_dt[anovel][cols_sentimentr_ls].plot(title=f'{global_vars.corpus_titles_dt[anovel][0]}\\n SentimentR 8 Models: Raw Sentiments', alpha=0.3)\n",
        "  # plt.show();\n",
        "\n",
        "  if Save_Raw_Plots:\n",
        "    # save_filename = f'{global_vars.SUBDIR_GRAPHS}plot_sentimentr_raw_{anovel}_dpi{Resolution_in_dpi}.png'\n",
        "    save_filename = f'{Path_to_SentimentArcs}{SUBDIR_GRAPHS}plot_{anovel}_sentimentr_raw_dpi{Resolution_in_dpi}.png'\n",
        "    print(f'\\n\\nSaving to: {save_filename}')\n",
        "    plt.savefig(save_filename, dpi=int(Resolution_in_dpi))\n",
        "\n",
        "  \n",
        "  # Smoothed Sentiments (SMA 10%)\n",
        "  # novel_sample = 'cdickens_achristmascarol'\n",
        "  win_10per = int(corpus_texts_dt[anovel].shape[0] * 0.1)\n",
        "  corpus_texts_dt[anovel][cols_sentimentr_ls].rolling(win_10per, center=True, min_periods=0).mean().plot(title=f'{global_vars.corpus_titles_dt[anovel][0]}\\n SentimentR 7 Models: Smoothed Sentiments (SMA 10%)', alpha=0.3)\n",
        "  # plt.show();\n",
        "\n",
        "  if Save_Smooth_Plots:\n",
        "    # save_filename = f'{global_vars.SUBDIR_GRAPHS}plot_sentimentr_smooth10sma_{anovel}_dpi{Resolution_in_dpi}.png'\n",
        "    save_filename = f'{Path_to_SentimentArcs}{SUBDIR_GRAPHS}plot_{anovel}_sentimentr_raw_dpi{Resolution_in_dpi}.png'\n",
        "    print(f'\\n\\nSaving to: {save_filename}')\n",
        "    plt.savefig(save_filename, dpi=int(Resolution_in_dpi))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_texts_ls"
      ],
      "metadata": {
        "id": "BvyiMRSNJOdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_indx = 0\n",
        "\n",
        "corpus_texts_dt[corpus_texts_ls[text_indx]].head()"
      ],
      "metadata": {
        "id": "S_Xd-Wy75zTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_start = 50\n",
        "sentence_end = 60\n",
        "\n",
        "' '.join(list(corpus_texts_dt[corpus_texts_ls[text_indx]].iloc[sentence_start:sentence_end]['text_raw']))"
      ],
      "metadata": {
        "id": "_zP4FF795qNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC3xCnGF22td"
      },
      "source": [
        "# **END OF NOTEBOOK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ADWh-aaImU"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HM4ePEaq6nrW",
        "CRb36wyH7moE",
        "BW6YDyHT7moF",
        "uNwIAUc6uDdo",
        "If55aLQYsk_K",
        "EA1yTaY_9Qod",
        "7dPPrZwyIIze",
        "X229IbToHwa2",
        "fys3dkJSB656"
      ],
      "name": "sentiment_arcs_part2_syuzhetr4_sentimentr8.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}