{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **SentimentArcs (Part 6): Analysis**\n","\n","By: Jon Chun\n","* Original: 12 Jun 2021\n","* Last Update: 14 Apr 2022\n"]},{"cell_type":"markdown","metadata":{"id":"43oGeYK19Pyq"},"source":["# **[STEP 0] Install Libaries**"]},{"cell_type":"code","source":["# If you see [Interactive namespace is empty] in response to the [%who] command below\n","#   your working with a fresh Linux Virtual Machine,\n","#   any previous work is lost,\n","#   and you need to SEQUENTIALLY execute EVERY cell this Notebook from the beginning \n","\n","%whos"],"metadata":{"id":"OBRU1f0vDVmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Takes far too long for inference, \n","#   currently not used\n","\n","# !pip install moepy"],"metadata":{"id":"MREjoHDm8juf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dtaidistance"],"metadata":{"id":"1N-dEQbfjAQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sktime"],"metadata":{"id":"-tqkikuj286D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mT-77aUWNOT_"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required (only needed for Plotly)\n","\n","# Designed Security Hole in older version of PyYAML, must upgrade to use plotly\n","\n","# !pip install pyyaml==5.4.1"]},{"cell_type":"code","source":["# To Reduce Time Series Dimensionality\n","\n","!pip install lttb"],"metadata":{"id":"1EIEJMfQkODG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tslearn"],"metadata":{"id":"uggjF0ptYV50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [STEP 1] Manual Configuration"],"metadata":{"id":"_Y0sLmhmSisA"}},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bfkqjgMiw7T"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n","# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n","print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n","\n","print('\\n')\n","\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"],"metadata":{"id":"sbNX_gP790_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 2] Automatic Configuration/Setup**"],"metadata":{"id":"5iUxLgUCjpJg"}},{"cell_type":"markdown","metadata":{"id":"zeLft8mw7moD"},"source":["## (each time) Custom Libraries & Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPdbnOjw6ycy"},"outputs":[],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvMuohQZ6ycz"},"outputs":[],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n","global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n","\n","global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n","global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","from utils import sa_config # (e.g. define TEST_WORDS_LS)\n","\n","sa_config.set_globals()\n","\n","global_vars.TEST_WORDS_LS\n","print('\\n')\n","\n","dir(global_vars)"]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"C16iaUH5luqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and clean for each iteration of notebook\n","\n","# dir(global_vars)\n","\n","global_vars.corpus_texts_dt = {}\n","global_vars.corpus_titles_dt = {}"],"metadata":{"id":"Y2IRi-3z7moE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IU1IHzA7moE"},"outputs":[],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","source":["global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"oQn0v1jaxTj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.SUBDIR_SENTIMENT_CLEAN = './sentiment_clean/sentiemnt_clean_novels_new_corpus2/'\n","global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"nl1CFufcxZ-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW6YDyHT7moF"},"source":["## (each time) Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUveIcUOzYav"},"outputs":[],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n","\n","global_vars.corpus_titles_dt"]},{"cell_type":"code","source":["global_vars.models_titles_dt.items()"],"metadata":{"id":"xvZ-G0wvCpdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_titles_dt"],"metadata":{"id":"2VkkiGTyAqDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRb36wyH7moE"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjoIK5U_7moE"},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"code","source":["# Intentionally left blank"],"metadata":{"id":"FuWGvo26zAlM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ6eN87UjpJh"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm._tqdm_notebook import tqdm_notebook\n","import pandas as pd\n","tqdm_notebook.pandas()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)\n","\n","import json\n","from collections import Counter\n","\n","# from glob import glob\n","# import copy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoJdRfJh7FSz"},"outputs":[],"source":["# Scikit Utilities, Metrics, Pipelines and Models\n","\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n"]},{"cell_type":"markdown","source":["## (del?) Define Global Parameters"],"metadata":{"id":"6WV29ZjPjpJh"}},{"cell_type":"code","source":["\"\"\"\n","\n","# Define Globals\n","\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"],"metadata":{"id":"vhoFy1ysjpJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GM-ZzGwkjpJh"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YN--P4PgjpJh"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"-XHwZJgsjpJh"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n"],"metadata":{"id":"qMECX12r_CNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DONrzMXxAmYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X229IbToHwa2"},"source":["## Python Utility Functions"]},{"cell_type":"markdown","source":["### (each time) Generate Convenient Data Lists"],"metadata":{"id":"gjmkC1FbAEpo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"If9dQpsIAm_h"},"outputs":[],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"WYVphsv3l8kO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNlQr4_Ckb1"},"outputs":[],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.model_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.model_ensemble_dt['lexicon']"]},{"cell_type":"markdown","source":["### File Functions"],"metadata":{"id":"B3sXwZOq_-0d"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"],"metadata":{"id":"3JQBWKKcN2Eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fys3dkJSB656"},"source":["# **[STEP 3] Read all Raw Sentiment Data**\n","\n","\n"]},{"cell_type":"markdown","source":["## Read Raw Sentiments"],"metadata":{"id":"O82nw_wvJsz9"}},{"cell_type":"code","source":["# Verify cwd and subdir of Raw Sentiment Data\n","\n","print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_SENTIMENT_RAW}')\n","\n","PATH_SENTIMENT_RAW = f'{Path_to_SentimentArcs}sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","\n","print(f'\\nPATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\\n')\n","\n","print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n","!ls -1 $PATH_SENTIMENT_RAW\n","\n","# glob(f'{PATH_SENTIMENT_RAW}/*')\n","\n","print('\\n')\n","\n","print(corpus_texts_ls)"],"metadata":{"id":"Roq-2Ol8yH5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClL4-1Gqe7g"},"outputs":[],"source":["# Create a List (sentiment_raw_json_ls) of all preprocessed text files\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","try:\n","    sentiment_raw_json_ls = glob(f'{PATH_SENTIMENT_RAW}/sentiment_raw_*.json')\n","    sentiment_raw_json_ls = [x.split('/')[-1] for x in sentiment_raw_json_ls]\n","    # sentiment_raw_json_ls = [x.split('.')[0] for x in sentiment_raw_json_ls]\n","except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","print('\\n'.join(sentiment_raw_json_ls))\n","print('\\n')\n","print(f'Found {len(sentiment_raw_json_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n"]},{"cell_type":"code","source":["# Global Dict for Sentiments\n","\n","# Only used in this Notebook so not in defined in shared utils/global_vars\n","#   like global_vars.corpus_texts_dt = {}\n","\n","# corpus_sentiments_dt[text] = DataFrame(Raw Sentiments, 1 Column per Model)\n","\n","corpus_sentiment_dt = {}"],"metadata":{"id":"Kr9cd_FzkqwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"3kLkggXAqYfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_raw_json_ls"],"metadata":{"id":"wxXc1pzoFc5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATH_SENTIMENT_RAW"],"metadata":{"id":"TA2Y0YB9FkHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:   2m37s @09:32 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:07 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:09 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","\n","# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","# Reset Dict for Sentiments\n","#   Only used in this notebook, not shared across notebooks so do not\n","#   share via utils/global_vars like global_vars.corpus_texts_dt\n","\n","corpus_sentiment_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing text #{i}: {atext}')\n","  corpus_sentiment_dt[atext] = pd.DataFrame(columns=['text_raw','text_clean'])\n","\n","  for j, ajson in enumerate(sentiment_raw_json_ls):\n","    print(f'  Reading json #{j}: {ajson}')\n","\n","    afile_fullpath = f'{PATH_SENTIMENT_RAW}{ajson}'\n","    print(f'               at: {afile_fullpath}')\n","\n","    if 'transformer' in ajson:\n","      print(f'   One Model Transformer *.json datafile')\n","    else:\n","      print(f'   Multi-Model non-Transformer *.json datafile')\n","\n","    with open(afile_fullpath) as fp:\n","      json_dt = json.load(fp)\n","      temp_df = pd.DataFrame.from_dict(json_dt[atext]).reset_index()\n","      # temp_df.head(5)\n","      # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","      \n","      # corpus_sentiment_dt[atext]\n","      # print(f'               type: {json_dt[atext]}')\n","\n","    # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","    corpus_sentiment_dt[atext] = pd.concat([corpus_sentiment_dt[atext], temp_df], axis=1).T.drop_duplicates().T #  = corpus_sentiment_dt[atext].update(temp_df)\n","    # pd.concat([DF1, DF2], axis = 1).T.drop_duplicates().T\n","    # corpus_sentiment_dt[atext] = pd.DataFrame.from_dict(json_dt)\n","\n","  # ajson_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  # global_vars.corpus_texts_dt[atext] = ajson_df\n","  # corpus_sentiment_dt[atext] = ajson_df\n","\n","\n","  # a_json = json.loads(json_string)\n","  # print(a_json)\n","\n"],"metadata":{"id":"TvbL8B-cDTCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt.keys()"],"metadata":{"id":"FAjV8WvHHV_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_indx = 1\n","\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].head()\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].info()\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].shape\n","\n","print(f'For Text: {corpus_texts_ls[title_indx]}')"],"metadata":{"id":"UoH1ll-HHZZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Identify and Drop Duplicate Columns"],"metadata":{"id":"e3fBlTEw0fV_"}},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"TN9z8tHwzQni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext]['roberta15lg'].columns"],"metadata":{"id":"GWN5bu0g315t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop all but the i-th copy of duplicated column\n","\n","def keep_nthdup_col(adf, acol, nthcopy):\n","  '''\n","  Given a DataFrame, duplicated col name and nthcopy into set of duplicated cols\n","  Drop the iloc version of the duplicated col list from the DataFrame\n","  '''\n","\n","  df_col_iloc_ls = []\n","\n","  # First, verify this is a duplicated column\n","  col_dup_ls = [x for x in corpus_sentiment_dt[atext].columns if acol == x]\n","  if len(col_dup_ls) <= 1:\n","    print(f'ERROR: Column: {acol} is not duplicated in the DataFrame cols: {adf.columns}')\n","    return\n","\n","  # Loop over all columns to get original iloc of duplicated columns\n","  # corpus_sentiment_dt[atext].columns.get_loc('roberta15lg')  # Return List of booleans\n","\n","  for i in range(adf.shape[1]):\n","\n","    # get current col name\n","    acol_name = adf.columns[i]\n","\n","    # if current col name matches our target col, save it\n","    if acol_name == acol:\n","      # save the iloc\n","      df_col_iloc_ls.append(i)\n","\n","  # Second, verify iloc points to one of the duplicated columns\n","  if nthcopy >= len(df_col_iloc_ls):\n","    print(f'ERROR: passed nthcopy {nthcopy} is bigger than the number of duplicated {acol} column [0 to {len(df_col_iloc_ls)-1}]')\n","    return\n","\n","  print(f' Duplicated col: {acol} indicies: {df_col_iloc_ls}')\n","  col_dup_indx = df_col_iloc_ls[nthcopy]\n","  print(f'     Keep Index: {col_dup_indx}')\n","  print(f'           Name: {adf.columns[col_dup_indx]}')\n","  df_col_iloc_ls.remove(col_dup_indx)\n","  print(f'      Drop Cols: {df_col_iloc_ls}')\n","  # Drop all cols by iloc index in list df_col_iloc_ls\n","  # adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j not in df_col_iloc_ls]]\n","  for acol_indx in df_col_iloc_ls:\n","    adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j != int(acol_indx)]]\n","\n","  \"\"\"\n","  for k, acol_indx in enumerate(df_col_iloc_ls):\n","    acol_drop = adf.columns[acol_indx]\n","    print(f'Dropping column #{k}: {acol_drop} at indx={acol_indx}')\n","    # adf.drop(adf.columns[acol_indx], axis=1, inplace=True)\n","    adf.drop(columns=[acol_drop], axis=1, inplace=True)\n","  \"\"\"\n","\n","  return adf\n","\n","# Test\n","# keep_nthdup_col(corpus_sentiment_dt[atext], 'text_raw', 1)"],"metadata":{"id":"cgGVTMYY-Yr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].iloc[:, [j for j,c in enumerate(list(corpus_sentiment_dt[atext].columns)) if j not in [13,0]]].info()"],"metadata":{"id":"aw2hYCSdT1HL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify and Drop Duplicate Columns\n","\n","col_before_ct = len(corpus_sentiment_dt[atext].columns)\n","dup_col_keep_dt = {}  # Dict[dup_col] = iloc index to keep (col with min nulls)\n","\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  cols_dup_ls = []\n","  row_ct = corpus_sentiment_dt[atext].shape[0]\n","\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","  \n","  # Count the frequency of each column name\n","  cols_ls = corpus_sentiment_dt[atext].columns\n","  # print(f'  Columns: {cols_ls}')\n","  col_count_dt = Counter(cols_ls)\n","\n","  # Create list of duplicate column names in cols_dup_ls\n","  for key,val in col_count_dt.items():\n","    if val > 1:\n","      cols_dup_ls.append(key)\n","      print(f'  Duplicate col: {key} with count: {val}')\n","\n","  # Count how many columns are duplicated\n","  dup_ct = len(cols_dup_ls)\n","\n","  # For every duplicated Column\n","  for j, adup_col in enumerate(cols_dup_ls):\n","    # Count how many duplicates it has\n","    adup_col_ct = len(corpus_sentiment_dt[atext][adup_col])\n","\n","    # Iterate through all duplicates and find the iloc index of the one\n","    #   with the least number of null values as the one to keep (deleting the other dups)\n","    col_iloc_min_null = 0  # Index to the col with min nulls\n","    col_min_null_ct = row_ct  # Current count of null in col with min nulls, init to row count\n","    dup_col_ls = corpus_sentiment_dt[atext][adup_col].columns\n","    for k, adup_col_ver in enumerate(dup_col_ls):\n","      adup_col_null_ct = corpus_sentiment_dt[atext][adup_col].iloc[:,k].isna().sum()\n","      if adup_col_null_ct < col_min_null_ct:\n","        col_min_null_ct = adup_col_null_ct\n","        col_iloc_min_null = k\n","\n","    # Drop all but one copy of the duplicated columns\n","    print(f'\\n      Keep iloc: {col_iloc_min_null} in adup_col: {adup_col} with {adup_col_null_ct} nulls out of {row_ct}')\n","    dup_col_keep_dt[adup_col] = col_iloc_min_null\n","    print(f'       Calling: keep_nthdup_col(adf, {adup_col}, {col_iloc_min_null})')\n","    corpus_sentiment_dt[atext] = keep_nthdup_col(corpus_sentiment_dt[atext], adup_col, col_iloc_min_null)\n","\n","\n","col_after_ct = len(corpus_sentiment_dt[atext].columns)\n","\n","print(f'\\n\\nColumn Count:\\n  Before: {col_before_ct}\\n   After: {col_after_ct}')"],"metadata":{"id":"OBr-XWR9y2UQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].info()"],"metadata":{"id":"7eYAAjDuSkHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reorder and Specify dtypes"],"metadata":{"id":"PPrnvErgWaSC"}},{"cell_type":"code","source":["# Get list of models\n","\n","models_ls = list(set(corpus_sentiment_dt[corpus_texts_ls[0]].columns) - set(['text_raw','text_clean','index']))\n","models_ls.sort()\n","\n","models_ls\n","\n","print(f'\\n\\nTotal of {len(models_ls)} Models')"],"metadata":{"id":"HQxAu4eUVpU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Put text_raw and text_clean at front\n","\n","# corpus_sentiment_dt[atext].sort_index(axis=1)\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(0, 'text_raw', corpus_sentiment_dt[atext].pop('text_raw'))\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(1, 'text_clean', corpus_sentiment_dt[atext].pop('text_clean'))\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","\n","  col_first = corpus_sentiment_dt[atext].pop('index')\n","  corpus_sentiment_dt[atext].insert(0, 'sentence_no', col_first)\n","\n","  col_second = corpus_sentiment_dt[atext].pop('text_raw')\n","  corpus_sentiment_dt[atext].insert(1, 'text_raw', col_second)\n","\n","  col_third = corpus_sentiment_dt[atext].pop('text_clean')\n","  corpus_sentiment_dt[atext].insert(2, 'text_clean', col_third)\n","\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"XD7gqGHGWjj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert objects to more specific dtypes\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    print(f'Processing Model #{j}: {amodel}')\n","\n","    corpus_sentiment_dt[atext][amodel] = corpus_sentiment_dt[atext][amodel].astype('float')\n","\n","  corpus_sentiment_dt[atext]['sentence_no'] = corpus_sentiment_dt[atext]['sentence_no'].astype('int')\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"V30-aErXofrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify sample DataFrame\n","\n","corpus_sentiment_dt[corpus_texts_ls[0]].head()"],"metadata":{"id":"KYdBWtNspn6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verify Raw Plots"],"metadata":{"id":"OnVUg2Z5apcf"}},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"nOdp0EAfktep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"XWEHgHUTlTBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"6QZ-Ep5UmFpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls"],"metadata":{"id":"ip7B09v0YpYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = corpus_sentiment_dt['tmorrison_songofsolomon'][models_ls].rolling(300, center=True, min_periods=0).mean().plot()"],"metadata":{"id":"BM8dfDzbYrpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Raw Sentiments with \n","\n","win_per = 10\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  \n","  win_aper = int(win_per/100 * corpus_sentiment_dt[atext].shape[0])\n","  _ = corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","  _= plt.title(f'Sentiment Analysis\\n{global_vars.corpus_titles_dt[atext][0]}\\nSmoothed SMA ({win_per}%)')\n","  plt.grid(True)\n","\n","print(f'Read Raw Sentiments for these texts:\\n  {corpus_sentiment_dt.keys()}\\n\\n')\n","\n","\n"],"metadata":{"id":"XNRTSir8Db4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Drop or Interpolate and NaN/None Values"],"metadata":{"id":"1bIaaaVgmtV3"}},{"cell_type":"code","source":["corpus_sentiment_dt[atext]['roberta15lg']"],"metadata":{"id":"w3A1ZUCOwG0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop Columns/Models with %NaN above Threshold\n","\n","null_threshold = 0.9  # Drop Col if %rows=null > Threshold\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    # print(f'Processing Model #{j}: {amodel}')\n","\n","    row_ct = len(corpus_sentiment_dt[atext][amodel])\n","    sum_null = corpus_sentiment_dt[atext][amodel].isnull().sum()\n","    # print(f'There are {sum_null} null values of a total {row_ct} rows')\n","    null_threshold = 0.5  # if > 50% null, drop col\n","    # print(f'Threshold: {null_threshold} of all {row_ct} rows')\n","    if sum_null > int(null_threshold * row_ct):\n","      print(f'  %NaNs above Threshold={null_threshold}: {corpus_sentiment_dt[atext][amodel].isna().sum()}')\n","      # TODO: Verify before dropping Col/Model here\n","      # corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","\n"],"metadata":{"id":"s-ruBWYkmuwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"wK44v9rnbJMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YMxWVHNybKhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop Cols with all NaNs (dup all NaNs 'text_raw')\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  # for j, amodel in enumerate(models_ls):\n","\n","  corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].dropna(axis=1, how='all')\n","    \n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"N7TZ1DD5muuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_texts_ls[0]"],"metadata":{"id":"WD0Tm2CSsXVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[1]]"],"metadata":{"id":"zVTKIyzKsL7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4CqJQY9rNRw"},"outputs":[],"source":["# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'Processing text #{i}: {atext}')\n","  corpus_sentiment_dt[atext] = pd.DataFrame(columns=['text_raw','text_clean'])\n","\n","  for j, ajson in enumerate(sentiment_raw_json_ls):\n","    print(f'  Reading json #{j}: {ajson}')\n","\n","    afile_fullpath = f'{PATH_SENTIMENT_RAW}/{ajson}'\n","    print(f'               at: {afile_fullpath}')\n","\n","    with open(afile_fullpath) as fp:\n","      json_dt = json.load(fp)\n","      temp_df = pd.DataFrame.from_dict(json_dt[atext])\n","      # temp_df.head(5)\n","      corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","      \n","      # corpus_sentiment_dt[atext]\n","      # print(f'               type: {json_dt[atext]}')\n","\n","    corpus_sentiment_dt[atext].head()\n","  # corpus_sentiment_dt[i] = pd.DataFrame.from_dict(json_dt)\n","\n","  # ajson_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  # global_vars.corpus_texts_dt[atext] = ajson_df\n","  # corpus_sentiment_dt[atext] = ajson_df\n","\n","\n","  # a_json = json.loads(json_string)\n","  # print(a_json)\n","\n"]},{"cell_type":"code","source":["# Verify Raw Sentiments read\n","\n","print(f'Read Raw Sentiments for these texts:\\n  {corpus_sentiment_dt.keys()}\\n\\n')\n","\n","for i,atext in enumerate(corpus_sentiment_dt.keys()):\n","  print(f'Processing Text #{i}: {atext}')\n","  corpus_sentiment_dt[atext].head()\n","  print('\\n\\n')\n"],"metadata":{"id":"0HxvsoeOUf5X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"cMNuxliUUhfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHLt5o78tJyR"},"outputs":[],"source":["# Verify the Text read into master Dictionary of DataFrames\n","\n","global_vars.corpus_texts_dt.keys()\n","print('\\n')\n","print(f'There were {len(global_vars.corpus_texts_dt)} preprocessed Text read into the Dict global_vars.corpus_texts_dt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JI2z5wCz8zz"},"outputs":[],"source":["# Check if there are any Null strings in the text_clean columns\n","\n","for i, atext in enumerate(list(global_vars.corpus_texts_dt.keys())):\n","  print(f'\\nNovel #{i}: {atext}')\n","  nan_ct = global_vars.corpus_texts_dt[atext].text_clean.isna().sum()\n","  if nan_ct > 0:\n","    print(f'      {nan_ct} Null strings in the text_clean column')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgLyDNrYzTuF"},"outputs":[],"source":["# Fill in all the Null value of text_clean with placeholder 'empty_string'\n","\n","for i, atext in enumerate(list(global_vars.corpus_texts_dt.keys())):\n","  # print(f'Novel #{i}: {atext}')\n","  # Fill all text_clean == Null with 'empty_string' so sentimentr::sentiment doesn't break\n","  global_vars.corpus_texts_dt[atext].iloc[global_vars.corpus_texts_dt[atext].text_clean.isna()] = 'empty_string'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7gE08b_rNPH"},"outputs":[],"source":["# Verify DataFrame of first Text in Corpus Dictionary\n","\n","global_vars.corpus_texts_dt[next(iter(global_vars.corpus_texts_dt))].head()"]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"0pGdRVhomoV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"eefPtETcmpmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (del?) If Sentiment Time Series exist, Verify with Plots"],"metadata":{"id":"kCraVO_7ZtnZ"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, RobustScaler\n","\n","r_scaler = RobustScaler() \n","z_scaler = StandardScaler()"],"metadata":{"id":"jDPkOlSqd42s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_texts_dt[corpus_texts_ls[0]].head()"],"metadata":{"id":"gUs1FFfSGT4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_texts_dt[corpus_texts_ls[0]].head()"],"metadata":{"id":"Ruqd1i32JJL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"tQ2vA6_KI-6j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## (del?) zScale Prior Sentiment Time Series"],"metadata":{"id":"Yfp-Nt7hJiHx"}},{"cell_type":"code","source":["# Plot Sentiment Time Series for all Models in this Notebook run so far\n","\n","df = pd.DataFrame()\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  col_rzscores_ls = []\n","  print(f\"Title #{i}: {atext}\")\n","  df = global_vars.corpus_texts_dt[atext].copy()\n","  numeric_cols_ls = df.select_dtypes(include=[np.number]).columns\n","  for anum_col_str in numeric_cols_ls:\n","    print(f'Processing anum_col: {anum_col_str}')\n","    anum_col_robust_np = r_scaler.fit_transform(df[anum_col_str].values.reshape(-1, 1) )\n","    anum_col_rzscore_np = z_scaler.fit_transform(anum_col_robust_np)\n","    anum_col_rzscore_str = f'{anum_col_str}_rzscore'\n","    df[anum_col_rzscore_str] = pd.Series(anum_col_rzscore_np.squeeze(-1,))\n","    col_rzscores_ls.append(anum_col_rzscore_str)\n","\n","  print(f'df.columns: {df.columns}')\n","  win_10per = int(0.10 * df.shape[0])\n","  # df[col_rzscores_ls].rolling(win_10per, center=True, min_periods=0).mean() # .plot(title=f\"Sentiment Analysis\\n{global_vars.corpus_texts_dt[atext][0]}\\nProcessing: SMA 10% (+ Robust IQR, zScore Scaling)\")"],"metadata":{"id":"OQU0usMYd9uK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dTWYpMQgm53j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jaGEWOh6m5wO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for clean DataFrame\n","\n","# Check for NaN values\n","# TODO:\n","\"\"\"\n","print(f'Any Null values: [{corpus_texts_dt[corpus_titles_ls[0]].isnull().values.any()}]')\n","\n","print('\\n')\n","\n","corpus_texts_dt[corpus_titles_ls[0]].columns.duplicated()\n","\n","print('\\n')\n","\n","print(corpus_texts_dt[corpus_titles_ls[0]].columns.value_counts())\n","\n","print('\\n')\n","\n","next(iter(zip(corpus_texts_dt[atext].columns.duplicated(), corpus_texts_dt[atext].columns)))\n","\"\"\";"],"metadata":{"id":"K7wdtXUCm9aK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clip Outliers and zScore Standardize"],"metadata":{"id":"mcbiCK2wm9aK"}},{"cell_type":"code","source":["import statsmodels.robust.scale as sm_robust"],"metadata":{"id":"mfhlcx0Qm9aK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, RobustScaler\n","\n","r_scaler = RobustScaler() \n","z_scaler = StandardScaler()"],"metadata":{"id":"W4wX7y5Ycwzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls[7]"],"metadata":{"id":"Qs5nk8CTb3Ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simple IQR\n","\n","def clip_iqr_outliers(floats_ser, iqr_limit=1.5):\n","  '''\n","  Given a Pandas Series of floats and an upper limit on IQR variance from the median\n","  Clip all outliers beyond the iqr_limit and return a list of floats\n","  '''\n","\n","  quantile10 = floats_ser.quantile(0.10)\n","  quantile90 = floats_ser.quantile(0.90)\n","  print(f'10% Quantile: {quantile10}')\n","  print(f'90% Quantile: {quantile90}')\n","\n","  floats_np = np.where(floats_ser < quantile10, quantile10, floats_ser)\n","  floats_np = np.where(floats_ser > quantile90, quantile90, floats_ser)\n","  print(f'        Skew: {pd.Series(floats_np).skew()}')\n","\n","  return floats_np # .tolist()\n","\n","# Test\n","\n","test_np = clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'])\n","len(test_np)"],"metadata":{"id":"6NaTReBnhXP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'].quantile(0.10)"],"metadata":{"id":"eME8b7khkfYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'],iqr_limit=1.5) # .values.reshape(-1, 1) )"],"metadata":{"id":"Ofr3nDIYj2-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']]"],"metadata":{"id":"3kJhxxYcnLEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ls = list(corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns) # .remove('sentence_no')\n","test_ls.remove('sentence_no')\n","test_ls"],"metadata":{"id":"N0h3o0NqpPni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls"],"metadata":{"id":"pJiADBMtqpxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns if 'rz' not in x]"],"metadata":{"id":"6yh0_nSbqiVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trim Outliers and zScore Standardize\n","\n","corpus_sentiment_rz_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  # atext_rz_df = corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']].copy(deep=True)\n","  # col_rzscores_ls = []\n","  print(f\"Title #{i}: {atext}\")\n","  # df = corpus_sentiment_dt[atext].copy()\n","  # numeric_cols_ls = list(corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns) # .remove('sentence_no')\n","  # numeric_cols_ls.remove('sentence_no')\n","\n","  # for anum_col_str in numeric_cols_ls:\n","  for j,amodel in enumerate(models_ls):\n","    print(f'  Model #{j}: {amodel}')\n","    # anum_col_robust_np = r_scaler.fit_transform(df[amodel].values.reshape(-1, 1) )\n","    arobust_col_np = clip_iqr_outliers(corpus_sentiment_dt[atext][amodel],iqr_limit=1.5)\n","    # scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))\n","    # arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np)\n","    arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np.reshape(-1,1))\n","    arobust_zscaled_col_str = f'{amodel}_rz'\n","    corpus_sentiment_dt[atext][arobust_zscaled_col_str] = pd.Series(arobust_zscaled_col_np.squeeze(-1,))\n","  # corpus_sentiment_rz_dt[atext] = atext_rz_df\n","\n","  # anum_col_rzscore_np = z_scaler.fit_transform(anum_col_robust_np)\n","  # anum_col_rzscore_str = f'{anum_col_str}_rzscore'\n","  # df[anum_col_rzscore_str] = pd.Series(anum_col_rzscore_np.squeeze(-1,))\n","  # col_rzscores_ls.append(anum_col_rzscore_str)\n","\n","  # print(f'df.columns: {df.columns}')\n","  # win_10per = int(0.10 * df.shape[0])\n","  # df[col_rzscores_ls].rolling(win_10per, center=True, min_periods=0).mean() # .plot(title=f\"Sentiment Analysis\\n{global_vars.corpus_texts_dt[atext][0]}\\nProcessing: SMA 10% (+ Robust IQR, zScore Scaling)\")"],"metadata":{"id":"S-BNslD6jepP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_sentiment_dt[atext] if 'rz' in x]"],"metadata":{"id":"WDLCas_DsDjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for atext in corpus_texts_ls:\n","  col_drop_ls = [x for x in corpus_sentiment_dt[atext] if 'rz' in x]\n","  print(f'Dropping: {len(col_drop_ls)} Columns\\n  {col_drop_ls}\\n\\n')\n","  corpus_sentiment_dt[atext].drop(columns=col_drop_ls, inplace=True)"],"metadata":{"id":"YszDtUe2q5pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_rz_ls = [x for x in corpus_sentiment_dt[corpus_texts_ls[0]] if 'rz' in x]\n","models_rz_ls"],"metadata":{"id":"gQbJtE8eqK33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[0]][models_rz_ls].median(axis=1).rolling(300, center=True, min_periods=0).mean().plot()"],"metadata":{"id":"i8NYIzyTtF6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_indx = 0\n","text_str = corpus_texts_ls[text_indx]\n","title_str = global_vars.corpus_titles_dt[text_str][0]\n","win_per = 10\n","win_size = int(win_per/100 * corpus_sentiment_dt[text_str].shape[0])\n","\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].rolling(win_size, center=True, min_periods=0).mean().plot(alpha=0.3)\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].mean(axis=1).rolling(win_size, center=True, min_periods=0).mean().plot(label='mean', color='red', linewidth=3, alpha=0.7)\n","_ = plt.legend(loc='best')\n","_ = plt.title(f'Sentiment Arc: {title_str}\\nSmoothed SMA ({win_per}%)')\n","plt.grid(True)"],"metadata":{"id":"WrCMhm26jems"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"aod-A7AYvjc2"}},{"cell_type":"markdown","metadata":{"id":"9xFkzLf2H7lJ"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","source":["# TODO: Norm all paths and subdirs as 'dir/dir/dir/' except for root: '/dir/dir/dir/'\n","\n","global_vars.SUBDIR_SENTIMENT_CLEAN = 'sentiment_clean/sentiment_clean_novels_new_corpus2/'\n","\n","print(f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"wOr5EsrMyNqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","print(f'\\nSaving Text_Type: {Corpus_Genre}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nThese Text Titles:')\n","list(corpus_sentiment_dt.keys())\n","\n","print(f'\\n\\nTo This Subdirectory:\\n  {global_vars.SUBDIR_SENTIMENT_CLEAN}')\n","\n","full_path = f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}'\n","print(f'\\nFull path to this Subdirectory:\\n  {full_path}')\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all.json'\n","else:\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_reference_all.json'\n","print(f'\\nUnder this Filename:\\n  {save_filename}')\n","\n","write_dict_dfs(corpus_sentiment_dt, out_file=save_filename, out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"zcH7XyNYH7lJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify json file created\n","\n","!ls -altr $global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"8IhmigV8H7lK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [SKIP]"],"metadata":{"id":"s7tj67PRjeiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"eNoxodZUjg_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WvNjdH3ujg79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clip Outliers based on IQR: RobustScaler())\n","\n","def clip_outliers(floats_ser):\n","  '''\n","  Given a pd.Series of float values\n","  Return a list with outliers removed, values limited within 3 median absolute deviations from median\n","  '''\n","  # https://www.statsmodels.org/stable/generated/statsmodels.robust.scale.mad.html#statsmodels.robust.scale.mad\n","\n","  # Old mean/std, less robust\n","  # ser_std = floats_ser.std()\n","  # ser_median = floats_ser.mean() # TODO: more robust: asym/outliers -> median/IQR or median/median abs deviation\n","\n","  floats_np = np.array(floats_ser)\n","  ser_median = floats_ser.median()\n","  ser_mad = sm_robust.mad(floats_np)\n","  # print(f'ser_median = {ser_median}')\n","  # print(f'ser_mad = {ser_mad}')\n","\n","  if ser_mad == 0:\n","    # for TS with small ranges (e.g. -1.0 to +1.0) Median Abs Deviation = 0\n","    #   so pass back the original time series\n","    floats_clip_ls = list(floats_ser)\n","\n","  else:\n","    ser_oldmax = floats_ser.max()\n","    ser_oldmin = floats_ser.min()\n","    # print(f'ser_max = {ser_oldmax}')\n","    # print(f'ser_min = {ser_oldmin}')\n","\n","    ser_upperlim = ser_median + 2.5*ser_mad\n","    ser_lowerlim = ser_median - 2.5*ser_mad\n","    # print(f'ser_upperlim = {ser_upperlim}')\n","    # print(f'ser_lowerlim = {ser_lowerlim}')\n","\n","    # Clip outliers to max or min values\n","    floats_clip_ls = np.clip(floats_np, ser_lowerlim, ser_upperlim)\n","    # print(f'max floast_ls {floats_ls.max()}')\n","\n","    # def map2range(value, low, high, new_low, new_high):\n","    #   '''map a value from one range to another'''\n","    #   return value * 1.0 / (high - low + 1) * (new_high - new_low + 1)\n","\n","    # Map all float values to range [-1.0 to 1.0]\n","    # floats_clip_sig_ls = [map2range(i, ser_oldmin, ser_oldmax, ser_upperlim, ser_lowerlim) for i in floats_clip_ls]\n","\n","    # listmax_fl = float(max(floats_ls))\n","    # floats_ls = [i/listmax_fl for i in floats_ls]\n","    #floats_ls = [1/(1+math.exp(-i)) for i in floats_ls]\n","\n","  return floats_clip_ls  # floats_clip_sig_ls\n","\n","# Test\n","\n","atext = corpus_texts_ls[0]  # 0 is first novel in corpus\n","amodel = models_ls[0]  # 7 is roberta15lg\n","\n","\n","\n","# Will not work on first run as corpus_sents_df is not defined yet\n","\n","# data = np.array([1, 4, 4, 7, 12, 13, 16, 19, 22, 24])\n","# test_ls = clip_outliers(pd.Series(data))\n","\n","print('Comparison Test: (a) Manual IRQ Clipping vs (b) RobustScaler()')\n","# Plot #1: Clipped Outliers with IQR\n","test_ls = clip_outliers(corpus_sentiment_dt[atext][amodel])\n","# test_ls = clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['afinn'].iloc[0])\n","# print(f'new min is {min(test_ls)}')\n","# print(f'new max is {max(test_ls)}')\n","_ = pd.DataFrame(test_ls).rolling(300, center=True, min_periods=0).mean().plot(label='clipped', alpha=0.7);\n","plt.show();\n","_ = corpus_sentiment_dt[atext][amodel].rolling(300, center=True, min_periods=0).mean().plot(label='original', alpha=0.7)\n","plt.grid(True)\n","\n","\n","# transformer = scaler_robust.fit(corpus_texts_dt[corpus_texts_ls[0]]['vader'].values.reshape(-1, 1))\n","\n","# Plot #2: Scale Outliers with RobustScaler()\n","test_df = corpus_sentiment_dt[atext][amodel].copy(deep=True) #   pd.DataFrame()\n","# test_df = pd.DataFrame({'vader': scaler_robust.fit_transform(np.array(corpus_texts_dt[corpus_texts_ls[0]]['vader']).reshape(-1, 1))})\n","test_df['test_model'] = pd.Series(RobustScaler.fit_transform(np.array(corpus_sentiment_dt[atext][amodel]).reshape(-1, 1)).flatten())\n","test_df['test_model'].rolling(300, center=True, min_periods=0).mean().plot(label='RobustScaler', alpha=0.7)\n","\n","plt.title('Dealing with Outliers in Sentiment Time Series\\n(a) Manually Clip with IQR or,\\n (b) Scale with RobustScaler()')\n","plt.grid(True, alpha=0.7)\n","plt.legend()\n","plt.show();"],"metadata":{"id":"Sk18vjf1m9aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"Le5p920Febcz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[0]].info()"],"metadata":{"id":"_rNfzaQYm9aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Deal with Outliers: (a) Manually clip with IQR, or (b) Automatically Scale RobustScaler()\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'Processing Text #{i}: {atext}')\n","  \n","  win_10per = int(0.10 * corpus_sentiment_dt[corpus_texts_ls[0]].shape[0])\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_rstd_ls = []\n","  for j, amodel in enumerate(models_ls):\n","    amodel_rstd = f'{amodel}_rstd'\n","    # print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rstd})')\n","    # clip_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['vader'])\n","\n","    # Option (a): Manually Clip with 2.5*IQR\n","    # corpus_sentiment_dt[atext][amodel_rstd] = pd.Series(clip_outliers(corpus_sentiment_dt[atext][amodel])) # .reshape(-1,1)).flatten())\n","\n","    # Option (b): Automatically Scale wit scikit-learns ScalerRobust()\n","    corpus_sentiment_dt[atext][amodel_rstd] = pd.Series(RobustScaler.fit_transform(np.array(corpus_sentiment_dt[atext][amodel]).reshape(-1,1)).flatten())\n","\n","    # Plot\n","    _ = ax.plot(corpus_sentiment_dt[atext][amodel_rstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rstd, alpha=0.3)\n","\n","    models_rstd_ls.append(amodel_rstd)\n","\n","  # Plot Median of Ensemble\n","  _ = ax.plot(corpus_sentiment_dt[atext][models_rstd_ls].median(axis=1).rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Shrink current axis by 20%\n","  # box = ax.get_position()\n","  # ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n","\n","  # Put a legend to the right of the current axis\n","  _ = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  atitle = plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%)\\nClipped with IQR + zScore Standardized')\n","  plt.show();\n"],"metadata":{"id":"QiRSoXCXm9aL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Standardization with zScore"],"metadata":{"id":"1854yLZAm9aL"}},{"cell_type":"code","source":["%%time\n","\n","# NOTE:\n","\n","# zScore Standardization (mean=0, std=1)\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  fig = plt.figure()\n","  ax = plt.subplot(111)\n","\n","  models_std_ls = []\n","  for j, amodel in enumerate(ensemble_ls):\n","    amodel_rstd = f'{amodel}_rstd'\n","    amodel_rzstd = f'{amodel}_rzstd'\n","    # print(f'  Model #{j}: {amodel} (Model_Std: {amodel_rzstd})')\n","    # clip_outliers(corpus_texts_dt[corpus_texts_ls[0]]['vader'])\n","\n","    # Get SMA 10% window length\n","    win_10per = int(0.10 * corpus_texts_dt[atext][amodel_rstd].shape[0])\n","\n","\n","    # UNCOMMENT only ONE of these TWO Options\n","    # ---------------------------------------\n","    # Option (a): Manually Clip with IQR\n","    corpus_texts_dt[atext][amodel_rzstd] = scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))\n","\n","    # Option (b): Automatically Scale wit scikit-learns ScalerRobust()\n","    # corpus_texts_dt[atext][amodel_rzstd] = pd.Series(scaler_robust.fit_transform(np.array(corpus_texts_dt[atext][amodel]).reshape(-1,1)).flatten())\n","\n","    # Plot amodel_rzstd\n","    _ = ax.plot(corpus_texts_dt[atext][amodel_rzstd].rolling(win_10per, center=True, min_periods=0).mean(), label=amodel_rzstd, alpha=0.3)\n","\n","    models_std_ls.append(amodel_rzstd)\n","\n","  # Plot Median of Ensemble\n","  _ = ax.plot(corpus_texts_dt[atext][models_std_ls].median(axis=1).rolling(win_10per, center=True, min_periods=0).mean(), label='Ensemble Median', color='r', linewidth=3)\n","\n","  # Put a legend to the right of the current axis\n","  ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","\n","  plt.grid(True, alpha=0.7)\n","  plt.title(f'{corpus_titles_dt[atext][0]}\\nSentimentArc Ensemble of {len(ensemble_ls)} Models\\nSmoothed: SMA (window=10%)\\nClipped with IQR + zScore Standardized')\n","  plt.show();\n"],"metadata":{"id":"P5LQgzwkm9aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop the Robust '_rstd' Columns in for all Texts\n","\n","for i, atext in enumerate(corpus_texts_dt.keys()):\n","  print(f'Text #{i}: {atext}')\n","\n","  models_rstd_ls = [x for x in corpus_texts_dt[atext].columns if '_rstd' in x]\n","  # models_rstd_ls\n","\n","  corpus_texts_dt[atext].drop(columns=models_rstd_ls, inplace=True)\n","  corpus_texts_dt[atext].info()\n","\n","  # Verify no '_rstd' columns exist\n","  [x for x in corpus_texts_dt[atext].columns if '_rstd' in x]\n","  print('\\n')"],"metadata":{"id":"rGVcaEpRm9aL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## [TEMP] Numpy Experiments"],"metadata":{"id":"0Pz3vCAzm9aM"}},{"cell_type":"code","source":["# Test numpy: line vector vs column vector\n","\n","x1_test = np.linspace(0,1,100)\n","x1_test.shape\n","print(x1_test[:5])\n","x2_test = np.linspace(0,1,100).reshape(-1,1)\n","x2_test.shape\n","x2_test[:5]"],"metadata":{"id":"zFd0TOTom9aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 4] Smoothing EDA**"],"metadata":{"id":"u4uQbqBQ04DU"}},{"cell_type":"code","source":[""],"metadata":{"id":"xJA93lJR08bN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 5] Peak Detection & Crux Extraction**"],"metadata":{"id":"gcDJMzX31A5z"}},{"cell_type":"code","source":[""],"metadata":{"id":"CpUbwJ3t08XE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF NOTEBOOK**"]}],"metadata":{"colab":{"collapsed_sections":["BW6YDyHT7moF","CRb36wyH7moE","6WV29ZjPjpJh","GM-ZzGwkjpJh","-XHwZJgsjpJh"],"name":"sentiment_arcs_part6_analysis.ipynb","provenance":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}