{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **SentimentArcs (Part 6): Analysis**\n","\n","By: Jon Chun\n","* Original: 12 Jun 2021\n","* Last Update: 14 Apr 2022\n"]},{"cell_type":"markdown","metadata":{"id":"43oGeYK19Pyq"},"source":["# **[STEP 0] Install Libaries**"]},{"cell_type":"code","source":["# If you see [Interactive namespace is empty] in response to the [%who] command below\n","#   your working with a fresh Linux Virtual Machine,\n","#   any previous work is lost,\n","#   and you need to SEQUENTIALLY execute EVERY cell this Notebook from the beginning \n","\n","%whos"],"metadata":{"id":"OBRU1f0vDVmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Takes far too long for inference, \n","#   currently not used\n","\n","# !pip install moepy"],"metadata":{"id":"MREjoHDm8juf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install dtaidistance"],"metadata":{"id":"1N-dEQbfjAQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sktime"],"metadata":{"id":"-tqkikuj286D"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mT-77aUWNOT_"},"outputs":[],"source":["# [RESTART RUNTIME] May be Required (only needed for Plotly)\n","\n","# Designed Security Hole in older version of PyYAML, must upgrade to use plotly\n","\n","# !pip install pyyaml==5.4.1"]},{"cell_type":"code","source":["# To Reduce Time Series Dimensionality\n","\n","!pip install lttb"],"metadata":{"id":"1EIEJMfQkODG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tslearn"],"metadata":{"id":"uggjF0ptYV50"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# [STEP 1] Manual Configuration"],"metadata":{"id":"_Y0sLmhmSisA"}},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bfkqjgMiw7T"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n","# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n","print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n","\n","print('\\n')\n","\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"],"metadata":{"id":"sbNX_gP790_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 2] Automatic Configuration/Setup**"],"metadata":{"id":"5iUxLgUCjpJg"}},{"cell_type":"markdown","metadata":{"id":"zeLft8mw7moD"},"source":["## (each time) Custom Libraries & Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPdbnOjw6ycy"},"outputs":[],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvMuohQZ6ycz"},"outputs":[],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n","global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n","\n","global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n","global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","from utils import sa_config # (e.g. define TEST_WORDS_LS)\n","\n","sa_config.set_globals()\n","\n","global_vars.TEST_WORDS_LS\n","print('\\n')\n","\n","dir(global_vars)"]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"C16iaUH5luqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and clean for each iteration of notebook\n","\n","# dir(global_vars)\n","\n","global_vars.corpus_texts_dt = {}\n","global_vars.corpus_titles_dt = {}"],"metadata":{"id":"Y2IRi-3z7moE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IU1IHzA7moE"},"outputs":[],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","source":["global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"oQn0v1jaxTj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.SUBDIR_SENTIMENT_CLEAN = './sentiment_clean/sentiemnt_clean_novels_new_corpus2/'\n","global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"nl1CFufcxZ-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW6YDyHT7moF"},"source":["## (each time) Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUveIcUOzYav"},"outputs":[],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n","\n","global_vars.corpus_titles_dt"]},{"cell_type":"code","source":["global_vars.models_titles_dt.items()"],"metadata":{"id":"xvZ-G0wvCpdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_titles_dt"],"metadata":{"id":"2VkkiGTyAqDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRb36wyH7moE"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjoIK5U_7moE"},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"code","source":["# Intentionally left blank"],"metadata":{"id":"FuWGvo26zAlM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ6eN87UjpJh"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm._tqdm_notebook import tqdm_notebook\n","import pandas as pd\n","tqdm_notebook.pandas()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)\n","\n","import json\n","from collections import Counter\n","\n","# from glob import glob\n","# import copy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoJdRfJh7FSz"},"outputs":[],"source":["# Scikit Utilities, Metrics, Pipelines and Models\n","\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n"]},{"cell_type":"markdown","source":["## (del?) Define Global Parameters"],"metadata":{"id":"6WV29ZjPjpJh"}},{"cell_type":"code","source":["\"\"\"\n","\n","# Define Globals\n","\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"],"metadata":{"id":"vhoFy1ysjpJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GM-ZzGwkjpJh"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YN--P4PgjpJh"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"-XHwZJgsjpJh"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n"],"metadata":{"id":"qMECX12r_CNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DONrzMXxAmYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X229IbToHwa2"},"source":["## Python Utility Functions"]},{"cell_type":"markdown","source":["### (each time) Generate Convenient Data Lists"],"metadata":{"id":"gjmkC1FbAEpo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"If9dQpsIAm_h"},"outputs":[],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"WYVphsv3l8kO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNlQr4_Ckb1"},"outputs":[],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.model_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.model_ensemble_dt['lexicon']"]},{"cell_type":"markdown","source":["### File Functions"],"metadata":{"id":"B3sXwZOq_-0d"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"],"metadata":{"id":"3JQBWKKcN2Eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fys3dkJSB656"},"source":["# **[STEP 3] Read all Raw Sentiment Data**\n","\n","\n"]},{"cell_type":"markdown","source":["## Read Raw Sentiments"],"metadata":{"id":"O82nw_wvJsz9"}},{"cell_type":"code","source":["# Verify cwd and subdir of Raw Sentiment Data\n","\n","print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_SENTIMENT_RAW}')\n","\n","PATH_SENTIMENT_RAW = f'{Path_to_SentimentArcs}sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","\n","print(f'\\nPATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\\n')\n","\n","print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n","!ls -1 $PATH_SENTIMENT_RAW\n","\n","# glob(f'{PATH_SENTIMENT_RAW}/*')\n","\n","print('\\n')\n","\n","print(corpus_texts_ls)"],"metadata":{"id":"Roq-2Ol8yH5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClL4-1Gqe7g"},"outputs":[],"source":["# Create a List (sentiment_raw_json_ls) of all preprocessed text files\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","try:\n","    sentiment_raw_json_ls = glob(f'{PATH_SENTIMENT_RAW}/sentiment_raw_*.json')\n","    sentiment_raw_json_ls = [x.split('/')[-1] for x in sentiment_raw_json_ls]\n","    # sentiment_raw_json_ls = [x.split('.')[0] for x in sentiment_raw_json_ls]\n","except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","print('\\n'.join(sentiment_raw_json_ls))\n","print('\\n')\n","print(f'Found {len(sentiment_raw_json_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n"]},{"cell_type":"code","source":["# Global Dict for Sentiments\n","\n","# Only used in this Notebook so not in defined in shared utils/global_vars\n","#   like global_vars.corpus_texts_dt = {}\n","\n","# corpus_sentiments_dt[text] = DataFrame(Raw Sentiments, 1 Column per Model)\n","\n","corpus_sentiment_dt = {}"],"metadata":{"id":"Kr9cd_FzkqwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"3kLkggXAqYfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_raw_json_ls"],"metadata":{"id":"wxXc1pzoFc5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATH_SENTIMENT_RAW"],"metadata":{"id":"TA2Y0YB9FkHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:   2m37s @09:32 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:07 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:09 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","\n","# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","# Reset Dict for Sentiments\n","#   Only used in this notebook, not shared across notebooks so do not\n","#   share via utils/global_vars like global_vars.corpus_texts_dt\n","\n","corpus_sentiment_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing text #{i}: {atext}')\n","  corpus_sentiment_dt[atext] = pd.DataFrame(columns=['text_raw','text_clean'])\n","\n","  for j, ajson in enumerate(sentiment_raw_json_ls):\n","    print(f'  Reading json #{j}: {ajson}')\n","\n","    afile_fullpath = f'{PATH_SENTIMENT_RAW}{ajson}'\n","    print(f'               at: {afile_fullpath}')\n","\n","    if 'transformer' in ajson:\n","      print(f'   One Model Transformer *.json datafile')\n","    else:\n","      print(f'   Multi-Model non-Transformer *.json datafile')\n","\n","    with open(afile_fullpath) as fp:\n","      json_dt = json.load(fp)\n","      temp_df = pd.DataFrame.from_dict(json_dt[atext]).reset_index()\n","      # temp_df.head(5)\n","      # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","      \n","      # corpus_sentiment_dt[atext]\n","      # print(f'               type: {json_dt[atext]}')\n","\n","    # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","    corpus_sentiment_dt[atext] = pd.concat([corpus_sentiment_dt[atext], temp_df], axis=1).T.drop_duplicates().T #  = corpus_sentiment_dt[atext].update(temp_df)\n","    # pd.concat([DF1, DF2], axis = 1).T.drop_duplicates().T\n","    # corpus_sentiment_dt[atext] = pd.DataFrame.from_dict(json_dt)\n","\n","  # ajson_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  # global_vars.corpus_texts_dt[atext] = ajson_df\n","  # corpus_sentiment_dt[atext] = ajson_df\n","\n","\n","  # a_json = json.loads(json_string)\n","  # print(a_json)\n","\n"],"metadata":{"id":"TvbL8B-cDTCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt.keys()"],"metadata":{"id":"FAjV8WvHHV_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_indx = 1\n","\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].head()\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].info()\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].shape\n","\n","print(f'For Text: {corpus_texts_ls[title_indx]}')"],"metadata":{"id":"UoH1ll-HHZZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Identify and Drop Duplicate Columns"],"metadata":{"id":"e3fBlTEw0fV_"}},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"TN9z8tHwzQni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext]['roberta15lg'].columns"],"metadata":{"id":"GWN5bu0g315t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop all but the i-th copy of duplicated column\n","\n","def keep_nthdup_col(adf, acol, nthcopy):\n","  '''\n","  Given a DataFrame, duplicated col name and nthcopy into set of duplicated cols\n","  Drop the iloc version of the duplicated col list from the DataFrame\n","  '''\n","\n","  df_col_iloc_ls = []\n","\n","  # First, verify this is a duplicated column\n","  col_dup_ls = [x for x in corpus_sentiment_dt[atext].columns if acol == x]\n","  if len(col_dup_ls) <= 1:\n","    print(f'ERROR: Column: {acol} is not duplicated in the DataFrame cols: {adf.columns}')\n","    return\n","\n","  # Loop over all columns to get original iloc of duplicated columns\n","  # corpus_sentiment_dt[atext].columns.get_loc('roberta15lg')  # Return List of booleans\n","\n","  for i in range(adf.shape[1]):\n","\n","    # get current col name\n","    acol_name = adf.columns[i]\n","\n","    # if current col name matches our target col, save it\n","    if acol_name == acol:\n","      # save the iloc\n","      df_col_iloc_ls.append(i)\n","\n","  # Second, verify iloc points to one of the duplicated columns\n","  if nthcopy >= len(df_col_iloc_ls):\n","    print(f'ERROR: passed nthcopy {nthcopy} is bigger than the number of duplicated {acol} column [0 to {len(df_col_iloc_ls)-1}]')\n","    return\n","\n","  print(f' Duplicated col: {acol} indicies: {df_col_iloc_ls}')\n","  col_dup_indx = df_col_iloc_ls[nthcopy]\n","  print(f'     Keep Index: {col_dup_indx}')\n","  print(f'           Name: {adf.columns[col_dup_indx]}')\n","  df_col_iloc_ls.remove(col_dup_indx)\n","  print(f'      Drop Cols: {df_col_iloc_ls}')\n","  # Drop all cols by iloc index in list df_col_iloc_ls\n","  # adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j not in df_col_iloc_ls]]\n","  for acol_indx in df_col_iloc_ls:\n","    adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j != int(acol_indx)]]\n","\n","  \"\"\"\n","  for k, acol_indx in enumerate(df_col_iloc_ls):\n","    acol_drop = adf.columns[acol_indx]\n","    print(f'Dropping column #{k}: {acol_drop} at indx={acol_indx}')\n","    # adf.drop(adf.columns[acol_indx], axis=1, inplace=True)\n","    adf.drop(columns=[acol_drop], axis=1, inplace=True)\n","  \"\"\"\n","\n","  return adf\n","\n","# Test\n","# keep_nthdup_col(corpus_sentiment_dt[atext], 'text_raw', 1)"],"metadata":{"id":"cgGVTMYY-Yr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].iloc[:, [j for j,c in enumerate(list(corpus_sentiment_dt[atext].columns)) if j not in [13,0]]].info()"],"metadata":{"id":"aw2hYCSdT1HL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify and Drop Duplicate Columns\n","\n","col_before_ct = len(corpus_sentiment_dt[atext].columns)\n","dup_col_keep_dt = {}  # Dict[dup_col] = iloc index to keep (col with min nulls)\n","\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  cols_dup_ls = []\n","  row_ct = corpus_sentiment_dt[atext].shape[0]\n","\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","  \n","  # Count the frequency of each column name\n","  cols_ls = corpus_sentiment_dt[atext].columns\n","  # print(f'  Columns: {cols_ls}')\n","  col_count_dt = Counter(cols_ls)\n","\n","  # Create list of duplicate column names in cols_dup_ls\n","  for key,val in col_count_dt.items():\n","    if val > 1:\n","      cols_dup_ls.append(key)\n","      print(f'  Duplicate col: {key} with count: {val}')\n","\n","  # Count how many columns are duplicated\n","  dup_ct = len(cols_dup_ls)\n","\n","  # For every duplicated Column\n","  for j, adup_col in enumerate(cols_dup_ls):\n","    # Count how many duplicates it has\n","    adup_col_ct = len(corpus_sentiment_dt[atext][adup_col])\n","\n","    # Iterate through all duplicates and find the iloc index of the one\n","    #   with the least number of null values as the one to keep (deleting the other dups)\n","    col_iloc_min_null = 0  # Index to the col with min nulls\n","    col_min_null_ct = row_ct  # Current count of null in col with min nulls, init to row count\n","    dup_col_ls = corpus_sentiment_dt[atext][adup_col].columns\n","    for k, adup_col_ver in enumerate(dup_col_ls):\n","      adup_col_null_ct = corpus_sentiment_dt[atext][adup_col].iloc[:,k].isna().sum()\n","      if adup_col_null_ct < col_min_null_ct:\n","        col_min_null_ct = adup_col_null_ct\n","        col_iloc_min_null = k\n","\n","    # Drop all but one copy of the duplicated columns\n","    print(f'\\n      Keep iloc: {col_iloc_min_null} in adup_col: {adup_col} with {adup_col_null_ct} nulls out of {row_ct}')\n","    dup_col_keep_dt[adup_col] = col_iloc_min_null\n","    print(f'       Calling: keep_nthdup_col(adf, {adup_col}, {col_iloc_min_null})')\n","    corpus_sentiment_dt[atext] = keep_nthdup_col(corpus_sentiment_dt[atext], adup_col, col_iloc_min_null)\n","\n","\n","col_after_ct = len(corpus_sentiment_dt[atext].columns)\n","\n","print(f'\\n\\nColumn Count:\\n  Before: {col_before_ct}\\n   After: {col_after_ct}')"],"metadata":{"id":"OBr-XWR9y2UQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].info()"],"metadata":{"id":"7eYAAjDuSkHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reorder and Specify dtypes"],"metadata":{"id":"PPrnvErgWaSC"}},{"cell_type":"code","source":["# Get list of models\n","\n","models_ls = list(set(corpus_sentiment_dt[corpus_texts_ls[0]].columns) - set(['text_raw','text_clean','index']))\n","models_ls.sort()\n","\n","models_ls\n","\n","print(f'\\n\\nTotal of {len(models_ls)} Models')"],"metadata":{"id":"HQxAu4eUVpU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Put text_raw and text_clean at front\n","\n","# corpus_sentiment_dt[atext].sort_index(axis=1)\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(0, 'text_raw', corpus_sentiment_dt[atext].pop('text_raw'))\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(1, 'text_clean', corpus_sentiment_dt[atext].pop('text_clean'))\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","\n","  col_first = corpus_sentiment_dt[atext].pop('index')\n","  corpus_sentiment_dt[atext].insert(0, 'sentence_no', col_first)\n","\n","  col_second = corpus_sentiment_dt[atext].pop('text_raw')\n","  corpus_sentiment_dt[atext].insert(1, 'text_raw', col_second)\n","\n","  col_third = corpus_sentiment_dt[atext].pop('text_clean')\n","  corpus_sentiment_dt[atext].insert(2, 'text_clean', col_third)\n","\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"XD7gqGHGWjj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert objects to more specific dtypes\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    print(f'Processing Model #{j}: {amodel}')\n","\n","    corpus_sentiment_dt[atext][amodel] = corpus_sentiment_dt[atext][amodel].astype('float')\n","\n","  corpus_sentiment_dt[atext]['sentence_no'] = corpus_sentiment_dt[atext]['sentence_no'].astype('int')\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"V30-aErXofrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify sample DataFrame\n","\n","corpus_sentiment_dt[corpus_texts_ls[0]].head()"],"metadata":{"id":"KYdBWtNspn6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verify Raw Plots"],"metadata":{"id":"OnVUg2Z5apcf"}},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"nOdp0EAfktep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"XWEHgHUTlTBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"6QZ-Ep5UmFpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls"],"metadata":{"id":"ip7B09v0YpYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = corpus_sentiment_dt['tmorrison_songofsolomon'][models_ls].rolling(300, center=True, min_periods=0).mean().plot()"],"metadata":{"id":"BM8dfDzbYrpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Raw Sentiments with \n","\n","win_per = 10\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  \n","  win_aper = int(win_per/100 * corpus_sentiment_dt[atext].shape[0])\n","  _ = corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","  _= plt.title(f'Sentiment Analysis\\n{global_vars.corpus_titles_dt[atext][0]}\\nSmoothed SMA ({win_per}%)')\n","  plt.grid(True)\n","\n","print(f'Read Raw Sentiments for these texts:\\n  {corpus_sentiment_dt.keys()}\\n\\n')\n","\n","\n"],"metadata":{"id":"XNRTSir8Db4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Drop or Interpolate and NaN/None Values"],"metadata":{"id":"1bIaaaVgmtV3"}},{"cell_type":"code","source":["corpus_sentiment_dt[atext]['roberta15lg']"],"metadata":{"id":"w3A1ZUCOwG0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop Columns/Models with %NaN above Threshold\n","\n","null_threshold = 0.9  # Drop Col if %rows=null > Threshold\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    # print(f'Processing Model #{j}: {amodel}')\n","\n","    row_ct = len(corpus_sentiment_dt[atext][amodel])\n","    sum_null = corpus_sentiment_dt[atext][amodel].isnull().sum()\n","    # print(f'There are {sum_null} null values of a total {row_ct} rows')\n","    null_threshold = 0.5  # if > 50% null, drop col\n","    # print(f'Threshold: {null_threshold} of all {row_ct} rows')\n","    if sum_null > int(null_threshold * row_ct):\n","      print(f'  %NaNs above Threshold={null_threshold}: {corpus_sentiment_dt[atext][amodel].isna().sum()}')\n","      # TODO: Verify before dropping Col/Model here\n","      # corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","\n"],"metadata":{"id":"s-ruBWYkmuwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clip Outliers and zScore Standardize"],"metadata":{"id":"mcbiCK2wm9aK"}},{"cell_type":"code","source":["import statsmodels.robust.scale as sm_robust"],"metadata":{"id":"mfhlcx0Qm9aK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, RobustScaler\n","\n","r_scaler = RobustScaler() \n","z_scaler = StandardScaler()"],"metadata":{"id":"W4wX7y5Ycwzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls[7]"],"metadata":{"id":"Qs5nk8CTb3Ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simple IQR\n","\n","def clip_iqr_outliers(floats_ser, iqr_limit=1.5):\n","  '''\n","  Given a Pandas Series of floats and an upper limit on IQR variance from the median\n","  Clip all outliers beyond the iqr_limit and return a list of floats\n","  '''\n","\n","  quantile10 = floats_ser.quantile(0.10)\n","  quantile90 = floats_ser.quantile(0.90)\n","  print(f'10% Quantile: {quantile10}')\n","  print(f'90% Quantile: {quantile90}')\n","\n","  floats_np = np.where(floats_ser < quantile10, quantile10, floats_ser)\n","  floats_np = np.where(floats_ser > quantile90, quantile90, floats_ser)\n","  print(f'        Skew: {pd.Series(floats_np).skew()}')\n","\n","  return floats_np # .tolist()\n","\n","# Test\n","\n","test_np = clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'])\n","len(test_np)"],"metadata":{"id":"6NaTReBnhXP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'].quantile(0.10)"],"metadata":{"id":"eME8b7khkfYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'],iqr_limit=1.5) # .values.reshape(-1, 1) )"],"metadata":{"id":"Ofr3nDIYj2-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']]"],"metadata":{"id":"3kJhxxYcnLEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls"],"metadata":{"id":"pJiADBMtqpxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns if 'rz' not in x]"],"metadata":{"id":"6yh0_nSbqiVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trim Outliers and zScore Standardize\n","\n","corpus_sentiment_rz_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  # atext_rz_df = corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']].copy(deep=True)\n","  # col_rzscores_ls = []\n","  print(f\"Title #{i}: {atext}\")\n","  # df = corpus_sentiment_dt[atext].copy()\n","  # numeric_cols_ls = list(corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns) # .remove('sentence_no')\n","  # numeric_cols_ls.remove('sentence_no')\n","\n","  # for anum_col_str in numeric_cols_ls:\n","  for j,amodel in enumerate(models_ls):\n","    print(f'  Model #{j}: {amodel}')\n","    # anum_col_robust_np = r_scaler.fit_transform(df[amodel].values.reshape(-1, 1) )\n","    arobust_col_np = clip_iqr_outliers(corpus_sentiment_dt[atext][amodel],iqr_limit=1.5)\n","    # scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))\n","    # arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np)\n","    arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np.reshape(-1,1))\n","    arobust_zscaled_col_str = f'{amodel}_rz'\n","    corpus_sentiment_dt[atext][arobust_zscaled_col_str] = pd.Series(arobust_zscaled_col_np.squeeze(-1,))\n","  # corpus_sentiment_rz_dt[atext] = atext_rz_df\n","\n","  # anum_col_rzscore_np = z_scaler.fit_transform(anum_col_robust_np)\n","  # anum_col_rzscore_str = f'{anum_col_str}_rzscore'\n","  # df[anum_col_rzscore_str] = pd.Series(anum_col_rzscore_np.squeeze(-1,))\n","  # col_rzscores_ls.append(anum_col_rzscore_str)\n","\n","  # print(f'df.columns: {df.columns}')\n","  # win_10per = int(0.10 * df.shape[0])\n","  # df[col_rzscores_ls].rolling(win_10per, center=True, min_periods=0).mean() # .plot(title=f\"Sentiment Analysis\\n{global_vars.corpus_texts_dt[atext][0]}\\nProcessing: SMA 10% (+ Robust IQR, zScore Scaling)\")"],"metadata":{"id":"S-BNslD6jepP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[x for x in corpus_sentiment_dt[atext] if 'rz' in x]"],"metadata":{"id":"WDLCas_DsDjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for atext in corpus_texts_ls:\n","  col_drop_ls = [x for x in corpus_sentiment_dt[atext] if 'rz' in x]\n","  print(f'Dropping: {len(col_drop_ls)} Columns\\n  {col_drop_ls}\\n\\n')\n","  corpus_sentiment_dt[atext].drop(columns=col_drop_ls, inplace=True)"],"metadata":{"id":"YszDtUe2q5pm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_rz_ls = [x for x in corpus_sentiment_dt[corpus_texts_ls[0]] if 'rz' in x]\n","models_rz_ls"],"metadata":{"id":"gQbJtE8eqK33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_indx = 0\n","text_str = corpus_texts_ls[text_indx]\n","title_str = global_vars.corpus_titles_dt[text_str][0]\n","win_per = 10\n","win_size = int(win_per/100 * corpus_sentiment_dt[text_str].shape[0])\n","\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].rolling(win_size, center=True, min_periods=0).mean().plot(alpha=0.3)\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].mean(axis=1).rolling(win_size, center=True, min_periods=0).mean().plot(label='mean', color='red', linewidth=3, alpha=0.7)\n","_ = plt.legend(loc='best')\n","_ = plt.title(f'Sentiment Arc: {title_str}\\nSmoothed SMA ({win_per}%)')\n","plt.grid(True)"],"metadata":{"id":"WrCMhm26jems"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"aod-A7AYvjc2"}},{"cell_type":"markdown","metadata":{"id":"9xFkzLf2H7lJ"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","source":["# TODO: Norm all paths and subdirs as 'dir/dir/dir/' except for root: '/dir/dir/dir/'\n","\n","global_vars.SUBDIR_SENTIMENT_CLEAN = 'sentiment_clean/sentiment_clean_novels_new_corpus2/'\n","\n","print(f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"wOr5EsrMyNqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","print(f'\\nSaving Text_Type: {Corpus_Genre}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nThese Text Titles:')\n","list(corpus_sentiment_dt.keys())\n","\n","print(f'\\n\\nTo This Subdirectory:\\n  {global_vars.SUBDIR_SENTIMENT_CLEAN}')\n","\n","full_path = f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}'\n","print(f'\\nFull path to this Subdirectory:\\n  {full_path}')\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all.json'\n","else:\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_reference_all.json'\n","print(f'\\nUnder this Filename:\\n  {save_filename}')\n","\n","write_dict_dfs(corpus_sentiment_dt, out_file=save_filename, out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"zcH7XyNYH7lJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify json file created\n","\n","!ls -altr $global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"8IhmigV8H7lK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 4] Smoothing EDA**"],"metadata":{"id":"u4uQbqBQ04DU"}},{"cell_type":"code","source":["from ipywidgets import interact, Dropdown, Select"],"metadata":{"id":"-6aTea3e4KHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_text = widgets.Dropdown(\n","    options=corpus_texts_ls,\n","    value=corpus_texts_ls[0],\n","    description='Text:',\n","    disabled=False,\n",")\n","selected_text\n","\n","selected_model = widgets.Dropdown(\n","    options=models_ls,\n","    value='roberta15lg',\n","    description='Model:',\n","    disabled=False,\n",")\n","selected_model"],"metadata":{"id":"T1YoQy6X7pz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA: Multiple SMA Window Sizes"],"metadata":{"id":"mlwCgYr57K6b"}},{"cell_type":"code","source":["win_1per = int(1/100 * corpus_sentiment_dt[selected_text.value].shape[0])\n","win_range_ls = [5,10,15,20]\n","\n","for i, awin_size in enumerate(win_range_ls):\n","  win_size = awin_size * win_1per\n","\n","  title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","\n","  _ = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean().plot()\n","  _ = plt.title(f'Sentiment Arc: {title_str}\\nModel: {selected_model.value}\\nSmoothing: SMA ({win_range_ls}%)')\n","  _ = plt.legend(loc='best')\n","  plt.grid(True)\n"],"metadata":{"id":"56HX1Di17KuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA: One SMA Window Size"],"metadata":{"id":"wsB9FBQb3XlP"}},{"cell_type":"code","source":["selected_text = widgets.Dropdown(\n","    options=corpus_texts_ls,\n","    value=corpus_texts_ls[0],\n","    description='Text:',\n","    disabled=False,\n",")\n","selected_text\n","\n","selected_model = widgets.Dropdown(\n","    options=models_ls,\n","    value='roberta15lg',\n","    description='Model:',\n","    disabled=False,\n",")\n","selected_model\n","\n","selected_sma_window = widgets.IntSlider(\n","    value=10,\n","    min=2, # max \n","    max=20, # min \n","    step=1, # step\n","    description='SMA Win%'\n",")\n","selected_sma_window"],"metadata":{"id":"gCbksMio4XDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["win_size = int(selected_sma_window.value/100 * corpus_sentiment_dt[selected_text.value].shape[0])\n","\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","\n","_ = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean().plot()\n","_ = plt.title(f'Sentiment Arc: {title_str}\\nModel: {selected_model.value}\\nSmoothing: SMA ({selected_sma_window.value}%)')\n","plt.grid(True)\n"],"metadata":{"id":"NFdQCbu663UZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LOWESS Smoothing"],"metadata":{"id":"RoXZ3aSg3ciw"}},{"cell_type":"code","source":["# Create DataFrame based on selected SentimentArc above\n","\n","df = pd.DataFrame(current_sentiment_arc_ser)\n","df.insert(0, 'sentence_no', corpus_sentiment_dt[atext]['sentence_no'])\n","df.insert(1, 'text_raw', corpus_sentiment_dt[atext]['text_raw'])\n","df.insert(2, 'text_clean', corpus_sentiment_dt[atext]['text_clean'])\n","df.head()"],"metadata":{"id":"FofWRQos8U8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.graph_objects as go # for data visualization\n","import plotly.express as px # for data visualization \n","import statsmodels.api as sm # to build a LOWESS model\n","from scipy.interpolate import interp1d # for interpolation of new data points"],"metadata":{"id":"2qw-yzN68Uzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a scatter plot\n","fig = px.scatter(df, x=df['sentence_no'], y=df['roberta15lg'], opacity=0.8, color_discrete_sequence=['black'])\n","\n","# Change chart background color\n","_ = fig.update_layout(dict(plot_bgcolor = 'white'))\n","\n","# Update axes lines\n","_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","# Set figure title\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_all_str = f'{title_str} ({selected_model.value} SMA {selected_sma_window.value}%)'\n","_ = fig.update_layout(title=dict(text=title_all_str, font=dict(color='black')))\n","\n","# Update marker size\n","_ = fig.update_traces(marker=dict(size=3))\n","\n","fig.show()"],"metadata":{"id":"jObjPCFL8n4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from statsmodels.nonparametric.smoothers_lowess import lowess"],"metadata":{"id":"blAY5y8v936s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"xhDU-onj-q-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------- Select variables -------\n","# y values for both\n","# y=df['roberta15lg'].values\n","y=df['roberta15lg'].values\n","\n","# x values for Linear Regression\n","# X=df['X3 distance to the nearest MRT station'].values.reshape(-1,1) # Note, we need X to be a 2D array, hence reshape\n","# x values for LOWESS\n","# x=df['sentence_no'].values \n","x=np.arange(df.shape[0])\n","\n","# ------- Linear Regression -------\n","# Define and fit the model\n","# model1 = LinearRegression()\n","# LR = model1.fit(X, y)\n","\n","# Predict a few points with Linear Regression model for the grpah\n","# Create 20 evenly spaced points from smallest X to largest X\n","# x_range = np.linspace(X.min(), X.max(), 20) \n","# Predict y values for our set of X values\n","# y_range = model1.predict(x_range.reshape(-1, 1))\n","\n","\n","# ------- LOWESS -------\n","# Generate y_hat values using lowess, try a couple values for hyperparameters\n","y_hat1 = lowess(y, x, frac=1/20) # note, default frac=2/3\n","y_hat2 = lowess(y, x, frac=1/30)"],"metadata":{"id":"Qdj_e2oa8n1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a scatter plot\n","_ = fig = px.scatter(df, x='sentence_no', y='roberta15lg', custom_data=['text_raw'], opacity=0.3) # , color_discrete_sequence=['black'], size=1)\n","\n","# Add the prediction line\n","# fig.add_traces(go.Scatter(x=x_range, y=y_range, name='Linear Regression', line=dict(color='limegreen')))\n","_ = fig.add_traces(go.Scatter(x=y_hat1[:,0], y=y_hat1[:,1], name='LOWESS, frac=1/20', line=dict(color='red')))\n","_ = fig.add_traces(go.Scatter(x=y_hat2[:,0], y=y_hat2[:,1], name='LOWESS, frac=1/30', line=dict(color='orange')))\n","\n","# Change chart background color\n","_ = fig.update_layout(dict(plot_bgcolor = 'white'))\n","\n","# Update axes lines\n","_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","# Set figure title\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_all_str = f'{title_str} ({selected_model.value} SMA {selected_sma_window.value}%)'\n","_ = fig.update_layout(title=dict(text=title_all_str, font=dict(color='black')))\n","\n","# Update marker size\n","_ = fig.update_traces(marker=dict(size=1))\n","\n","# _ = fig.update_traces(mode=\"markers+lines\", hovertemplate=None)\n","# _ = fig.update_layout(hovermode=\"x unified\")\n","\n","_ = fig.update_traces(\n","    hovertemplate=\"<br>\".join([\n","        \"Sentence No: %{x}\",\n","        \"Norm Sentiment: %{y}\",\n","        \"Text: %{customdata[0]}\",\n","    ])\n",")\n","\n","fig.show()"],"metadata":{"id":"ynZQ0Le18nx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**[NOTE] In Graph Above carefully roll over only the red & orange lines to view corresponding Text (not blue)**"],"metadata":{"id":"GxE9LcFCGFWg"}},{"cell_type":"markdown","source":["# **[STEP 5] Peak Detection & Crux Extraction**"],"metadata":{"id":"gcDJMzX31A5z"}},{"cell_type":"markdown","source":["## Peak Detection"],"metadata":{"id":"0i1PNjfZ3ggb"}},{"cell_type":"code","source":[""],"metadata":{"id":"CpUbwJ3t08XE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Crux Extraction"],"metadata":{"id":"va0_iRCT3iMe"}},{"cell_type":"code","source":[""],"metadata":{"id":"FCz_N8IS3kQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF NOTEBOOK**"]}],"metadata":{"colab":{"collapsed_sections":["BW6YDyHT7moF","CRb36wyH7moE","6WV29ZjPjpJh","GM-ZzGwkjpJh","-XHwZJgsjpJh"],"name":"sentiment_arcs_part6_analysis.ipynb","provenance":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}