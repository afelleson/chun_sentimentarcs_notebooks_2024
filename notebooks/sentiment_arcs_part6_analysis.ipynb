{"cells":[{"cell_type":"markdown","metadata":{"id":"ibHFmIWoU3Vx"},"source":["# **SentimentArcs (Part 6): Analysis**\n","\n","By: Jon Chun\n","* Original: 12 Jun 2021\n","* Last Update: 20 Apr 2022\n"]},{"cell_type":"markdown","source":["# [STEP 1] Manual Configuration"],"metadata":{"id":"_Y0sLmhmSisA"}},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bfkqjgMiw7T"},"outputs":[],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive')\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"novels\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"new\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 4 #@param {type:\"slider\", min:1, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","# PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_TEXT_RAW}'\n","# PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","PATH_SENTIMENT_RAW = f'./sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","# SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'PATH_SENTIMENT_RAW:\\n  [{PATH_SENTIMENT_RAW}]')\n","print(f'SUBDIR_SENTIMENT_RAW:\\n  [{SUBDIR_SENTIMENT_RAW}]')\n","\n","print('\\n')\n","\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')"],"metadata":{"id":"sbNX_gP790_M","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 2] Automatic Configuration/Setup**"],"metadata":{"id":"5iUxLgUCjpJg"}},{"cell_type":"markdown","metadata":{"id":"zeLft8mw7moD"},"source":["## (each time) Custom Libraries & Define Globals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPdbnOjw6ycy"},"outputs":[],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvMuohQZ6ycz"},"outputs":[],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_SENTIMENT_RAW = SUBDIR_SENTIMENT_RAW\n","global_vars.PATH_SENTIMENT_RAW = PATH_SENTIMENT_RAW\n","\n","global_vars.SUBDIR_TEXT_CLEAN = SUBDIR_TEXT_CLEAN\n","global_vars.PATH_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","from utils import sa_config # (e.g. define TEST_WORDS_LS)\n","\n","sa_config.set_globals()\n","\n","global_vars.TEST_WORDS_LS\n","print('\\n')\n","\n","dir(global_vars)"]},{"cell_type":"code","source":["%whos dict"],"metadata":{"id":"C16iaUH5luqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize and clean for each iteration of notebook\n","\n","# dir(global_vars)\n","\n","global_vars.corpus_texts_dt = {}\n","global_vars.corpus_titles_dt = {}"],"metadata":{"id":"Y2IRi-3z7moE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IU1IHzA7moE"},"outputs":[],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","source":["global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"oQn0v1jaxTj3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: correct typo in config file\n","\n","global_vars.SUBDIR_SENTIMENT_CLEAN = './sentiment_clean/sentiemnt_clean_novels_new_corpus2/'\n","global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"nl1CFufcxZ-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BW6YDyHT7moF"},"source":["## (each time) Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUveIcUOzYav"},"outputs":[],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n","\n","global_vars.corpus_titles_dt"]},{"cell_type":"code","source":["global_vars.models_titles_dt.items()"],"metadata":{"id":"xvZ-G0wvCpdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_vars.corpus_titles_dt"],"metadata":{"id":"2VkkiGTyAqDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRb36wyH7moE"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjoIK5U_7moE"},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"code","source":["# Intentionally left blank"],"metadata":{"id":"FuWGvo26zAlM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQ6eN87UjpJh"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm._tqdm_notebook import tqdm_notebook\n","import pandas as pd\n","tqdm_notebook.pandas()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","pd.set_option('max_colwidth', 100) # -1)\n","\n","from glob import glob\n","import json\n","from collections import Counter\n","\n","# import copy\n"]},{"cell_type":"code","source":["from IPython.display import Markdown, display\n","\n","from ipywidgets import interact, Dropdown, Select\n","\n","import plotly.graph_objects as go # for data visualization\n","import plotly.express as px # for data visualization "],"metadata":{"id":"KY3kXqWH8cM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, RobustScaler\n","\n","r_scaler = RobustScaler() \n","z_scaler = StandardScaler()\n"],"metadata":{"id":"W4wX7y5Ycwzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import statsmodels.robust.scale as sm_robust\n","# import statsmodels.api as sm # to build a LOWESS model\n","from statsmodels.nonparametric.smoothers_lowess import lowess"],"metadata":{"id":"mfhlcx0Qm9aK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.signal import find_peaks\n","from scipy import signal"],"metadata":{"id":"GTj-TTmeT8bX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from scipy.interpolate import interp1d # for interpolation of new data points"],"metadata":{"id":"ltZSk5pfWWxL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GM-ZzGwkjpJh"},"source":["## Setup Matplotlib Style\n","\n","* https://matplotlib.org/stable/tutorials/introductory/customizing.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YN--P4PgjpJh"},"outputs":[],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"-XHwZJgsjpJh"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')\n"],"metadata":{"id":"qMECX12r_CNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DONrzMXxAmYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X229IbToHwa2"},"source":["## Python Utility Functions"]},{"cell_type":"markdown","source":["### (each time) Generate Convenient Data Lists"],"metadata":{"id":"gjmkC1FbAEpo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"If9dQpsIAm_h"},"outputs":[],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"WYVphsv3l8kO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNlQr4_Ckb1"},"outputs":[],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.model_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.model_ensemble_dt['lexicon']"]},{"cell_type":"markdown","source":["### File Functions"],"metadata":{"id":"B3sXwZOq_-0d"}},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"],"metadata":{"id":"3JQBWKKcN2Eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fys3dkJSB656"},"source":["# **[STEP 3] Read all Raw Sentiment Data**\n","\n","\n"]},{"cell_type":"markdown","source":["## Read Raw Sentiments"],"metadata":{"id":"O82nw_wvJsz9"}},{"cell_type":"code","source":["# Verify cwd and subdir of Raw Sentiment Data\n","\n","print('Current Working Directory:')\n","!pwd\n","\n","print(f'\\nSubdir with all Cleaned Texts of Corpus:\\n  {SUBDIR_SENTIMENT_RAW}')\n","\n","PATH_SENTIMENT_RAW = f'{Path_to_SentimentArcs}sentiment_raw/{SUBDIR_SENTIMENT_RAW}'\n","\n","print(f'\\nPATH_SENTIMENT_RAW: {PATH_SENTIMENT_RAW}\\n')\n","\n","print(f'\\n\\nFilenames of Cleaned Texts:\\n')\n","!ls -1 $PATH_SENTIMENT_RAW\n","\n","# glob(f'{PATH_SENTIMENT_RAW}/*')\n","\n","print('\\n')\n","\n","print(corpus_texts_ls)"],"metadata":{"id":"Roq-2Ol8yH5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ClL4-1Gqe7g"},"outputs":[],"source":["# Create a List (sentiment_raw_json_ls) of all preprocessed text files\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","try:\n","    sentiment_raw_json_ls = glob(f'{PATH_SENTIMENT_RAW}/sentiment_raw_*.json')\n","    sentiment_raw_json_ls = [x.split('/')[-1] for x in sentiment_raw_json_ls]\n","    # sentiment_raw_json_ls = [x.split('.')[0] for x in sentiment_raw_json_ls]\n","except IndexError:\n","    raise RuntimeError('No csv file found')\n","\n","print('\\n'.join(sentiment_raw_json_ls))\n","print('\\n')\n","print(f'Found {len(sentiment_raw_json_ls)} Preprocessed files in {SUBDIR_TEXT_CLEAN}')\n"]},{"cell_type":"code","source":["# Global Dict for Sentiments\n","\n","# Only used in this Notebook so not in defined in shared utils/global_vars\n","#   like global_vars.corpus_texts_dt = {}\n","\n","# corpus_sentiments_dt[text] = DataFrame(Raw Sentiments, 1 Column per Model)\n","\n","corpus_sentiment_dt = {}"],"metadata":{"id":"Kr9cd_FzkqwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%whos list"],"metadata":{"id":"3kLkggXAqYfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_raw_json_ls"],"metadata":{"id":"wxXc1pzoFc5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATH_SENTIMENT_RAW"],"metadata":{"id":"TA2Y0YB9FkHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# NOTE:   2m37s @09:32 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:07 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m07s @10:09 on 20220416 Colab Pro CPU (634k, 668k, 909k)\n","#         2m03s @10:18 on 20220417 Colab Pro CPU (634k, 668k, 909k)\n","\n","\n","# Read all preprocessed text files into master DataFrame (corpus_dt)\n","\n","# Reset Dict for Sentiments\n","#   Only used in this notebook, not shared across notebooks so do not\n","#   share via utils/global_vars like global_vars.corpus_texts_dt\n","\n","corpus_sentiment_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing text #{i}: {atext}')\n","  corpus_sentiment_dt[atext] = pd.DataFrame(columns=['text_raw','text_clean'])\n","\n","  for j, ajson in enumerate(sentiment_raw_json_ls):\n","    print(f'  Reading json #{j}: {ajson}')\n","\n","    afile_fullpath = f'{PATH_SENTIMENT_RAW}{ajson}'\n","    print(f'               at: {afile_fullpath}')\n","\n","    if 'transformer' in ajson:\n","      print(f'   One Model Transformer *.json datafile')\n","    else:\n","      print(f'   Multi-Model non-Transformer *.json datafile')\n","\n","    with open(afile_fullpath) as fp:\n","      json_dt = json.load(fp)\n","      temp_df = pd.DataFrame.from_dict(json_dt[atext]).reset_index()\n","      # temp_df.head(5)\n","      # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","      \n","      # corpus_sentiment_dt[atext]\n","      # print(f'               type: {json_dt[atext]}')\n","\n","    # corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].update(temp_df)\n","    corpus_sentiment_dt[atext] = pd.concat([corpus_sentiment_dt[atext], temp_df], axis=1).T.drop_duplicates().T #  = corpus_sentiment_dt[atext].update(temp_df)\n","    # pd.concat([DF1, DF2], axis = 1).T.drop_duplicates().T\n","    # corpus_sentiment_dt[atext] = pd.DataFrame.from_dict(json_dt)\n","\n","  # ajson_df = pd.read_csv(afile_fullpath, index_col=[0])\n","  # global_vars.corpus_texts_dt[atext] = ajson_df\n","  # corpus_sentiment_dt[atext] = ajson_df\n","\n","\n","  # a_json = json.loads(json_string)\n","  # print(a_json)\n","\n"],"metadata":{"id":"TvbL8B-cDTCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt.keys()"],"metadata":{"id":"FAjV8WvHHV_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["title_indx = 0\n","\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].info()\n","\n","corpus_sentiment_dt[corpus_texts_ls[title_indx]].head()\n","\n","print(f'For Text: {corpus_texts_ls[title_indx]}')"],"metadata":{"id":"UoH1ll-HHZZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Identify and Drop Duplicate Columns"],"metadata":{"id":"e3fBlTEw0fV_"}},{"cell_type":"code","source":["# Drop all but the i-th copy of duplicated column\n","\n","def keep_nthdup_col(adf, acol, nthcopy):\n","  '''\n","  Given a DataFrame, duplicated col name and nthcopy into set of duplicated cols\n","  Drop the iloc version of the duplicated col list from the DataFrame\n","  '''\n","\n","  df_col_iloc_ls = []\n","\n","  # First, verify this is a duplicated column\n","  col_dup_ls = [x for x in corpus_sentiment_dt[atext].columns if acol == x]\n","  if len(col_dup_ls) <= 1:\n","    print(f'ERROR: Column: {acol} is not duplicated in the DataFrame cols: {adf.columns}')\n","    return\n","\n","  # Loop over all columns to get original iloc of duplicated columns\n","  # corpus_sentiment_dt[atext].columns.get_loc('roberta15lg')  # Return List of booleans\n","\n","  for i in range(adf.shape[1]):\n","\n","    # get current col name\n","    acol_name = adf.columns[i]\n","\n","    # if current col name matches our target col, save it\n","    if acol_name == acol:\n","      # save the iloc\n","      df_col_iloc_ls.append(i)\n","\n","  # Second, verify iloc points to one of the duplicated columns\n","  if nthcopy >= len(df_col_iloc_ls):\n","    print(f'ERROR: passed nthcopy {nthcopy} is bigger than the number of duplicated {acol} column [0 to {len(df_col_iloc_ls)-1}]')\n","    return\n","\n","  print(f' Duplicated col: {acol} indicies: {df_col_iloc_ls}')\n","  col_dup_indx = df_col_iloc_ls[nthcopy]\n","  print(f'     Keep Index: {col_dup_indx}')\n","  print(f'           Name: {adf.columns[col_dup_indx]}')\n","  df_col_iloc_ls.remove(col_dup_indx)\n","  print(f'      Drop Cols: {df_col_iloc_ls}')\n","  # Drop all cols by iloc index in list df_col_iloc_ls\n","  # adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j not in df_col_iloc_ls]]\n","  for acol_indx in df_col_iloc_ls:\n","    adf = adf.iloc[:, [j for j, c in enumerate(list(adf.columns)) if j != int(acol_indx)]]\n","\n","  \"\"\"\n","  for k, acol_indx in enumerate(df_col_iloc_ls):\n","    acol_drop = adf.columns[acol_indx]\n","    print(f'Dropping column #{k}: {acol_drop} at indx={acol_indx}')\n","    # adf.drop(adf.columns[acol_indx], axis=1, inplace=True)\n","    adf.drop(columns=[acol_drop], axis=1, inplace=True)\n","  \"\"\"\n","\n","  return adf\n","\n","# Test\n","# keep_nthdup_col(corpus_sentiment_dt[atext], 'text_raw', 1)"],"metadata":{"id":"cgGVTMYY-Yr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].iloc[:, [j for j,c in enumerate(list(corpus_sentiment_dt[atext].columns)) if j not in [13,0]]].info()"],"metadata":{"id":"aw2hYCSdT1HL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Identify and Drop Duplicate Columns\n","\n","col_before_ct = len(corpus_sentiment_dt[atext].columns)\n","dup_col_keep_dt = {}  # Dict[dup_col] = iloc index to keep (col with min nulls)\n","\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  cols_dup_ls = []\n","  row_ct = corpus_sentiment_dt[atext].shape[0]\n","\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","  \n","  # Count the frequency of each column name\n","  cols_ls = corpus_sentiment_dt[atext].columns\n","  # print(f'  Columns: {cols_ls}')\n","  col_count_dt = Counter(cols_ls)\n","\n","  # Create list of duplicate column names in cols_dup_ls\n","  for key,val in col_count_dt.items():\n","    if val > 1:\n","      cols_dup_ls.append(key)\n","      print(f'  Duplicate col: {key} with count: {val}')\n","\n","  # Count how many columns are duplicated\n","  dup_ct = len(cols_dup_ls)\n","\n","  # For every duplicated Column\n","  for j, adup_col in enumerate(cols_dup_ls):\n","    # Count how many duplicates it has\n","    adup_col_ct = len(corpus_sentiment_dt[atext][adup_col])\n","\n","    # Iterate through all duplicates and find the iloc index of the one\n","    #   with the least number of null values as the one to keep (deleting the other dups)\n","    col_iloc_min_null = 0  # Index to the col with min nulls\n","    col_min_null_ct = row_ct  # Current count of null in col with min nulls, init to row count\n","    dup_col_ls = corpus_sentiment_dt[atext][adup_col].columns\n","    for k, adup_col_ver in enumerate(dup_col_ls):\n","      adup_col_null_ct = corpus_sentiment_dt[atext][adup_col].iloc[:,k].isna().sum()\n","      if adup_col_null_ct < col_min_null_ct:\n","        col_min_null_ct = adup_col_null_ct\n","        col_iloc_min_null = k\n","\n","    # Drop all but one copy of the duplicated columns\n","    print(f'\\n      Keep iloc: {col_iloc_min_null} in adup_col: {adup_col} with {adup_col_null_ct} nulls out of {row_ct}')\n","    dup_col_keep_dt[adup_col] = col_iloc_min_null\n","    print(f'       Calling: keep_nthdup_col(adf, {adup_col}, {col_iloc_min_null})')\n","    corpus_sentiment_dt[atext] = keep_nthdup_col(corpus_sentiment_dt[atext], adup_col, col_iloc_min_null)\n","\n","\n","col_after_ct = len(corpus_sentiment_dt[atext].columns)\n","\n","print(f'\\n\\nColumn Count:\\n  Before: {col_before_ct}\\n   After: {col_after_ct}')"],"metadata":{"id":"OBr-XWR9y2UQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext].info()"],"metadata":{"id":"7eYAAjDuSkHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reorder and Specify dtypes"],"metadata":{"id":"PPrnvErgWaSC"}},{"cell_type":"code","source":["# Get list of models\n","\n","models_ls = list(set(corpus_sentiment_dt[corpus_texts_ls[0]].columns) - set(['text_raw','text_clean','index']))\n","models_ls.sort()\n","\n","models_ls\n","\n","print(f'\\n\\nTotal of {len(models_ls)} Models')"],"metadata":{"id":"HQxAu4eUVpU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Put text_raw and text_clean at front\n","\n","# corpus_sentiment_dt[atext].sort_index(axis=1)\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(0, 'text_raw', corpus_sentiment_dt[atext].pop('text_raw'))\n","# corpus_sentiment_dt[atext] = corpus_sentiment_dt[atext].insert(1, 'text_clean', corpus_sentiment_dt[atext].pop('text_clean'))\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","\n","  col_first = corpus_sentiment_dt[atext].pop('index')\n","  corpus_sentiment_dt[atext].insert(0, 'sentence_no', col_first)\n","\n","  col_second = corpus_sentiment_dt[atext].pop('text_raw')\n","  corpus_sentiment_dt[atext].insert(1, 'text_raw', col_second)\n","\n","  col_third = corpus_sentiment_dt[atext].pop('text_clean')\n","  corpus_sentiment_dt[atext].insert(2, 'text_clean', col_third)\n","\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"XD7gqGHGWjj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert objects to more specific dtypes\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    print(f'Processing Model #{j}: {amodel}')\n","\n","    corpus_sentiment_dt[atext][amodel] = corpus_sentiment_dt[atext][amodel].astype('float')\n","\n","  corpus_sentiment_dt[atext]['sentence_no'] = corpus_sentiment_dt[atext]['sentence_no'].astype('int')\n","  corpus_sentiment_dt[atext].info()"],"metadata":{"id":"V30-aErXofrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify sample DataFrame\n","\n","corpus_sentiment_dt[corpus_texts_ls[0]].head()"],"metadata":{"id":"KYdBWtNspn6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verify Raw Plots"],"metadata":{"id":"OnVUg2Z5apcf"}},{"cell_type":"code","source":["global_vars.corpus_titles_dt.keys()"],"metadata":{"id":"6QZ-Ep5UmFpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls"],"metadata":{"id":"ip7B09v0YpYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify Raw Sentiments with \n","\n","win_per = 10\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  \n","  win_aper = int(win_per/100 * corpus_sentiment_dt[atext].shape[0])\n","  _ = corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","  _= plt.title(f'Sentiment Analysis\\n{global_vars.corpus_titles_dt[atext][0]}\\nSmoothed SMA ({win_per}%)')\n","  plt.grid(True)\n","\n","print(f'Read Raw Sentiments for these texts:\\n  {corpus_sentiment_dt.keys()}\\n\\n')\n","\n","\n"],"metadata":{"id":"XNRTSir8Db4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Drop or Interpolate and NaN/None Values"],"metadata":{"id":"1bIaaaVgmtV3"}},{"cell_type":"code","source":["corpus_sentiment_dt[atext]['roberta15lg']"],"metadata":{"id":"w3A1ZUCOwG0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop Columns/Models with %NaN above Threshold\n","\n","null_threshold = 0.9  # Drop Col if %rows=null > Threshold\n","\n","for i,atext in enumerate(corpus_texts_ls):\n","  print(f'\\n\\nProcessing Text #{i}: {atext}')\n","\n","  for j, amodel in enumerate(models_ls):\n","  \n","    # print(f'Processing Model #{j}: {amodel}')\n","\n","    row_ct = len(corpus_sentiment_dt[atext][amodel])\n","    sum_null = corpus_sentiment_dt[atext][amodel].isnull().sum()\n","    # print(f'There are {sum_null} null values of a total {row_ct} rows')\n","    null_threshold = 0.5  # if > 50% null, drop col\n","    # print(f'Threshold: {null_threshold} of all {row_ct} rows')\n","    if sum_null > int(null_threshold * row_ct):\n","      print(f'  %NaNs above Threshold={null_threshold}: {corpus_sentiment_dt[atext][amodel].isna().sum()}')\n","      # TODO: Verify before dropping Col/Model here\n","      # corpus_sentiment_dt[atext][models_ls].rolling(win_aper, center=True, min_periods=0).mean().plot()\n","\n"],"metadata":{"id":"s-ruBWYkmuwS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clip Outliers and zScore Standardize"],"metadata":{"id":"mcbiCK2wm9aK"}},{"cell_type":"code","source":["# Simple IQR\n","\n","def clip_iqr_outliers(floats_ser, iqr_limit=1.5):\n","  '''\n","  Given a Pandas Series of floats and an upper limit on IQR variance from the median\n","  Clip all outliers beyond the iqr_limit and return a list of floats\n","  '''\n","\n","  quantile10 = floats_ser.quantile(0.10)\n","  quantile90 = floats_ser.quantile(0.90)\n","  print(f'10% Quantile: {quantile10}')\n","  print(f'90% Quantile: {quantile90}')\n","\n","  floats_np = np.where(floats_ser < quantile10, quantile10, floats_ser)\n","  floats_np = np.where(floats_ser > quantile90, quantile90, floats_ser)\n","  print(f'        Skew: {pd.Series(floats_np).skew()}')\n","\n","  return floats_np # .tolist()\n","\n","# Test\n","\n","test_np = clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'])\n","len(test_np)"],"metadata":{"id":"6NaTReBnhXP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'].quantile(0.10)"],"metadata":{"id":"eME8b7khkfYK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clip_iqr_outliers(corpus_sentiment_dt[corpus_texts_ls[0]]['roberta15lg'],iqr_limit=1.5) # .values.reshape(-1, 1) )"],"metadata":{"id":"Ofr3nDIYj2-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']]"],"metadata":{"id":"3kJhxxYcnLEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_ls = [x for x in corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns if 'rz' not in x]\n","models_ls"],"metadata":{"id":"pJiADBMtqpxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Trim Outliers, zScore Standardize and Create 'rz' versions\n","\n","corpus_sentiment_rz_dt = {}\n","\n","for i, atext in enumerate(corpus_texts_ls):\n","  # atext_rz_df = corpus_sentiment_dt[atext][['sentence_no', 'text_raw', 'text_clean']].copy(deep=True)\n","  # col_rzscores_ls = []\n","  print(f\"Title #{i}: {atext}\")\n","  # df = corpus_sentiment_dt[atext].copy()\n","  # numeric_cols_ls = list(corpus_sentiment_dt[atext].select_dtypes(include=[np.number]).columns) # .remove('sentence_no')\n","  # numeric_cols_ls.remove('sentence_no')\n","\n","  # for anum_col_str in numeric_cols_ls:\n","  for j,amodel in enumerate(models_ls):\n","    print(f'  Model #{j}: {amodel}')\n","    # anum_col_robust_np = r_scaler.fit_transform(df[amodel].values.reshape(-1, 1) )\n","    arobust_col_np = clip_iqr_outliers(corpus_sentiment_dt[atext][amodel],iqr_limit=1.5)\n","    # scaler_zscore.fit_transform(np.array(corpus_texts_dt[atext][amodel_rstd]).reshape(-1,1))\n","    # arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np)\n","    arobust_zscaled_col_np = z_scaler.fit_transform(arobust_col_np.reshape(-1,1))\n","    arobust_zscaled_col_str = f'{amodel}_rz'\n","    corpus_sentiment_dt[atext][arobust_zscaled_col_str] = pd.Series(arobust_zscaled_col_np.squeeze(-1,))\n","  # corpus_sentiment_rz_dt[atext] = atext_rz_df\n","\n","  # anum_col_rzscore_np = z_scaler.fit_transform(anum_col_robust_np)\n","  # anum_col_rzscore_str = f'{anum_col_str}_rzscore'\n","  # df[anum_col_rzscore_str] = pd.Series(anum_col_rzscore_np.squeeze(-1,))\n","  # col_rzscores_ls.append(anum_col_rzscore_str)\n","\n","  # print(f'df.columns: {df.columns}')\n","  # win_10per = int(0.10 * df.shape[0])\n","  # df[col_rzscores_ls].rolling(win_10per, center=True, min_periods=0).mean() # .plot(title=f\"Sentiment Analysis\\n{global_vars.corpus_texts_dt[atext][0]}\\nProcessing: SMA 10% (+ Robust IQR, zScore Scaling)\")"],"metadata":{"id":"S-BNslD6jepP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_rz_ls = [f'{x}_rz' for x in models_ls]\n","models_rz_ls "],"metadata":{"id":"WDLCas_DsDjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_rz_ls = [x for x in corpus_sentiment_dt[corpus_texts_ls[0]] if 'rz' in x and 'rz_rz' not in x]\n","models_rz_ls = [x for x in models_rz_ls if 'sentence_no' not in x]\n","models_rz_ls"],"metadata":{"id":"gQbJtE8eqK33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sma_percent = widgets.IntSlider(\n","    value=10,\n","    min=2,\n","    max=20,\n","    step=1,\n","    description='Percent:',\n","    disabled=False,\n","    continuous_update=False,\n","    orientation='horizontal',\n","    readout=True,\n","    readout_format='d'\n",")\n","print('Select Smoothing SMA Window Size:')\n","sma_percent"],"metadata":{"id":"Ief0-GzxFt9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify SMA Sentiment Arcs\n","\n","text_indx = 0\n","text_str = corpus_texts_ls[text_indx]\n","title_str = global_vars.corpus_titles_dt[text_str][0]\n","win_per = sma_percent.value\n","win_size = int(win_per/100 * corpus_sentiment_dt[text_str].shape[0])\n","\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].rolling(win_size, center=True, min_periods=0).mean().plot(alpha=0.3)\n","_ = corpus_sentiment_dt[text_str][models_rz_ls].mean(axis=1).rolling(win_size, center=True, min_periods=0).mean().plot(label='mean', color='red', linewidth=3, alpha=0.7)\n","_ = plt.legend(loc='best')\n","_ = plt.title(f'Sentiment Arc: {title_str}\\nSmoothed SMA ({win_per}%)')\n","plt.grid(True)"],"metadata":{"id":"WrCMhm26jems"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xFkzLf2H7lJ"},"source":["### **Save Checkpoint**"]},{"cell_type":"code","source":["# TODO: Norm all paths and subdirs as 'dir/dir/dir/' except for root: '/dir/dir/dir/'\n","\n","if Corpus_Type == 'new':\n","  global_vars.SUBDIR_SENTIMENT_CLEAN = f'sentiment_clean/sentiment_clean_{Corpus_Genre}_new_corpus{Corpus_Number}/'\n","elif Corpus_Type == 'reference':\n","  global_vars.SUBDIR_SENTIMENT_CLEAN = f'sentiment_clean/sentiment_clean_{Corpus_Genre}_reference/'\n","else:\n","  print(f'ERROR: Illegal value Corpus_Genre: {Corpus_Genre}')\n","\n","print(f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"wOr5EsrMyNqf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","print(f'\\nSaving Text_Type: {Corpus_Genre}')\n","print(f'     Corpus_Type: {Corpus_Type}')\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nThese Text Titles:')\n","list(corpus_sentiment_dt.keys())\n","\n","print(f'\\n\\nTo This Subdirectory:\\n  {global_vars.SUBDIR_SENTIMENT_CLEAN}')\n","\n","full_path = f'{Path_to_SentimentArcs}{global_vars.SUBDIR_SENTIMENT_CLEAN}'\n","print(f'\\nFull path to this Subdirectory:\\n  {full_path}')\n","\n","if Corpus_Type == 'new':\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}_all.json'\n","else:\n","  save_filename = f'sentiment_clean_{Corpus_Genre}_{Corpus_Type}_reference_all.json'\n","print(f'\\nUnder this Filename:\\n  {save_filename}')\n","\n","write_dict_dfs(corpus_sentiment_dt, out_file=save_filename, out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}/')  # broken with novels_corpus4\n","# write_dict_dfs(corpus_sentiment_dt, out_file=f'{save_filename}', out_dir=f'{global_vars.SUBDIR_SENTIMENT_CLEAN}')"],"metadata":{"id":"zcH7XyNYH7lJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify json file created\n","\n","!ls -altr $global_vars.SUBDIR_SENTIMENT_CLEAN"],"metadata":{"id":"8IhmigV8H7lK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **[STEP 4] Smoothing EDA**"],"metadata":{"id":"u4uQbqBQ04DU"}},{"cell_type":"code","source":["selected_text = widgets.Dropdown(\n","    options=corpus_texts_ls,\n","    value=corpus_texts_ls[0],\n","    description='Text:',\n","    disabled=False,\n",")\n","selected_text\n","\n","selected_model = widgets.Dropdown(\n","    options=models_ls,\n","    value='roberta15lg',\n","    description='Model:',\n","    disabled=False,\n",")\n","selected_model"],"metadata":{"id":"T1YoQy6X7pz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA: Multiple SMA Window Sizes"],"metadata":{"id":"mlwCgYr57K6b"}},{"cell_type":"code","source":["_ = plt.figure(figsize=(20,10))\n","\n","win_1per = int(1/100 * corpus_sentiment_dt[selected_text.value].shape[0])\n","win_range_ls = [5,10,15,20]\n","\n","for i, awin_size in enumerate(win_range_ls):\n","  win_size = awin_size * win_1per\n","\n","  title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","  \n","  _ = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean().plot(label=win_size)\n","  _ = plt.title(f'Sentiment Arc: {title_str}\\nModel: {selected_model.value}\\nSmoothing: SMA ({win_range_ls}%)')\n","  _ = plt.legend(loc='best', title='Window Size')\n","  plt.grid(True)\n"],"metadata":{"id":"56HX1Di17KuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA: Multiple Sentiment Arcs"],"metadata":{"id":"NEKkNVa50_xJ"}},{"cell_type":"code","source":["# models_selected_ls = [widgets.Checkbox(value=False, description=label) for label in models_ls if 'sentence' not in x]\n","# models_selected_ls"],"metadata":{"id":"DXI2HTrc7wMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(corpus_texts_ls)"],"metadata":{"id":"YGzdCcEg0glg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select which Models to Compare\n","\n","print(f'Select which Models to compare (at least one):')\n","models_ls.sort()\n","models_nosentno_ls = [x for x in models_ls if 'sentence_no' not in x]\n","models_selected_ls = [widgets.Checkbox(value=True, description=label) for label in models_nosentno_ls]\n","models_sel_vbox = widgets.VBox(children=models_selected_ls)\n","display(models_sel_vbox)\n","\n","\n","display(Markdown('---'))\n","print('Select which Text to Analyze:')\n","if (len(corpus_texts_ls) <= 1):\n","  print(f'  {corpus_texts_ls[0]} (there is only one Text in this Corpus)')\n","else:\n","  corpus_texts_ls.sort()\n","  selected_text = widgets.Dropdown(\n","      options=corpus_texts_ls,\n","      value=corpus_texts_ls[0],\n","      description='Model:',\n","      disabled=False,\n","  )\n","\n","display(Markdown('---'))\n","print('Select a SMA Window Size:')\n","selected_sma_window = widgets.IntSlider(\n","    value=10,\n","    min=2, # max \n","    max=20, # min \n","    step=1, # step\n","    description='Percent'\n",")\n","selected_sma_window"],"metadata":{"id":"Og0Q3lvn0_d3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models_subset_ls = []\n","\n","for i in range(0, len(models_selected_ls)):\n","  if models_selected_ls[i].value == True:\n","    models_subset_ls.append(models_selected_ls[i].description + '_rz')\n","\n","print(f'Selected Modesl:\\n')\n","\n","models_subset_ls"],"metadata":{"id":"OCocerqQ0_XP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot Selected Normed Models for Chosen Text (using SMA Smoothing)\n","\n","temp_df = pd.DataFrame(corpus_sentiment_dt[selected_text.value].rolling(win_size, center=True, min_periods=0).mean())\n","temp_df['sentence_no'] = corpus_sentiment_dt[selected_text.value]['sentence_no']\n","temp_df['text_raw'] = corpus_sentiment_dt[selected_text.value]['text_raw']\n","\n","_ = fig = px.line(temp_df, x='sentence_no', y=models_subset_ls, custom_data=['text_raw']) #  labels={'x':'Sentence No', 'y':'Sentiment'})\n","\n","# Update axes lines\n","_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_full_str = f'SentimentArcs: {title_str}<br>Ensemble of {len(models_subset_ls)} with {selected_sma_window.value}% SMA'\n","\n","_ = fig.update_layout(\n","    plot_bgcolor='white',\n","    title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),\n","    xaxis_title=\"Line No\",\n","    yaxis_title='Sentiment',\n","    xaxis=dict(showgrid=True), \n","    yaxis=dict(showgrid=True),\n","    legend=dict(\n","    title = \"Models:\",\n","    orientation=\"h\",\n","    xanchor=\"center\",\n","    yanchor=\"bottom\",\n","    y=-0.3, # 0.99,\n","    x=0.5 # x=0.01\n","))\n","\n","\n","_ = fig.update_traces(\n","    hovertemplate=\"<br>\".join([\n","        \"Sentence No: %{x}\",\n","        \"Norm Sentiment: %{y}\",\n","        \"Text: %{customdata[0]}\",\n","    ])\n",")\n","\n","# Show plot \n","fig.show()"],"metadata":{"id":"dTdiPm2p5oze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EDA: One SMA Window Size"],"metadata":{"id":"wsB9FBQb3XlP"}},{"cell_type":"code","source":["selected_text = widgets.Dropdown(\n","    options=corpus_texts_ls,\n","    value=corpus_texts_ls[0],\n","    description='Text:',\n","    disabled=False,\n",")\n","selected_text\n","\n","selected_model = widgets.Dropdown(\n","    options=models_ls,\n","    value='roberta15lg',\n","    description='Model:',\n","    disabled=False,\n",")\n","selected_model\n","\n","selected_sma_window = widgets.IntSlider(\n","    value=10,\n","    min=2, # max \n","    max=20, # min \n","    step=1, # step\n","    description='SMA Win%'\n",")\n","selected_sma_window"],"metadata":{"id":"gCbksMio4XDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Interactive Plotly Graph\n","\n","_ = plt.figure(figsize=(20,10))\n","\n","win_size = int(selected_sma_window.value/100 * corpus_sentiment_dt[selected_text.value].shape[0])\n","\n","current_sentiment_arc_df = pd.DataFrame(corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean())\n","current_sentiment_arc_df['sentence_no'] = corpus_sentiment_dt[selected_text.value]['sentence_no']\n","current_sentiment_arc_df['text_raw'] = corpus_sentiment_dt[selected_text.value]['text_raw']\n","\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} with {selected_sma_window.value}% SMA'\n","\n","_ = fig = px.line(current_sentiment_arc_df, x='sentence_no', y=selected_model.value, custom_data=['text_raw'])\n","\n","# Update axes lines\n","_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_layout(\n","    plot_bgcolor='white',\n","    title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),\n","    xaxis_title=\"Line No\",\n","    yaxis_title='Sentiment',\n","    xaxis=dict(showgrid=True), \n","    yaxis=dict(showgrid=True),\n","    legend=dict(\n","    title = \"Models:\",\n","    orientation=\"h\",\n","    xanchor=\"center\",\n","    yanchor=\"bottom\",\n","    y=-0.3, # 0.99,\n","    x=0.5 # x=0.01\n","))\n","\n","_ = fig.update_traces(\n","    hovertemplate=\"<br>\".join([\n","        \"Sentence No: %{x}\",\n","        \"Norm Sentiment: %{y}\",\n","        \"Text: %{customdata[0]}\",\n","    ])\n",")\n","\n","fig.show();\n"],"metadata":{"id":"_wyTMCvusuMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LOWESS Smoothing"],"metadata":{"id":"RoXZ3aSg3ciw"}},{"cell_type":"code","source":["# Reorder DataFrame based on selected SentimentArc above\n","#   add SMA with Window Per selected above, and reorder columns\n","\n","current_sentiment_arc_df = current_sentiment_arc_df[['sentence_no','text_raw', selected_model.value]]\n","current_sentiment_arc_df['sma'] = corpus_sentiment_dt[selected_text.value][selected_model.value].rolling(win_size, center=True, min_periods=0).mean()\n","# current_sentiment_arc_df.head()\n","\n","# Create pointer alias to save typing/make more legible\n","df = current_sentiment_arc_df\n","df.head()"],"metadata":{"id":"py1N_KhNcwi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute LOWESS \n","\n","# ------- Select variables -------\n","# y values for both\n","# y=df['roberta15lg'].values\n","y=df[selected_model.value].values\n","\n","# x values for Linear Regression\n","# X=df['X3 distance to the nearest MRT station'].values.reshape(-1,1) # Note, we need X to be a 2D array, hence reshape\n","# x values for LOWESS\n","# x=df['sentence_no'].values \n","x=np.arange(df.shape[0])\n","\n","# ------- Linear Regression -------\n","# Define and fit the model\n","# model1 = LinearRegression()\n","# LR = model1.fit(X, y)\n","\n","# Predict a few points with Linear Regression model for the grpah\n","# Create 20 evenly spaced points from smallest X to largest X\n","# x_range = np.linspace(X.min(), X.max(), 20) \n","# Predict y values for our set of X values\n","# y_range = model1.predict(x_range.reshape(-1, 1))\n","\n","\n","# ------- LOWESS -------\n","# Generate y_hat values using lowess, try a couple values for hyperparameters\n","y_hat1 = lowess(y, x, frac=1/20) # note, default frac=2/3\n","y_hat2 = lowess(y, x, frac=1/30)\n","\n","# Save into \n","current_sentiment_arc_df['lowess20'] = y_hat1[:,1].tolist()\n","current_sentiment_arc_df['lowess30'] = y_hat2[:,1].tolist()\n","current_sentiment_arc_df.head()"],"metadata":{"id":"Qdj_e2oa8n1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure df reflects updates to underlying current_sentiment_arc_df DataFrame\n","\n","df.head()"],"metadata":{"id":"j2F_3ST-fsY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a scatter plot\n","\n","# _ = fig = px.scatter(df, x='sentence_no', y=selected_model.value, custom_data=['text_raw'], opacity=0.3, color_discrete_sequence=['black']) # , size=1)\n","_ = fig = px.scatter(df, x='sentence_no', y=[selected_model.value, 'lowess20', 'lowess30'], custom_data=['text_raw'], opacity=0.7) # , color_discrete_sequence=['black']) # , size=1)\n","\n","# Add the prediction line\n","# fig.add_traces(go.Scatter(x=x_range, y=y_range, name='Linear Regression', line=dict(color='limegreen')))\n","# _ = fig.add_traces(go.Scatter(x=y_hat1[:,0], y=y_hat1[:,1], name='LOWESS, frac=1/20', line=dict(color='red')))\n","# _ = fig.add_traces(go.Scatter(x=y_hat2[:,0], y=y_hat2[:,1], name='LOWESS, frac=1/30', line=dict(color='orange')))\n","\n","# _ = fig.add_traces(go.Scatter(x=df['sentence_no'], y=df['lowess20'], name='LOWESS, frac=1/20', line=dict(color='red')))\n","# _ = fig.add_traces(go.Scatter(x=df['sentence_no'], y=df['lowess30'], name='LOWESS, frac=1/30', line=dict(color='orange')))\n","\n","# _ = fig.add_traces(px.scatter(df, x='sentence_no', y='lowess20', custom_data=['text_raw'])) #, name='LOWESS, frac=1/20', line=dict(color='red')))\n","# _ = fig.add_traces(px.scatter(df, x='sentence_no', y='lowess30', custom_data=['text_raw'])) #, name='LOWESS, frac=1/30', line=dict(color='orange')))\n","\n","\n","# _ = fig.add_traces(go.Scatter(x=df['sentence_no'], y=df['sma'], name=f'SMA, {selected_sma_window.value}%', line=dict(color='green'), opacity=0.1)) # , opacity=0.4, color_discrete_sequence=['green']))\n","\n","# Change chart background color\n","# _ = fig.update_layout(dict(plot_bgcolor = 'white'))\n","\n","# title_str = f'SentimentArcs: {title_str}<br>{selected_model.value} with {selected_sma_window.value}% SMA'\n","\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} with {selected_sma_window.value}% SMA'\n","\n","\n","\n","# Change chart background color\n","_ = fig.update_layout(dict(plot_bgcolor = 'white'))\n","\n","# Update axes lines\n","_ = fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","_ = fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', \n","                 zeroline=True, zerolinewidth=1, zerolinecolor='lightgrey', \n","                 showline=True, linewidth=1, linecolor='black')\n","\n","# Set figure title\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_full_str = f'SentimentArcs: {title_str}<br>Model: {selected_model.value} SMA {selected_sma_window.value}%'\n","\n","# Update marker size\n","_ = fig.update_traces(marker=dict(size=1))\n","\n","# title_str = f'Sentiment Arc: {title_str}<br>{selected_model.value} with {selected_sma_window.value}% SMA'\n","\n","_ = fig.update_layout(\n","    plot_bgcolor='white',\n","    title=dict(text=title_full_str, y=0.95, x=0.5, xanchor='center', yanchor='bottom'), # font=dict(color='black')),\n","    xaxis_title=\"Line No\",\n","    yaxis_title='Sentiment',\n","    xaxis=dict(showgrid=True), \n","    yaxis=dict(showgrid=True),\n","    legend=dict(\n","    title = \"Models:\",\n","    orientation=\"h\",\n","    xanchor=\"center\",\n","    yanchor=\"bottom\",\n","    y=-0.3, # 0.99,\n","    x=0.5 # x=0.01\n","))\n","\n","_ = fig.update_traces(\n","    hovertemplate=\"<br>\".join([\n","        \"Sentence No: %{x}\",\n","        \"Norm Sentiment: %{y}\",\n","        \"Text: %{customdata[0]}\",\n","    ])\n",")\n","\n","fig.show()"],"metadata":{"id":"ynZQ0Le18nx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Search Text for Matching Substring"],"metadata":{"id":"GokTNAsZoujB"}},{"cell_type":"code","source":["# Search Text for substrings\n","\n","print('Enter a substring to find in the Text:')\n","display(Markdown('---'))\n","search_str = widgets.Text(\n","    value='enter search string',\n","    placeholder='Type something',\n","    description='Substring:',\n","    disabled=False\n",")\n","\n","search_str\n"],"metadata":{"id":"kE0Me6V2moFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","string.punctuation"],"metadata":{"id":"5ImADpbG6h8R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Search Results\n","\n","# Pad beginning and end of search string with one space and lowercase\n","search_clean_str = f' {search_str.value.strip().lower()} '\n","\n","# TODO: use regex to account for optional leading/trailing punctuation\n","\n","# search_res_df = pd.DataFrame(current_sentiment_arc_df[current_sentiment_arc_df['text_raw'].str.contains(search_clean_str)]['text_raw'])\n","# search_res_df = pd.DataFrame(current_sentiment_arc_df[current_sentiment_arc_df['text_raw']str.apply(lambda x : lower(x).contains(search_clean_str)]['text_raw'])\n","# search_res_df.columns = ['text_raw']\n","# search_res_df.index.name = 'sentence_no'\n","search_res_df = current_sentiment_arc_df[current_sentiment_arc_df['text_raw'].apply(lambda x : search_clean_str in str(x).lower())][['sentence_no','text_raw']] # .contains(search_clean_str))]\n","\n","print(f'\\n\\nThere were {search_res_df.shape[0]} matches for \"{search_str.value}\"\\n')\n","search_res_df.head(20)"],"metadata":{"id":"oyrnKQyhnb3d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentno_max = current_sentiment_arc_df.shape[0] - 1\n","sentno_max"],"metadata":{"id":"fw_Q5cxDdN6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View a range of sentences\n","\n","sentno_max = current_sentiment_arc_df.shape[0] - 1\n","\n","print('Enter Range of Sentence Numbers to View:')\n","display(Markdown('---'))\n","\n","sent_range = widgets.IntRangeSlider(\n","    value=[0,11],\n","    min=0,\n","    max=sentno_max,\n","    step=1,\n","    description='Start-End:',\n","    disabled=False,\n","    continuous_update=False,\n","    orientation='horizontal',\n","    readout=True,\n","    readout_format='d',\n",")\n","\n","sent_range"],"metadata":{"id":"lxTTNsszcJoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**[DIRECTIONS] Plotly Graph Above**\n","\n","* carefully roll over only the red & orange lines to view corresponding Text (not blue)\n","\n","* Use mouse to click+drag over a region of interest to magnify it\n","\n","* Roll over the top right corner to view the toolbar (download, zoom in/out, pan, autoscale)"],"metadata":{"id":"GxE9LcFCGFWg"}},{"cell_type":"markdown","source":["# **[STEP 5] Peak Detection & Crux Extraction**"],"metadata":{"id":"gcDJMzX31A5z"}},{"cell_type":"markdown","source":["## Peak Detection"],"metadata":{"id":"0i1PNjfZ3ggb"}},{"cell_type":"code","source":["# https://github.com/jankoslavic/py-tools/blob/master/findpeaks/findpeaks.py\n","\n","def findpeaks(data, spacing=1, limit=None):\n","    \"\"\"Finds peaks in `data` which are of `spacing` width and >=`limit`.\n","    :param data: values\n","    :param spacing: minimum spacing to the next peak (should be 1 or more)\n","    :param limit: peaks should have value greater or equal\n","    :return:\n","    \"\"\"\n","    ln = data.size\n","    x = np.zeros(ln+2*spacing)\n","    x[:spacing] = data[0]-1.e-6\n","    x[-spacing:] = data[-1]-1.e-6\n","    x[spacing:spacing+ln] = data\n","    peak_candidate = np.zeros(ln)\n","    peak_candidate[:] = True\n","    for s in range(spacing):\n","        start = spacing - s - 1\n","        h_b = x[start : start + ln]  # before\n","        start = spacing\n","        h_c = x[start : start + ln]  # central\n","        start = spacing + s + 1\n","        h_a = x[start : start + ln]  # after\n","        peak_candidate = np.logical_and(peak_candidate, np.logical_and(h_c > h_b, h_c > h_a))\n","\n","    ind = np.argwhere(peak_candidate)\n","    ind = ind.reshape(ind.size)\n","    if limit is not None:\n","        ind = ind[data[ind] > limit]\n","    return ind"],"metadata":{"id":"qCQEIpM6Cuma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://github.com/jankoslavic/py-tools/blob/master/findpeaks/Findpeaks%20example.ipynb\n","\n","_ = plt.figure(figsize=(20,10))\n","\n","# List of (x,y) coordinates of peaks and valleys\n","peaks_xy_ls = []\n","valleys_xy_ls = []\n","\n","n = 80\n","m = 20\n","limit = 0\n","spacing = 3\n","# t = np.linspace(0., 1, n)\n","# x = np.zeros(n)\n","# np.random.seed(0)\n","# phase = 2 * np.pi * np.random.random(m)\n","# for i in range(m):\n","#     x += np.sin(phase[i] + 2 * np.pi * t * i)\n","\n","x = df[selected_model.value].values\n","t = df['sentence_no'].values\n","\n","# Plot SMA\n","_ = plt.plot(t, x, alpha=0.3)\n","\n","# Plot LOWESS\n","# _ = plt.plot(y_hat2[:,0], y_hat2[:,1])\n","_ = plt.plot(df['sentence_no'], df['lowess30'])\n","\n","# Find Peaks\n","# peaks = findpeaks(y_hat2[:,1]) # , spacing=spacing, limit=limit)\n","peaks = findpeaks(df['lowess30'].values) # , spacing=spacing, limit=limit)\n","\n","# Find Valleys\n","# valleys = findpeaks(-y_hat2[:,1])\n","valleys = findpeaks(df['lowess30'].mul(-1.0).values)\n","\n","# _ = plt.axhline(limit, color='r')\n","_ = plt.plot(t[peaks], x[peaks], 'g^', markersize=15)\n","peaks_xy_ls = list(zip(t[peaks],x[peaks]))\n","_ = plt.plot(t[valleys], x[valleys], 'rv', markersize=20)\n","valleys_xy_ls = list(zip(t[valleys],x[valleys]))\n","\n","# Set figure title\n","title_str = global_vars.corpus_titles_dt[selected_text.value][0]\n","title_all_str = f'{title_str}\\n{selected_model.value} SMA {selected_sma_window.value}%'\n","peak_str = f'Peaks: minimum value {limit}, minimum spacing {spacing} points'\n","\n","_ = plt.title(f'{title_all_str}\\n{peak_str}')\n","_ = plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"NhmIR0toDFOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Oo1fAA5umipe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge and sort tuples of crux points (both peaks and valleys)\n","\n","crux_peaks_xy_ls = [((x[0], x[1], 'peak')) for x in peaks_xy_ls]\n","# crux_peaks_xy_ls\n","crux_valleys_xy_ls = [((x[0], x[1], 'valley')) for x in valleys_xy_ls]\n","# crux_valleys_xy_ls\n","\n","crux_xy_ls = crux_peaks_xy_ls + crux_valleys_xy_ls\n","crux_xy_ls.sort(key=lambda tup: tup[0])\n","\n","crux_xy_ls\n"],"metadata":{"id":"gOdHqx_mRHuQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Crux Extraction"],"metadata":{"id":"va0_iRCT3iMe"}},{"cell_type":"code","source":["crux_window = widgets.IntSlider(\n","    value=11,\n","    min=1,\n","    max=55,\n","    step=2,\n","    description='Crux Window:',\n","    disabled=False,\n","    continuous_update=False,\n","    orientation='horizontal',\n","    readout=True,\n","    readout_format='d'\n",")\n","print('Retrieve how many Sentences around each Crux Point for context?')\n","crux_window"],"metadata":{"id":"UC8qWXdrFqxk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explore Peak Crux Points\n","\n","crux_peaks_ls = []\n","text_len = df.shape[0]\n","\n","crux_half_win = int((crux_window.value - 1)/2)\n","print(f'crux_half_win: {crux_half_win}')\n","\n","for i, acrux_sentno in enumerate(peaks):\n","  print(f'Peak #{i} at Line #{acrux_sentno} ------------------------------')\n","  start_indx = acrux_sentno-crux_half_win\n","  if start_indx < 0:\n","    start_indx = 0\n","  end_indx = acrux_sentno+crux_half_win\n","  if end_indx > text_len:\n","    end_indx = text_len\n","  print(f'  Context around Crux Point: Lines {start_indx} to {end_indx} ({100*acrux_sentno/text_len:.2f}% point)')\n","  temp_crux_ls = []\n","  for i in range(start_indx,end_indx):\n","    if i == acrux_sentno:\n","      temp_crux_ls.append('[CRUX >>>] ' + df.iloc[i]['text_raw'] + ' [<<< CRUX]')\n","    else:\n","      temp_crux_ls.append(df.iloc[i]['text_raw'])\n","  # crux_str = df.iloc[start_indx:end_indx]['text_raw'].str.cat(sep='\\n')\n","  crux_str = '\\n'.join(temp_crux_ls)\n","  crux_peaks_ls.append([acrux_sentno, crux_str])\n","  print(f'\\n\\n{crux_str}\\n\\n')\n"],"metadata":{"id":"B9C3Zwp7Fqmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explore Valley Crux Points\n","\n","crux_valleys_ls = []\n","text_len = df.shape[0]\n","\n","crux_half_win = int((crux_window.value - 1)/2)\n","print(f'crux_half_win: {crux_half_win}')\n","\n","for i, acrux_sentno in enumerate(valleys):\n","  print(f'Processing Valley #{i} at Line #{acrux_sentno} ------------------------------')\n","  start_indx = acrux_sentno-crux_half_win\n","  if start_indx < 0:\n","    start_indx = 0\n","  end_indx = acrux_sentno+crux_half_win\n","  if end_indx > text_len:\n","    end_indx = text_len\n","  print(f'  Context around Crux Point: Lines {start_indx} to {end_indx} ({100*acrux_sentno/text_len:.2f}% point)')\n","  temp_crux_ls = []\n","  for i in range(start_indx,end_indx):\n","    if i == acrux_sentno:\n","      temp_crux_ls.append('[CRUX >>>] ' + df.iloc[i]['text_raw'] + ' [<<< CRUX]')\n","    else:\n","      temp_crux_ls.append(df.iloc[i]['text_raw'])\n","  # crux_str = df.iloc[start_indx:end_indx]['text_raw'].str.cat(sep='\\n')\n","  crux_str = '\\n'.join(temp_crux_ls)\n","  crux_valleys_ls.append([acrux_sentno, crux_str])\n","  print(f'\\n\\n{crux_str}\\n\\n')"],"metadata":{"id":"OG0XycyWFqjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save Crux Points and Contexts"],"metadata":{"id":"3CMl1Ml2lrgh"}},{"cell_type":"code","source":["# Save Crux Points to File\n","\n","# crux_peaks_ls\n","\n","peaks_sentno_ls = [item[0] for item in crux_peaks_ls]\n","peaks_value_ls = [y_hat2[x] for x in peaks_sentno_ls]\n","peaks_value_ls = [item[1] for item in peaks_value_ls]\n","peaks_sentstr_ls = [item[1] for item in crux_peaks_ls]\n","valleys_sentno_ls = [item[0] for item in crux_valleys_ls]\n","valleys_value_ls = [y_hat2[x] for x in valleys_sentno_ls]\n","valleys_value_ls = [item[1] for item in valleys_value_ls]\n","valleys_sentstr_ls = [item[1] for item in crux_valleys_ls]\n","\n","\n"],"metadata":{"id":"FCz_N8IS3kQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Capture statistics for Peaks\n","\n","crux_peaks_df = pd.DataFrame({'sentence_no':peaks_sentno_ls, 'type':['peak']*len(peaks_sentno_ls), 'sentiment':peaks_value_ls, 'crux':peaks_sentstr_ls})\n","crux_peaks_df.sort_values(by='sentiment', ascending=False, inplace=True)\n","crux_peaks_df['rank_abs'] = crux_peaks_df['sentiment'].rank(ascending=False)\n","crux_peaks_df.head()"],"metadata":{"id":"sHu7RU-bvjNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Capture statistics for Valleys\n","\n","crux_valleys_df = pd.DataFrame({'sentence_no':valleys_sentno_ls, 'type':['valley']*len(valleys_sentno_ls), 'sentiment':valleys_value_ls, 'crux':valleys_sentstr_ls})\n","crux_valleys_df.sort_values(by='sentiment', ascending=False, inplace=True)\n","crux_valleys_df['rank_abs'] = crux_valleys_df['sentiment'].rank(ascending=False)\n","crux_valleys_df.head()"],"metadata":{"id":"t7a-nbIwxPvf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine, Sort and Compute relative Ranks\n","\n","crux_df = pd.concat([crux_peaks_df, crux_valleys_df], axis=0)\n","\n","crux_df.sort_values(by='sentence_no', inplace=True)\n","\n","crux_df.head()\n","crux_df.tail()\n","crux_df.info()\n"],"metadata":{"id":"0OBtGvtbiOnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save DataFrame of all Crux Points/Lines as *.csv (can import into Excel to explore further)\n","\n","err_fl = False\n","\n","if Corpus_Type == 'new':\n","  # crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}.csv'\n","  crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/graphs_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}.csv'\n","elif Corpus_Type == 'reference':\n","  # crux_filepath = f'./graphs/graphs_{Corpus_Genre}_{Corpus_Type}/graphs_{Corpus_Genre}_{Corpus_Type}.csv'\n","  crux_filepath = f'./graphs//graphs_{Corpus_Genre}_{Corpus_Type}/graphs_{Corpus_Genre}_{Corpus_Type}.csv'\n","else:\n","  err_fl = True\n","  print(f'ERROR: Illegal value for Corpus_Type: {Corpus_Type}')\n","\n","os.chdir(Path_to_SentimentArcs)\n","print(f'Current working directory:\\n  {os.getcwd()}\\n')\n","\n","if not err_fl:\n","  crux_df.to_csv(crux_filepath)\n","  print(f'\\nSaved in:\\n  {crux_filepath}')\n","else:\n","  print(f'\\nCould not save due to error above')"],"metadata":{"id":"N6k640w7kKLX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verify file was written\n","\n","!ls $crux_filepath"],"metadata":{"id":"LhhqiqEf9iYm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_j92DSgyhGc"},"source":["# **END OF NOTEBOOK**"]}],"metadata":{"colab":{"collapsed_sections":["5iUxLgUCjpJg","CRb36wyH7moE","ajD8hCbzkStO","GM-ZzGwkjpJh","-XHwZJgsjpJh","fys3dkJSB656","e3fBlTEw0fV_","PPrnvErgWaSC","OnVUg2Z5apcf"],"name":"sentiment_arcs_part6_analysis.ipynb","provenance":[],"private_outputs":true,"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}