{"cells":[{"cell_type":"markdown","metadata":{"id":"3i0Fg4SYB7g0"},"source":["# **SentimentArcs (Part 1): Text Preprocessing**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","Welcome! \n","\n","SentimentArcs is a methodlogy and software framework for analyzing narrative in text. Virtually all long text contains narrative elements...(TODO: Insert excerpts from Paper Abstract/Intro Sections here)\n","\n","***\n","\n","* **SentimentArcs: Cloning the Github repository to your gDrive**\n","\n","If this is the first time using SentimentArcs, you will need to copy the software from our Github.com repository (github repo). The default recommended gDrive path is ./gdrive/MyDrive/research/sentiment_arcs/'. \n","\n","The first time you run this notebook and connect your Google gDrive, it will allow to to specify the path to your SentimentArcs subdirectory. If it does not exists, this notebook will copy/clone the SentimentArcs github repository code to your gDrive at the path you specify.\n","\n","\n","***\n","\n","* **NovelText: A Reference Corpus of 24 Diverse Novel**\n","\n","Sentiment Arcs comes with a carefully curated reference corpus of Novels to illustrate the unique diachronic sentiment analysis characteristic of long form fictional narrativeas. This corpus of 24 diverse novels also provides a baseline for exploring and comparing new novels with sentiment analysis using SentimentArcs.\n","\n","***\n","\n","* **Preparing New Novels: Formatting and adding to subdirectory**\n","\n","To analyze new novels with SentimentArcs, the body of the text should consist of plain text organized in to blocks separated by two newlines which visually look like a single blank line between blocks. These blocks are usually paragraphs but can also include title headers, separate lines of dialog or quotes. Please reference any of the 24 novels in the NovelText corpus for examples of this expected format.\n","\n","Once the new novel is correctly formatted as a plain text file, it should follow this standard file naming convention:\n","\n","[first letter of first name]+[full lastname]_[abbreviated book title].txt\n","\n","Examples:\n","\n","* fdouglass_narrativelifeofaslave\n","* fscottfitzgerald_thegreatgatsby.txt\n","* vwoolf_mrsdalloway.txt\n","* homer-ewilson_odyssey.txt (trans. E.Wilson)\n","* mproust-mtreharne_3guermantesway.txt (Book 3, trans. M.Treharne)\n","* staugustine_confessions9end.txt (Upto and incl Book 9)\n","\n","Note the optional author suffix (-translator) and optional title suffix (-selected chapters/books)\n","\n","***\n","\n","* **Adding New Novels: Add file to subdirectory and Update this Notebook**\n","\n","Once you have a cleaned and text file named according the standard rule above, you must move that file to the subdirectory of all input novels and update the global variable in this notebook that defines which novels to analyze.\n","\n","First, copy your cleaned text file to the subdirectory containing all novels read by this notebook. This subdir is defined by the program variable 'subdir_novels' with the default value './in1_novels/'\n","\n","Second, update the program variable 'novels_dt'. This is a Dictionary data structure that following the pattern below:\n","```\n","novels_dt = {\n","  'cdickens_achristmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n","```\n","Where the first string (the dictionary key) must match the filename root without the '.txt' suffix (e.g. cdickens_achristmascarol). The Dictionary value after the ':' is a list of three elements:\n","\n","* A nicely formatted string of the form '(title) by (full first and last name of author)' that should be a human friendly string used to label plots and saved files.\n","\n","* The (publication year) and the (sentence count). Both are optional, but should have placeholder string '0' if unknown. These are intended for future reference and analytics.\n","\n","* Your future self will thank you if you insert new novels into the 'novels_dt' in alphabetic order for faster and more accurate reference.\n","\n","***\n","\n","* **How to Execute SentimentArcs Notebooks:**\n","\n","This is a Jupyter Notebook created to run on Google's free Colab service using only a browers and your exiting Google email account. We chose Google Colab because it is relatively, fast, free, easy to use and makes collaboration as simple as web browsing.\n","\n","A few reminders about using Jupyter Notebooks general and SentimentArcs in particular:\n","\n","* All cells must be run ***in order*** as later code cells often depend upon the output of earlier code cells\n","\n","* ***Cells that take more time to execute*** (> 1 min) usually begin with *%%time* which outputs the *total execution time* of the last run.  This timing output is deleted and recalculated each time the code cell is executed.\n","\n","* **[OPTIONAL]** at the top of a cell indicates you *may* change a setting in that cell to customize behavior.\n","\n","* **[CUSTOMIZE]** at the top of a cell indicates you *must* change a setting in that cell.\n","\n","* **[RESTART REQUIRED]** at the top of a cell indicates you *may* see a *[RESTART REQUIRED] button* at the end of the output. *If you see this button, you must select [Runtime]->[Restart Runtime] from the top menubar.\n","\n","* **[INPUT REQUIRED]** at the top of a cell indicates you will be required to take some action for execution to proceed, usually by clicking a button or entering the response to a prompt.\n","\n","All cells with a top comment prefixed with # [OPTIONAL]: indicates that you can change a setting to customize behavior, the prefix [CUSTOMIZE] indicates you MUST set/change a setting\n","\n","* SentimentArcs divides workflow into a series of chronological Jupyter Notebooks that must be run in order. Here is an overview of the workflow:\n","\n","***\n","\n","**SentimentArcs Notebooks Workflow**\n","1. Notebook #1: Preprocess Text\n","2. Notebook #2: Compute Sentiment Values (Simple Models/CPUs)\n","3. Notebook #3: Compute Sentiment Values (Complex Models/GPUs)\n","4. Notebook #4: Combine all Sentiment Values, perform Time Series analysis, and extract Crux points and surrounding text\n","\n","If you are unfamilar with setting up and using Google Colab or Jupyter Notebooks, here are a series of resources to quickly bring you up to speed. If you are using SentimentArcs with the Cambridge University Press Elements textbook, there are also a series of videos by Prof Elkins and Chun stepping you through these notebooks.\n","\n","***\n","\n","**Additional Resources and Tutorials**\n","\n","\n","**Google Colab and Jupyter Resources:**\n","\n","* Coming...\n","* [IPython, Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/01.00-ipython-beyond-normal-python.html) \n","\n","**Cambridge University Press Videos:**\n","\n","* Coming...\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bfkqjgMiw7T","executionInfo":{"status":"ok","timestamp":1649188487886,"user_tz":240,"elapsed":20687,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"1e7b5b38-68d0-46f8-8e99-f06835178fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n","Mounted at /gdrive\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":492,"status":"ok","timestamp":1649188489191,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"kskWCX1KyrV_","outputId":"301718bf-c931-4a4d-edc8-54ab04ff7f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","SUBDIR_TEXT_RAW:\n","  [text_raw_finance_ref]\n","PATH_TEXT_RAW:\n","  [./text_raw/text_raw_finance_ref]\n","SUBDIR_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_ref]\n","PATH_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_ref]\n"]}],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"finance\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_ref'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_ref'\n","else:\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","PATH_TEXT_RAW = f'./text_raw/{SUBDIR_TEXT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'SUBDIR_TEXT_RAW:\\n  [{SUBDIR_TEXT_RAW}]')\n","print(f'PATH_TEXT_RAW:\\n  [{PATH_TEXT_RAW}]')\n","\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1008,"status":"ok","timestamp":1649188494420,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"uRx8mIVxUyXM","outputId":"76ce6393-6ec2-4a0c-e53a-5be62502b898"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n","\n","\n","Contents of Subdirectory [./sentiment_arcs/utils/]\n","\n","config_matplotlib.py   global_constants.py    sentiment_arcs_config.py\n","config_seaborn.py      global_vars.py\t      set_globals.py\n","file_utils.py\t       __init__.py\t      subdir_constants.py\n","get_fullpath.py        __pycache__\t      test.py\n","get_model_families.py  read_yaml.py\t      text_cleaners_new.py\n","get_sentimentr.R       sa_config_20220404.py  text_cleaners.py\n","get_sentiments.py      sa_config.py\n","get_subdirs.py\t       sentiment_analysis.py\n"]}],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1649188495997,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"RBtWnOBxiw8H","outputId":"480359b0-12f5-44d2-edc1-a82aa9357436"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt']"]},"metadata":{},"execution_count":4}],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_TEXT_RAW = SUBDIR_TEXT_RAW\n","global_vars.PATH_TEXT_RAW = PATH_TEXT_RAW\n","\n","dir(global_vars)"]},{"cell_type":"markdown","metadata":{"id":"P00BhwLVyL8X"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"markdown","metadata":{"id":"CBoEHX9Z9imD"},"source":["## Custom Libraries & Define Globals"]},{"cell_type":"code","source":["dir(global_vars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mfm1mYioS4jT","executionInfo":{"status":"ok","timestamp":1649188561530,"user_tz":240,"elapsed":175,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"e786c4b6-2f44-400d-8ce3-5bb5fe502b43"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Define Global Dict to hold cleaned Texts\n","\n","global_vars.corpus_texts_dt = {}"],"metadata":{"id":"qzJSUvadztx9","executionInfo":{"status":"ok","timestamp":1649187533815,"user_tz":240,"elapsed":201,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["!ls utils/*.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUzefhTQSHFs","executionInfo":{"status":"ok","timestamp":1649188600114,"user_tz":240,"elapsed":225,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"d897a4e5-6faa-4590-ca2b-f23f966854af"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["utils/config_matplotlib.py   utils/read_yaml.py\n","utils/config_seaborn.py      utils/sa_config_20220404.py\n","utils/file_utils.py\t     utils/sa_config.py\n","utils/get_fullpath.py\t     utils/sentiment_analysis.py\n","utils/get_model_families.py  utils/sentiment_arcs_config.py\n","utils/get_sentiments.py      utils/set_globals.py\n","utils/get_subdirs.py\t     utils/subdir_constants.py\n","utils/global_constants.py    utils/test.py\n","utils/global_vars.py\t     utils/text_cleaners_new.py\n","utils/__init__.py\t     utils/text_cleaners.py\n"]}]},{"cell_type":"code","source":["!head -n 40 ./utils/sa_config.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWRM6xbWR9Jx","executionInfo":{"status":"ok","timestamp":1649188601523,"user_tz":240,"elapsed":383,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"b3deca99-5798-40b3-9bf5-1ee225460d64"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","import global_vars\n","\n","def get_subdirs(SA_root,Corpus_Genre, Corpus_Type, Corpus_Number, NotebookModels):\n","    '''\n","    Given a two strings: Corpus, Text_type\n","    Set all global SUB/DIR constants\n","    '''\n","\n","    # NotebookModels indicates which notebook is currently running that imported this get_subdirs() function\n","    if global_vars.NotebookModels == 'syuzhetr2sentimentr':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_syuzhetr2sentimentr.json'\n","    elif NotebookModels == 'lex2ml':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_lex2ml.json'\n","    elif NotebookModels == 'dnn2transformers':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_dnn2transformers.json'\n","    elif NotebookModels == 'none':\n","        global_vars.FNAME_SENTIMENT_RAW = f'[NONE]'\n","    else:\n","        print(f'ERROR: Illegal value for NotebookModels: {global_vars.NotebookModels}')\n","        return\n","\n","    # Define a universal syntax for a common directory structure across all notebooks\n","    # global_vars.SUBDIR_SENTIMENTARCS = '/gdrive/MyDrive/cdh/sentiment_arcs'\n","    global_vars.SUBDIR_SENTIMENTARCS = SA_root\n","    if Corpus_Type == 'new':\n","        global_vars.SUBDIR_TEXT_RAW = f\"./text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TEXT_CLEAN = f\"./text_clean/text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/sentiemnt_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TIMESERIES_RAW = f\"./timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TIMESERIES_CLEAN = f\"./timeseries_clean/timeseries_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","    elif Corpus_Type == 'reference':\n","        global_vars.SUBDIR_TEXT_RAW = f\"./text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TEXT_CLEAN = f\"./text_clean/text_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/sentiment_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/sentiemnt_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TIMESERIES_RAW = f\"./timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TIMESERIES_CLEAN = f\"./timeseries_clean/timeseries_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","    else:\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1649188610568,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"VtaPyy4VSohJ","outputId":"58c4d771-5e0b-41d8-aebf-e762712453e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","Objects in sa_config()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_subdirs', 'global_vars', 'set_globals']\n","\n","\n","Verify the Directory Structure:\n","\n","-------------------------------\n","\n","           [Corpus Genre]: finance\n","\n","            [Corpus Type]: reference\n","\n","\n","    [FNAME_SENTIMENT_RAW]: [NONE]\n","\n","\n","\n","\n","INPUTS:\n","-------------------------------\n","\n","   [SUBDIR_SENTIMENTARCS]: /gdrive/MyDrive/sentimentarcs_notebooks/\n","\n","\n","STEP 1: Clean Text\n","--------------------\n","\n","        [SUBDIR_TEXT_RAW]: ./text_raw/text_raw_finance_reference/\n","\n","      [SUBDIR_TEXT_CLEAN]: ./text_clean/text_clean_finance_reference/\n","\n","\n","STEP 2: Get Sentiments\n","--------------------\n","\n","   [SUBDIR_SENTIMENT_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n"," [SUBDIR_SENTIMENT_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","STEP 3: Smooth Time Series and Get Crux Points\n","--------------------\n","\n","  [SUBDIR_TIMESERIES_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n","[SUBDIR_TIMESERIES_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","\n","OUTPUTS:\n","-------------------------------\n","\n","          [SUBDIR_GRAPHS]: ./graphs/graphs_finance/\n","\n","            [SUBDIR_DATA]: ./data/data_finance\n","\n","           [SUBDIR_UTILS]: ./utils/\n","\n"]}],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1649188619964,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tx8j_3Y6qQna","outputId":"97640f1c-affd-4be9-b5cc-d412622a6430"},"outputs":[{"output_type":"stream","name":"stdout","text":["MIN_PARAG_LEN: 10\n","STOPWORDS_ADD_EN: ['a', 'the', 'an']\n","TEST_WORDS_LS: ['Love', 'Hate', 'bizarre', 'strange', 'furious', 'elated', 'curious', 'beserk', 'gambaro']\n","SLANG_DT: {'$': ' dollar ', '€': ' euro ', '4ao': 'for adults only', 'a.m': 'before midday', 'a3': 'anytime anywhere anyplace', 'aamof': 'as a matter of fact', 'acct': 'account', 'adih': 'another day in hell', 'afaic': 'as far as i am concerned', 'afaict': 'as far as i can tell', 'afaik': 'as far as i know', 'afair': 'as far as i remember', 'afk': 'away from keyboard', 'app': 'application', 'approx': 'approximately', 'apps': 'applications', 'asap': 'as soon as possible', 'asl': 'age, sex, location', 'atk': 'at the keyboard', 'ave.': 'avenue', 'aymm': 'are you my mother', 'ayor': 'at your own risk', 'b&b': 'bed and breakfast', 'b+b': 'bed and breakfast', 'b.c': 'before christ', 'b2b': 'business to business', 'b2c': 'business to customer', 'b4': 'before', 'b4n': 'bye for now', 'b@u': 'back at you', 'bae': 'before anyone else', 'bak': 'back at keyboard', 'bbbg': 'bye bye be good', 'bbc': 'british broadcasting corporation', 'bbias': 'be back in a second', 'bbl': 'be back later', 'bbs': 'be back soon', 'be4': 'before', 'bfn': 'bye for now', 'blvd': 'boulevard', 'bout': 'about', 'brb': 'be right back', 'bros': 'brothers', 'brt': 'be right there', 'bsaaw': 'big smile and a wink', 'btw': 'by the way', 'bwl': 'bursting with laughter', 'c/o': 'care of', 'cet': 'central european time', 'cf': 'compare', 'cia': 'central intelligence agency', 'csl': 'can not stop laughing', 'cu': 'see you', 'cul8r': 'see you later', 'cv': 'curriculum vitae', 'cwot': 'complete waste of time', 'cya': 'see you', 'cyt': 'see you tomorrow', 'dae': 'does anyone else', 'dbmib': 'do not bother me i am busy', 'diy': 'do it yourself', 'dm': 'direct message', 'dwh': 'during work hours', 'e123': 'easy as one two three', 'eet': 'eastern european time', 'eg': 'example', 'embm': 'early morning business meeting', 'encl': 'enclosed', 'encl.': 'enclosed', 'etc': 'and so on', 'faq': 'frequently asked questions', 'fawc': 'for anyone who cares', 'fb': 'facebook', 'fc': 'fingers crossed', 'fig': 'figure', 'fimh': 'forever in my heart', 'ft.': 'feet', 'ft': 'featuring', 'ftl': 'for the loss', 'ftw': 'for the win', 'fwiw': 'for what it is worth', 'fyi': 'for your information', 'g9': 'genius', 'gahoy': 'get a hold of yourself', 'gal': 'get a life', 'gcse': 'general certificate of secondary education', 'gfn': 'gone for now', 'gg': 'good game', 'gl': 'good luck', 'glhf': 'good luck have fun', 'gmt': 'greenwich mean time', 'gmta': 'great minds think alike', 'gn': 'good night', 'g.o.a.t': 'greatest of all time', 'goat': 'greatest of all time', 'goi': 'get over it', 'gps': 'global positioning system', 'gr8': 'great', 'gratz': 'congratulations', 'gyal': 'girl', 'h&c': 'hot and cold', 'hp': 'horsepower', 'hr': 'hour', 'hrh': 'his royal highness', 'ht': 'height', 'ibrb': 'i will be right back', 'ic': 'i see', 'icq': 'i seek you', 'icymi': 'in case you missed it', 'idc': 'i do not care', 'idgadf': 'i do not give a damn fuck', 'idgaf': 'i do not give a fuck', 'idk': 'i do not know', 'ie': 'that is', 'i.e': 'that is', 'ifyp': 'i feel your pain', 'IG': 'instagram', 'iirc': 'if i remember correctly', 'ilu': 'i love you', 'ily': 'i love you', 'imho': 'in my humble opinion', 'imo': 'in my opinion', 'imu': 'i miss you', 'iow': 'in other words', 'irl': 'in real life', 'j4f': 'just for fun', 'jic': 'just in case', 'jk': 'just kidding', 'jsyk': 'just so you know', 'l8r': 'later', 'lb': 'pound', 'lbs': 'pounds', 'ldr': 'long distance relationship', 'lmao': 'laugh my ass off', 'lmfao': 'laugh my fucking ass off', 'lol': 'laughing out loud', 'ltd': 'limited', 'ltns': 'long time no see', 'm8': 'mate', 'mf': 'motherfucker', 'mfs': 'motherfuckers', 'mfw': 'my face when', 'mofo': 'motherfucker', 'mph': 'miles per hour', 'mr': 'mister', 'mrw': 'my reaction when', 'ms': 'miss', 'mte': 'my thoughts exactly', 'nagi': 'not a good idea', 'nbc': 'national broadcasting company', 'nbd': 'not big deal', 'nfs': 'not for sale', 'ngl': 'not going to lie', 'nhs': 'national health service', 'nrn': 'no reply necessary', 'nsfl': 'not safe for life', 'nsfw': 'not safe for work', 'nth': 'nice to have', 'nvr': 'never', 'nyc': 'new york city', 'oc': 'original content', 'og': 'original', 'ohp': 'overhead projector', 'oic': 'oh i see', 'omdb': 'over my dead body', 'omg': 'oh my god', 'omw': 'on my way', 'p.a': 'per annum', 'p.m': 'after midday', 'pm': 'prime minister', 'poc': 'people of color', 'pov': 'point of view', 'pp': 'pages', 'ppl': 'people', 'prw': 'parents are watching', 'ps': 'postscript', 'pt': 'point', 'ptb': 'please text back', 'pto': 'please turn over', 'qpsa': 'what happens', 'ratchet': 'rude', 'rbtl': 'read between the lines', 'rlrt': 'real life retweet', 'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off', 'rt': 'retweet', 'ruok': 'are you ok', 'sfw': 'safe for work', 'sk8': 'skate', 'smh': 'shake my head', 'sq': 'square', 'srsly': 'seriously', 'ssdd': 'same stuff different day', 'tbh': 'to be honest', 'tbs': 'tablespooful', 'tbsp': 'tablespooful', 'tfw': 'that feeling when', 'thks': 'thank you', 'tho': 'though', 'thx': 'thank you', 'tia': 'thanks in advance', 'til': 'today i learned', 'tl;dr': 'too long i did not read', 'tldr': 'too long i did not read', 'tmb': 'tweet me back', 'tntl': 'trying not to laugh', 'ttyl': 'talk to you later', 'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'utc': 'coordinated universal time', 'w/': 'with', 'w/o': 'without', 'w8': 'wait', 'wassup': 'what is up', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go', 'wtpa': 'where the party at', 'wuf': 'where are you from', 'wuzup': 'what is up', 'wywh': 'wish you were here', 'yd': 'yard', 'ygtr': 'you got that right', 'ynk': 'you never know', 'zzz': 'sleeping bored and tired'}\n"]}],"source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kK8zKENjsyig","executionInfo":{"status":"ok","timestamp":1649188622230,"user_tz":240,"elapsed":190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"Ns5NwArZmush"},"source":["## Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1649189838487,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"mUveIcUOzYav","outputId":"48d31df2-bbb1-42c3-ae92-cb398f64f682"},"outputs":[{"output_type":"stream","name":"stdout","text":["Objects in read_yaml()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'global_vars', 'read_corpus_yaml', 'yaml']\n","\n","\n","YAML Directory: text_raw/text_raw_finance_reference\n","YAML File: text_raw_finance_reference_info.yaml\n","SentimentArcs Model Ensemble ------------------------------\n","\n","AutoGluon_Text\n","BERT_2IMDB\n","BERT_Dual_Coding\n","BERT_Multilingual\n","BERT_Yelp\n","CNN_DNN\n","Distilled_BERT\n","FLAML_AutoML\n","Fully_Connected_Network\n","HyperOpt_CNN_Flair_AutoML\n","LSTM_DNN\n","Logistic_Regression\n","Logistic_Regression_CV\n","Multilingual_CNN_Stanza_AutoML\n","Multinomial_Naive_Bayes\n","Pattern\n","Random_Forest\n","RoBERTa_Large_15DB\n","RoBERTa_XML_8Language\n","SentimentR_JockersRinker\n","SentimentR_Jockers\n","SentimentR_Bing\n","SentimentR_NRC\n","SentimentR_SentiWord\n","SentimentR_SenticNet\n","SentimentR_LMcD\n","SentimentR_SentimentR\n","PySentimentR_JockersRinker\n","PySentimentR_Huliu\n","PySentimentR_NRC\n","PySentimentR_SentiWord\n","PySentimentR_SenticNet\n","PySentimentR_LMcD\n","SyuzhetR_AFINN\n","SyuzhetR_Bing\n","SyuzhetR_NRC\n","SyuzhetR_SyuzhetR\n","T5_IMDB\n","TextBlob\n","VADER\n","AFINN\n","XGBoost\n","\n","\n","Corpus Texts ------------------------------\n","\n","bogfederalreserve_speech_1997-2022\n","eucentralbank_speeches_1998-2022\n","\n","\n","There are 42 Models in the SentimentArcs Ensemble above.\n","\n","\n","There are 2 Texts in the Corpus above.\n","\n","\n","\n"]}],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = global_vars.corpus_titles_dt.keys()\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3626,"status":"ok","timestamp":1649189127200,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tz5jGrDYi9Qe","outputId":"7ea037fe-1207-4fe3-da62-6daddc1c755a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyreadr\n","  Downloading pyreadr-0.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 92 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 215 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 225 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 235 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 245 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 256 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 266 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 276 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 286 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 296 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 307 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 317 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 327 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 337 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 348 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 358 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 361 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyreadr) (1.3.5)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (1.21.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.15.0)\n","Installing collected packages: pyreadr\n","Successfully installed pyreadr-0.4.4\n"]}],"source":["# Library to Read R datafiles from within Python programs\n","\n","!pip install pyreadr"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19192,"status":"ok","timestamp":1649189146388,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"WFXzmfPQouNR","outputId":"a8432c83-becc-4ee7-bc3f-e0aa0e3d1df1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n","Collecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 51.0 MB/s \n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 50.4 MB/s \n","\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 59.1 MB/s \n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Collecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[K     |████████████████████████████████| 653 kB 68.8 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.2 thinc-8.0.15 typer-0.4.1\n"]}],"source":["# Powerful Industry-Grade NLP Library\n","\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15498,"status":"ok","timestamp":1649189161882,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"sD_ZVbywJ4_e","outputId":"2d9c6896-b355-436c-f6b4-478fb9021391"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting texthero\n","  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n","Collecting spacy<3.0.0\n","  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n","Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Collecting nltk>=3.3\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n","Collecting unidecode>=1.1.1\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.63.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (3.10.0.2)\n","Collecting regex>=2021.8.3\n","  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 35.1 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 53.8 MB/s \n","\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Installing collected packages: srsly, catalogue, thinc, regex, unidecode, spacy, nltk, texthero\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.2\n","    Uninstalling srsly-2.4.2:\n","      Successfully uninstalled srsly-2.4.2\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.7\n","    Uninstalling catalogue-2.0.7:\n","      Successfully uninstalled catalogue-2.0.7\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.0.15\n","    Uninstalling thinc-8.0.15:\n","      Successfully uninstalled thinc-8.0.15\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.2.4\n","    Uninstalling spacy-3.2.4:\n","      Successfully uninstalled spacy-3.2.4\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed catalogue-1.0.0 nltk-3.7 regex-2022.3.15 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.4\n"]}],"source":["# NLP Library to Simply Cleaning Text\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3184,"status":"ok","timestamp":1649189165062,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"T94fr2ymLFgV","outputId":"0cb0d814-c727-462b-bcf9-e3cea8d20b1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysbd\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: pysbd\n","Successfully installed pysbd-0.3.4\n"]}],"source":["# Advanced Sentence Boundry Detection Pythn Library\n","#   for splitting raw text into grammatical sentences\n","#   (can be difficult due to common motifs like Mr., ..., ?!?, etc)\n","\n","!pip install pysbd"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4461,"status":"ok","timestamp":1649189169515,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"E3ev2lK-MU9E","outputId":"6471a492-bf0b-479e-ac9f-e3f1986225c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 8.4 MB/s \n","\u001b[?25hCollecting anyascii\n","  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 57.4 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}],"source":["# Python Library to expand contractions to aid in Sentiment Analysis\n","#   (e.g. aren't -> are not, can't -> can not)\n","\n","!pip install contractions"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4355,"status":"ok","timestamp":1649189173865,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"kScSfHO1Q8Y-","outputId":"c565ee2e-6180-4766-97e9-851a11321270"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 21 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}],"source":["# Library for dealing with Emoticons (punctuation) and Emojis (icons)\n","\n","!pip install emot"]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"oCRgJK2ri9Nx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649189174784,"user_tz":240,"elapsed":923,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a0477706-6da8-4944-d797-ca7c41089f57"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-05 20:06:10,057 : INFO : NumExpr defaulting to 2 threads.\n"]}],"source":["# Core Python Libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","import re\n","import string\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import json\n","from pathlib import Path\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"pify1umf6A8K","executionInfo":{"status":"ok","timestamp":1649189174784,"user_tz":240,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# More advanced Sentence Tokenizier Object from PySBD\n","from pysbd.utils import PySBDFactory"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1649189176073,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"EEPQ67KrCO6f","outputId":"90663b74-c184-4005-c2c8-d096415ea6c8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}],"source":["# Simplier Sentence Tokenizer Object from NLTK\n","import nltk \n","from nltk.tokenize import sent_tokenize\n","\n","# Download required NLTK tokenizer data\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5184,"status":"ok","timestamp":1649189181253,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"NRYua8r7MP07","outputId":"55c3f6bf-c3b9-4faf-ccc2-43c1e1a8644a"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-05 20:06:12,472 : INFO : 'pattern' package not found; tag filters are not available for English\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Instantiate and Import Text Cleaning Ojects into Global Variable space\n","import texthero as hero\n","from texthero import preprocessing"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1626,"status":"ok","timestamp":1649189182873,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"7PBsG4WRMvrN","outputId":"6de8f82a-58e1-49a4-a3be-c3013326d0c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'flag': True,\n"," 'location': [[20, 23], [24, 27], [28, 33]],\n"," 'mean': ['Happy face smiley',\n","  'Frown, sad, andry or pouting',\n","  'Very very Happy face or smiley'],\n"," 'value': [':-)', ':-(', ':-)))']}"]},"metadata":{},"execution_count":31}],"source":["# Expand contractions (e.g. can't -> can not)\n","import contractions\n","\n","# Translate emoticons :0 and emoji icons to text\n","import emot \n","emot_obj = emot.core.emot() \n","\n","from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n","\n","# Test\n","text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\" \n","emot_obj.emoticons(text)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1649189183997,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"JPFS4MEm6MyF","outputId":"906ee72c-c3a8-45e7-cb52-27732a30b32d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Token Attributes: \n"," token.text, token.pos_, token.tag_, token.dep_, token.lemma_\n","Apples                                          Apples      \n","and                                             and         \n","oranges                                         orange      \n","are                                             be          \n","similar                                         similar     \n",".                                               .           \n","Boots                                           Boots       \n","and                                             and         \n","hippos                                          hippo       \n","are         AUX         VBP                     be          \n","n't         PART        RB                      not         \n",".                                               .           \n","\n","Another Test:\n","\n","Apples      9297668116247400838           Apples      \n","and         2283656566040971221           and         \n","oranges     2208928596161743350           orange      \n","are         10382539506755952630          be          \n","similar     18166476740537071113          similar     \n",".           12646065887601541794          .           \n","Boots       18231591219755621867          Boots       \n","and         2283656566040971221           and         \n","hippos      6542994350242320795           hippo       \n","are         10382539506755952630          be          \n","n't         447765159362469301            not         \n",".           12646065887601541794          .           \n"]}],"source":["# Import spaCy, language model and setup minimal pipeline\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","# nlp.max_length = 1027203\n","nlp.max_length = 2054406\n","nlp.add_pipe(nlp.create_pipe('sentencizer')) # https://stackoverflow.com/questions/51372724/how-to-speed-up-spacy-lemmatization\n","\n","# Test some edge cases, try to find examples that break spaCy\n","doc= nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","print('\\n')\n","print(\"Token Attributes: \\n\", \"token.text, token.pos_, token.tag_, token.dep_, token.lemma_\")\n","for token in doc:\n","    # Print the text and the predicted part-of-speech tag\n","    print(\"{:<12}{:<12}{:<12}{:<12}{:<12}\".format(token.text, token.pos_, token.tag_, token.dep_, token.lemma_))\n","\n","print('\\nAnother Test:\\n')\n","doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","\n","for token in doc:\n","    print(\"{:<12}{:<30}{:<12}\".format(token.text, token.lemma, token.lemma_))"]},{"cell_type":"markdown","metadata":{"id":"umZfB0YCqajW"},"source":["## Define/Customize Stopwords"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"e8_qLJBbtoOA","executionInfo":{"status":"ok","timestamp":1649189545671,"user_tz":240,"elapsed":159,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Define Globals\n","\"\"\"\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"DWFx7-P-Ai5D","outputId":"61a1196e-7d33-494d-9fbb-9d7ab20c559b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['$', '€', '4ao', 'a.m', 'a3', 'aamof', 'acct', 'adih', 'afaic', 'afaict', 'afaik', 'afair', 'afk', 'app', 'approx', 'apps', 'asap', 'asl', 'atk', 'ave.', 'aymm', 'ayor', 'b&b', 'b+b', 'b.c', 'b2b', 'b2c', 'b4', 'b4n', 'b@u', 'bae', 'bak', 'bbbg', 'bbc', 'bbias', 'bbl', 'bbs', 'be4', 'bfn', 'blvd', 'bout', 'brb', 'bros', 'brt', 'bsaaw', 'btw', 'bwl', 'c/o', 'cet', 'cf', 'cia', 'csl', 'cu', 'cul8r', 'cv', 'cwot', 'cya', 'cyt', 'dae', 'dbmib', 'diy', 'dm', 'dwh', 'e123', 'eet', 'eg', 'embm', 'encl', 'encl.', 'etc', 'faq', 'fawc', 'fb', 'fc', 'fig', 'fimh', 'ft.', 'ft', 'ftl', 'ftw', 'fwiw', 'fyi', 'g9', 'gahoy', 'gal', 'gcse', 'gfn', 'gg', 'gl', 'glhf', 'gmt', 'gmta', 'gn', 'g.o.a.t', 'goat', 'goi', 'gps', 'gr8', 'gratz', 'gyal', 'h&c', 'hp', 'hr', 'hrh', 'ht', 'ibrb', 'ic', 'icq', 'icymi', 'idc', 'idgadf', 'idgaf', 'idk', 'ie', 'i.e', 'ifyp', 'IG', 'iirc', 'ilu', 'ily', 'imho', 'imo', 'imu', 'iow', 'irl', 'j4f', 'jic', 'jk', 'jsyk', 'l8r', 'lb', 'lbs', 'ldr', 'lmao', 'lmfao', 'lol', 'ltd', 'ltns', 'm8', 'mf', 'mfs', 'mfw', 'mofo', 'mph', 'mr', 'mrw', 'ms', 'mte', 'nagi', 'nbc', 'nbd', 'nfs', 'ngl', 'nhs', 'nrn', 'nsfl', 'nsfw', 'nth', 'nvr', 'nyc', 'oc', 'og', 'ohp', 'oic', 'omdb', 'omg', 'omw', 'p.a', 'p.m', 'pm', 'poc', 'pov', 'pp', 'ppl', 'prw', 'ps', 'pt', 'ptb', 'pto', 'qpsa', 'ratchet', 'rbtl', 'rlrt', 'rofl', 'roflol', 'rotflmao', 'rt', 'ruok', 'sfw', 'sk8', 'smh', 'sq', 'srsly', 'ssdd', 'tbh', 'tbs', 'tbsp', 'tfw', 'thks', 'tho', 'thx', 'tia', 'til', 'tl;dr', 'tldr', 'tmb', 'tntl', 'ttyl', 'u', 'u2', 'u4e', 'utc', 'w/', 'w/o', 'w8', 'wassup', 'wb', 'wtf', 'wtg', 'wtpa', 'wuf', 'wuzup', 'wywh', 'yd', 'ygtr', 'ynk', 'zzz'])"]},"metadata":{},"execution_count":44}],"source":["global_vars.SLANG_DT.keys()"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"zqrk5TEuAzzq","outputId":"389b972b-12c9-41c8-dc05-a21132b34d89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt',\n"," 'models_titles_dt']"]},"metadata":{},"execution_count":45}],"source":["dir(global_vars)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"wdTV7rVOAsNO","outputId":"059aa626-a280-46a9-93f8-740f9906a5fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Variable                Type             Data/Info\n","--------------------------------------------------\n","Corpus_Genre            str              finance\n","Corpus_Number           int              2\n","Corpus_Type             str              reference\n","EMOTICONS_EMO           dict             n=221\n","IN_COLAB                bool             True\n","Image                   type             <class 'IPython.core.display.Image'>\n","InteractiveShell        MetaHasTraits    <class 'IPython.core.inte<...>eshell.InteractiveShell'>\n","PATH_TEXT_CLEAN         str              ./text_clean/text_clean_finance_ref\n","PATH_TEXT_RAW           str              ./text_raw/text_raw_finance_ref\n","PATH_UTILS              str              /gdrive/MyDrive/sentimentarcs_notebooks/utils\n","Path                    type             <class 'pathlib.Path'>\n","Path_to_SentimentArcs   str              /gdrive/MyDrive/sentimentarcs_notebooks/\n","PySBDFactory            type             <class 'pysbd.utils.PySBDFactory'>\n","SUBDIR_TEXT_CLEAN       str              ./text_clean/text_clean_finance_ref\n","SUBDIR_TEXT_RAW         str              text_raw_finance_ref\n","UNICODE_EMOJI           dict             n=3521\n","contractions            module           <module 'contractions' fr<...>ontractions/__init__.py'>\n","corpus_titles_ls        dict_keys        dict_keys(['federalreserv<...>'european_central_bank'])\n","datetime                type             <class 'datetime.datetime'>\n","deepcopy                function         <function deepcopy at 0x7fc806412830>\n","display                 function         <function display at 0x7fc804d14290>\n","doc                     Doc              Apples and oranges are si<...> Boots and hippos aren't.\n","drive                   module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n","emot                    module           <module 'emot' from '/usr<...>ckages/emot/__init__.py'>\n","emot_obj                emot             <emot.core.emot object at 0x7fc750907550>\n","glob                    module           <module 'glob' from '/usr/lib/python3.7/glob.py'>\n","global_vars             module           <module 'global_vars' fro<...>ks/utils/global_vars.py'>\n","hero                    module           <module 'texthero' from '<...>es/texthero/__init__.py'>\n","interactive             MetaHasTraits    <class 'ipywidgets.widget<...>interaction.interactive'>\n","json                    module           <module 'json' from '/usr<...>hon3.7/json/__init__.py'>\n","logging                 module           <module 'logging' from '/<...>3.7/logging/__init__.py'>\n","model_titles_ls         dict_keys        dict_keys(['AutoGluon_Tex<...>ER', 'AFINN', 'XGBoost'])\n","nlp                     English          <spacy.lang.en.English object at 0x7fc74cb32dd0>\n","nltk                    module           <module 'nltk' from '/usr<...>ckages/nltk/__init__.py'>\n","np                      module           <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","os                      module           <module 'os' from '/usr/lib/python3.7/os.py'>\n","pd                      module           <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","plt                     module           <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n","preprocessing           module           <module 'texthero.preproc<...>xthero/preprocessing.py'>\n","re                      module           <module 're' from '/usr/lib/python3.7/re.py'>\n","read_yaml               module           <module 'utils.read_yaml'<...>ooks/utils/read_yaml.py'>\n","sa_config               module           <module 'utils.sa_config'<...>ooks/utils/sa_config.py'>\n","sent_tokenize           function         <function sent_tokenize at 0x7fc7d5899d40>\n","sns                     module           <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n","spacy                   module           <module 'spacy' from '/us<...>kages/spacy/__init__.py'>\n","string                  module           <module 'string' from '/u<...>lib/python3.7/string.py'>\n","sys                     module           <module 'sys' (built-in)>\n","text                    str              I love python ☮ 🙂 ❤ :-) :-( :-)))\n","token                   Token            .\n","warnings                module           <module 'warnings' from '<...>b/python3.7/warnings.py'>\n","widgets                 module           <module 'ipywidgets.widge<...>ets/widgets/__init__.py'>\n","yaml                    module           <module 'yaml' from '/usr<...>ckages/yaml/__init__.py'>\n"]}],"source":["%whos"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"j1Lp4GLndZhY","outputId":"6bdd18a2-6a03-4304-9563-d5dd2deb589b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"mine,will,that,made,until,our,both,off,are,’ll,this,many,whether,but,herself,by,get,since,move,two,’m,‘ll,as,thus,with,much,me,doing,it,do,him,anyway,‘s,so,already,full,sometimes,might,whoever,seeming,please,after,everything,none,bottom,anyhow,become,across,very,always,’d,mostly,eleven,most,quite,empty,although,n‘t,yours,seems,not,then,n’t,really,also,nor,these,never,own,front,becoming,who,every,various,amongst,seemed,its,'ve,indeed,am,perhaps,few,thence,together,myself,through,a,ever,forty,above,down,via,on,side,others,ours,itself,however,here,himself,throughout,behind,amount,four,i,while,meanwhile,anyone,where,were,can,other,all,back,see,three,up,nevertheless,be,hereafter,make,serious,go,‘m,top,may,now,nothing,how,take,further,there,in,whence,once,show,if,formerly,beyond,wherein,during,had,becomes,no,'m,keep,next,whatever,could,somehow,last,even,latterly,twelve,everywhere,due,’s,to,seem,should,name,along,part,being,did,'s,from,upon,still,something,whereupon,them,under,which,elsewhere,whom,against,you,than,namely,someone,except,done,she,ca,first,why,wherever,third,whereby,sixty,whose,those,is,he,nine,hundred,they,onto,whither,though,enough,'ll,what,her,anywhere,thereby,unless,same,over,six,'d,of,ten,beforehand,whenever,often,eight,toward,my,therefore,beside,‘re,’ve,around,re,noone,before,became,yourselves,several,anything,the,whereas,almost,because,when,thru,out,themselves,yourself,one,any,hers,fifteen,at,sometime,their,former,call,besides,into,give,’re,has,well,afterwards,‘ve,everyone,among,nobody,within,otherwise,either,n't,your,just,hereupon,or,thereafter,again,was,have,another,regarding,'re,each,must,whole,we,some,per,thereupon,neither,fifty,‘d,towards,say,least,us,too,yet,rather,put,five,nowhere,hence,below,using,his,twenty,only,used,alone,such,an,and,without,hereby,else,herein,more,ourselves,moreover,for,latter,about,therein,would,whereafter,less,somewhere,cannot,been,does,between\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47},{"output_type":"stream","name":"stdout","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n"]}],"source":["# Verify English Stopword List\n","\n","stopwords_spacy_en_ls = nlp.Defaults.stop_words\n","\n","','.join([x for x in stopwords_spacy_en_ls])\n","\n","stopwords_en_ls = stopwords_spacy_en_ls\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')"]},{"cell_type":"markdown","metadata":{"id":"ZD8Qe-H12p6j"},"source":["## (Optional) Customize Stopword List (add/del)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189546521,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Q_438Act0jds","outputId":"5b86f992-2f82-4a90-abc0-3a0158d58b52"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n","    Deleting these stopwords: {'a', 'jimmy', 'but', 'an', 'the', 'dean', 'yet'}\n","    Adding these stopwords: {'a', 'an', 'but', 'the', 'yet'}\n","\n","Final Count: 326 Stopwords\n"]}],"source":["# Customize Default SpaCy English Stopword List\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')\n","\n","# [CUSTOMIZE] Stopwords to ADD or DELETE from default spaCy English stopword list\n","LOCAL_STOPWORDS_DEL_EN = set(global_vars.STOPWORDS_DEL_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Deleting these stopwords: {LOCAL_STOPWORDS_DEL_EN}')\n","LOCAL_STOPWORDS_ADD_EN = set(global_vars.STOPWORDS_ADD_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Adding these stopwords: {LOCAL_STOPWORDS_ADD_EN}\\n')\n","\n","stopwords_en_ls = list(set(stopwords_spacy_en_ls).difference(set(LOCAL_STOPWORDS_DEL_EN)).union(set(LOCAL_STOPWORDS_ADD_EN)))\n","print(f'Final Count: {len(stopwords_en_ls)} Stopwords')"]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1649189547999,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"xmf7SFb7PnUi","outputId":"d8a8b8ac-ac9e-4425-8009-6833364fb35a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n"," New figure size:  (20, 10)\n","Matplotlib Configuration ------------------------------\n","\n","  (Uncomment to view)\n","\n","  Edit ./utils/config_matplotlib.py to change\n"]}],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1649189548709,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"hM3oRY-UOmzX","outputId":"c7a2e502-64a2-43bb-b180-cf575d968c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Seaborn Configuration ------------------------------\n","\n"]}],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')"]},{"cell_type":"markdown","metadata":{"id":"xBpIUgstnE62"},"source":["## **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"JXG_G6um4ijG"},"source":["### Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1649189551096,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"TO08GFoGlP3y","outputId":"b36507a0-cc0d-4ba6-cedc-8ea5a11fcad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary: corpus_titles_dt\n"]},{"output_type":"execute_result","data":{"text/plain":["{'european_central_bank': ['European Central Bank Speeches',\n","  datetime.date(1998, 7, 17),\n","  datetime.date(2022, 3, 1)],\n"," 'federalreserve_speech_bog': ['Federal Reserve Board of Governor Speeches',\n","  datetime.date(1997, 1, 8),\n","  datetime.date(2022, 2, 25)]}"]},"metadata":{},"execution_count":51},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Corpus Texts:\n","  federalreserve_speech_bog\n","  european_central_bank\n","\n","\n","\n","Natural Corpus Titles:\n","  Federal Reserve Board of Governor Speeches\n","  European Central Bank Speeches\n"]}],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535,"status":"ok","timestamp":1649189563379,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"rQNlQr4_Ckb1","outputId":"9dcb4e99-3462-4804-a00b-8094784ac05a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","There are 12 Lexicon Models\n","  Lexicon Model #0: sentimentr_sentimentr\n","  Lexicon Model #1: pysentimentr_jockersrinker\n","  Lexicon Model #2: pysentimentr_huliu\n","  Lexicon Model #3: pysentimentr_nrc\n","  Lexicon Model #4: pysentimentr_sentiword\n","  Lexicon Model #5: pysentimentr_senticnet\n","  Lexicon Model #6: pysentimentr_lmcd\n","  Lexicon Model #7: syuzhetr_afinn\n","  Lexicon Model #8: syuzhetr_bing\n","  Lexicon Model #9: syuzhetr_nrc\n","  Lexicon Model #10: syuzhetr_syuzhetr\n","  Lexicon Model #11: afinn\n","\n","There are 9 Heuristic Models\n","  Heuristic Model #0: pattern\n","  Heuristic Model #1: sentimentr_jockersrinker\n","  Heuristic Model #2: sentimentr_jockers\n","  Heuristic Model #3: sentimentr_bing\n","  Heuristic Model #4: sentimentr_nrc\n","  Heuristic Model #5: sentimentr_sentiword\n","  Heuristic Model #6: sentimentr_senticnet\n","  Heuristic Model #7: sentimentr_lmcd\n","  Heuristic Model #8: vader\n","\n","There are 8 Traditional ML Models\n","  Traditional ML Model #0: autogluon\n","  Traditional ML Model #1: flaml\n","  Traditional ML Model #2: logreg\n","  Traditional ML Model #3: logreg_cv\n","  Traditional ML Model #4: multinb\n","  Traditional ML Model #5: rf\n","  Traditional ML Model #6: textblob\n","  Traditional ML Model #7: xgb\n","\n","There are 5 DNN Models\n","  DNN Model #0: cnn\n","  DNN Model #1: fcn\n","  DNN Model #2: flair\n","  DNN Model #3: lstm\n","  DNN Model #4: stanza\n","\n","There are 8 Transformer Models\n","  Transformer Model #0: imdb2way\n","  Transformer Model #1: hinglish\n","  Transformer Model #2: nlptown\n","  Transformer Model #3: yelp\n","  Transformer Model #4: huggingface\n","  Transformer Model #5: roberta15lg\n","  Transformer Model #6: robertaxml8lang\n","  Transformer Model #7: t5imdb50k\n","\n","There are 5 Total Models:\n","  Model # 0: lexicon\n","  Model # 1: heuristic\n","  Model # 2: ml\n","  Model # 3: dnn\n","  Model # 4: transformer\n","\n","There are 5 Total Models (+1 for Ensemble Mean)\n","\n","Test: Lexicon Family of Models:\n"]},{"output_type":"execute_result","data":{"text/plain":["['sentimentr_sentimentr',\n"," 'pysentimentr_jockersrinker',\n"," 'pysentimentr_huliu',\n"," 'pysentimentr_nrc',\n"," 'pysentimentr_sentiword',\n"," 'pysentimentr_senticnet',\n"," 'pysentimentr_lmcd',\n"," 'syuzhetr_afinn',\n"," 'syuzhetr_bing',\n"," 'syuzhetr_nrc',\n"," 'syuzhetr_syuzhetr',\n"," 'afinn']"]},"metadata":{},"execution_count":52}],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"]},{"cell_type":"markdown","metadata":{"id":"pjQBAoLjOzDO"},"source":["### Text Cleaning "]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1649189569965,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"qpchjKtfy2H4","outputId":"d26984d4-d26d-406a-cbcd-19c772ad07bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<function texthero.preprocessing.fillna>,\n"," <function texthero.preprocessing.lowercase>,\n"," <function texthero.preprocessing.remove_digits>,\n"," <function texthero.preprocessing.remove_punctuation>,\n"," <function texthero.preprocessing.remove_diacritics>,\n"," <function texthero.preprocessing.remove_stopwords>,\n"," <function texthero.preprocessing.remove_whitespace>]"]},"metadata":{},"execution_count":53}],"source":["# [VERIFY]: Texthero preprocessing pipeline\n","\n","hero.preprocessing.get_default_pipeline()\n","\n","\n","\n","# Create Default and Custom Stemming TextHero pipeline\n","\n","# Create a custom cleaning pipeline\n","def_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                # , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace]\n","\n","# Create a custom cleaning pipeline\n","stem_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace\n","                , preprocessing.stem]\n","                   \n","# Test: pass the custom_pipeline to the pipeline argument\n","# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":568,"status":"ok","timestamp":1649189570707,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"J2DLzn1TJ12C","outputId":"4711f227-f001-435e-f788-7bb7a68522bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Test #1:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i be run late for a meeting with all the many people .',\n"," 'what time be it when you fall down run away from a grow problem ?',\n"," 'you have get to be kid me - you be joke right ?']"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","Test #2:\n","\n","['I', 'will', 'not', 'go', 'and', 'you', 'can', 'not', 'make', 'me', '.']\n","['Billy', 'be', 'run', 'really', 'quickly', 'and', 'with', 'great', 'haste', '.']\n","['Eating', 'freshly', 'catch', 'seafood', '.']\n","\n","Test #3:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i will not go and you can not make me .',\n"," 'billy be run really quickly and with great haste .',\n"," 'eating freshly catch seafood .']"]},"metadata":{},"execution_count":54}],"source":["# Test Text Cleaning Functions\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/text_cleaners.py'\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# test_suite_ls = []\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1649189570882,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"FNiSgwZqElpJ","outputId":"96817d2b-c49f-4f19-b79b-9acd7dd316af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Test: lemma_pipe()\\nif \\'lemma_pipe\\' in test_suite_ls:\\n  print(\\'\\nTest #1:\\n\\')\\n  test_ls = [\\'I am running late for a meetings with all the many people.\\',\\n            \\'What time is it when you fall down running away from a growing problem?\\',\\n            \"You\\'ve got to be kidding me - you\\'re joking right?\"]\\n  lemma_pipe(test_ls)\\n  print(\\'\\nTest #2:\\n\\')\\n  texts = pd.Series([\"I won\\'t go and you can\\'t make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\\n  for doc in nlp.pipe(texts):\\n    print([tok.lemma_ for tok in doc])\\n  print(\\'\\nTest #3:\\n\\')\\n  lemma_pipe(texts)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}],"source":["# Test Text Cleaning Functions\n","\n","%run -i './utils/text_cleaners.py'\n","# from utils.text_cleaners import text2lemmas, text_str2sents, emojis2text, expand_slang, clean_text, lemma_pipe\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# Comment out this line to active tests above\n","# test_suite_ls = []\n","\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\"\"\"\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WlfujTpEKhCp"},"source":["### File Functions"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"yOX-fpiuApL4","executionInfo":{"status":"ok","timestamp":1649189571381,"user_tz":240,"elapsed":147,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"Ilz5X9AEbP8r"},"source":["# **[STEP 2] Read in Corpus and Clean**"]},{"cell_type":"markdown","metadata":{"id":"1a-1wyeiGt4Z"},"source":["## Create List of Raw Textfiles"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1649189574720,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tcl8BtfGfvgZ","outputId":"ca187b21-ec38-402f-e99c-fcf26d1b3348"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/gdrive/MyDrive/sentimentarcs_notebooks/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["global_vars.SUBDIR_SENTIMENTARCS"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189574857,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"cHXNIMtPfZ1j","outputId":"04456698-0817-44fe-9ae4-fd6df401ebcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["path_text_raw: ./text_raw/text_raw_finance_reference\n","\n","Full Path to Corpus text_raw: ./text_raw/./text_raw/text_raw_finance_reference/\n"]}],"source":["# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","path_text_raw = './' + '/'.join(global_vars.SUBDIR_TEXT_RAW.split('/')[1:-1])\n","print(f'path_text_raw: {path_text_raw}\\n')\n","# SUBDIR_TEXT_RAW = path_text_raw + '/'\n","print(f'Full Path to Corpus text_raw: ./text_raw/{global_vars.SUBDIR_TEXT_RAW}')"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1649189575037,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"eyDzL7zJfYZf","outputId":"8bf1bf9b-e27d-4e4e-b923-2b194c470aa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1649189575037,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"iXN4pdZReL4Y","outputId":"136f1fab-a82c-45d5-dbf8-986c1dcd76d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus_Genre: finance\n","Corpus_Type: reference\n","\n","Corpus Subdir: ./text_raw/./text_raw/text_raw_finance_reference/\n","\n","texts_raw_root_ls:\n","  []\n","\n","\n","There are 0 Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]\n"]}],"source":["# Get a list of all the Textfile filename roots in Subdir text_raw\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","\n","print(f'Corpus_Genre: {global_vars.Corpus_Genre}')\n","print(f'Corpus_Type: {global_vars.Corpus_Type}\\n')\n","\n","# Build path to Corpus Subdir\n","# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","# path_text_raw = './' + '/'.join(SUBDIR_TEXT_RAW.split('/')[1:-1]) + '/' + SUBDIR_TEXT_RAW\n","path_text_raw = './text_raw/' + global_vars.SUBDIR_TEXT_RAW\n","print(f'Corpus Subdir: {path_text_raw}')\n","\n","# Create a List (preprocessed_ls) of all preprocessed text files\n","try:\n","  # texts_raw_ls = glob.glob(f'{SUBDIR_TEXT_RAW}*.txt')\n","  texts_raw_root_ls = glob.glob(f'{path_text_raw}/*.txt')\n","  texts_raw_root_ls = [x.split('/')[-1] for x in texts_raw_root_ls]\n","  texts_raw_root_ls = [x.split('.')[0] for x in texts_raw_root_ls]\n","except IndexError:\n","  raise RuntimeError('No *.txt files found')\n","\n","print(f'\\ntexts_raw_root_ls:\\n  {texts_raw_root_ls}\\n')\n","\n","text_ct = 0\n","for afile_root in texts_raw_root_ls:\n","  # file_root = file_fullpath.split('/')[-1].split('.')[0]\n","  text_ct += 1\n","  print(f'{afile_root}: ') # {corpus_titles_dt[afile_root]}')\n","\n","print(f'\\nThere are {text_ct} Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]')"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1649189575190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"lKpausmwhXJU","outputId":"5e9c3bd3-704a-4479-ec8c-8e559d989ae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access './text_raw/./text_raw/text_raw_finance_reference/': No such file or directory\n"]}],"source":["!ls -altr $path_text_raw"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189575190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"EvqS1TQthfxF","outputId":"ede4e56d-e9fa-4e25-c022-d9f19ab340b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":62}],"source":["glob.glob(f'{path_text_raw}/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"KkXBipRrGoCQ"},"source":["## Read and Segment into Sentences"]},{"cell_type":"code","source":["corpus_titles_ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jP4fbmMuqwe","executionInfo":{"status":"ok","timestamp":1649189577800,"user_tz":240,"elapsed":146,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"5f089a64-df36-48f0-9258-0349af67dabb"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['federalreserve_speech_bog', 'european_central_bank']"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq50LyAMHKYX"},"outputs":[],"source":["%%time\n","%%capture\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE:   3m30s Entire Corpus of 25 \n","#         7m30s Ref Corpus 32 Novels\n","#         7m24s Ref Corpus 32 Novels\n","#         1m00s New Corpus 2 Novels\n","\n","#        00m00s Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","\n","for i, file_root in enumerate(corpus_titles_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  # print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  global_vars.corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","global_vars.corpus_texts_dt[corpus_titles_ls[0]].head()\n"]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"zYeZnVSbuiZt","executionInfo":{"status":"aborted","timestamp":1649189577956,"user_tz":240,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls text_clean/text_clean_novels_new_corpus1"],"metadata":{"id":"OJzpbeqRu7Rh","executionInfo":{"status":"aborted","timestamp":1649189577957,"user_tz":240,"elapsed":4,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tw-Ll-fdI_yb"},"source":["## Clean Sentences"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPrpL1wyNPva","outputId":"6fbfa631-5534-4d1c-9100-656c3882d3fd","executionInfo":{"status":"ok","timestamp":1649181805916,"user_tz":240,"elapsed":13550,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Novel #0: tmorrison_songofsolomon...\n","  shape: (10507, 2)\n","Processing Novel #1: cliu_threebodyproblem...\n","  shape: (9081, 2)\n","Processing Novel #2: sking_doctorsleep...\n","  shape: (15754, 2)\n","CPU times: user 12.9 s, sys: 459 ms, total: 13.4 s\n","Wall time: 14 s\n"]}],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s (24 Novels)\n","#       (w/ stem) 4m24s (24 Novels)\n","\n","i = 0\n","\n","for key_novel, atext_df in global_vars.corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('empty_placeholder')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":839},"id":"RXuwwg2_XgZu","outputId":"6b16d990-aa42-466b-c966-783dec720ee6","executionInfo":{"status":"ok","timestamp":1649181805917,"user_tz":240,"elapsed":11,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             text_raw  \\\n","0   The North Carolina Mutual Life Insurance agent...   \n","1   Two days before the event was to take place he...   \n","2   At 3:00 p.m. on Wednesday the 18th of February...   \n","3                                  Please forgive me.   \n","4                                    I loved you all.   \n","5                              (signed) Robert Smith,   \n","6                                                Ins.   \n","7                                               agent   \n","8   Mr. Smith didnt draw as big a crowd as Lindber...   \n","9   At that time of day, during the middle of the ...   \n","10  Children were in school; men were at work; and...   \n","11  Only the unemployed, the self-employed, and th...   \n","12  Town maps registered the street as Mains Avenu...   \n","13  Later, when other Negroes moved there, and whe...   \n","14  The post office workers returned these envelop...   \n","15  Then in 1918, when colored men were being draf...   \n","16  In that way, the name acquired a quasi-officia...   \n","17                                  But not for long.   \n","18  Some of the city legislators, whose concern fo...   \n","19  And since they knew that only Southside reside...   \n","\n","                                           text_clean  \n","0   the north carolina mutual life insurance agent...  \n","1   two day before the event be to take place he t...  \n","2   at p be on wednesday the 18th of february i wi...  \n","3                                   please forgive me  \n","4                                      i love you all  \n","5                                   sign robert smith  \n","6                                                 ins  \n","7                                               agent  \n","8   mr smith do not draw a big a crowd a lindbergh...  \n","9   at that time of day during the middle of the w...  \n","10  child be in school man be at work and much of ...  \n","11  only the unemployed the self employ and the ve...  \n","12  town map register the street a main avenue but...  \n","13  late when other negroes move there and when th...  \n","14  the post office worker return this envelope or...  \n","15  then in when color man be be draft a few give ...  \n","16  in that way the name acquire a quasi official ...  \n","17                                   but not for long  \n","18  some of the city legislator whose concern for ...  \n","19  and since they know that only southside reside...  "],"text/html":["\n","  <div id=\"df-e66e9c27-c12d-4fcb-8b3e-3b0a3f808c59\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_raw</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The North Carolina Mutual Life Insurance agent...</td>\n","      <td>the north carolina mutual life insurance agent...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Two days before the event was to take place he...</td>\n","      <td>two day before the event be to take place he t...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>At 3:00 p.m. on Wednesday the 18th of February...</td>\n","      <td>at p be on wednesday the 18th of february i wi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Please forgive me.</td>\n","      <td>please forgive me</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I loved you all.</td>\n","      <td>i love you all</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>(signed) Robert Smith,</td>\n","      <td>sign robert smith</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Ins.</td>\n","      <td>ins</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>agent</td>\n","      <td>agent</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Mr. Smith didnt draw as big a crowd as Lindber...</td>\n","      <td>mr smith do not draw a big a crowd a lindbergh...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>At that time of day, during the middle of the ...</td>\n","      <td>at that time of day during the middle of the w...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Children were in school; men were at work; and...</td>\n","      <td>child be in school man be at work and much of ...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Only the unemployed, the self-employed, and th...</td>\n","      <td>only the unemployed the self employ and the ve...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Town maps registered the street as Mains Avenu...</td>\n","      <td>town map register the street a main avenue but...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Later, when other Negroes moved there, and whe...</td>\n","      <td>late when other negroes move there and when th...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>The post office workers returned these envelop...</td>\n","      <td>the post office worker return this envelope or...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Then in 1918, when colored men were being draf...</td>\n","      <td>then in when color man be be draft a few give ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>In that way, the name acquired a quasi-officia...</td>\n","      <td>in that way the name acquire a quasi official ...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>But not for long.</td>\n","      <td>but not for long</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Some of the city legislators, whose concern fo...</td>\n","      <td>some of the city legislator whose concern for ...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>And since they knew that only Southside reside...</td>\n","      <td>and since they know that only southside reside...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e66e9c27-c12d-4fcb-8b3e-3b0a3f808c59')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e66e9c27-c12d-4fcb-8b3e-3b0a3f808c59 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e66e9c27-c12d-4fcb-8b3e-3b0a3f808c59');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":53},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10507 entries, 0 to 10506\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   text_raw    10507 non-null  object\n"," 1   text_clean  10507 non-null  string\n","dtypes: object(1), string(1)\n","memory usage: 164.3+ KB\n"]}],"source":["# Verify the first Text in Corpus is cleaned\n","\n","global_vars.corpus_texts_dt[corpus_titles_ls[0]].head(20)\n","global_vars.corpus_texts_dt[corpus_titles_ls[0]].info()"]},{"cell_type":"markdown","metadata":{"id":"WAjjOEFx7F5J"},"source":["## Save Cleaned Corpus"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CrH24Dv7YwK","outputId":"3e5c8efc-100d-4c81-b6df-6204eda87ddf","executionInfo":{"status":"ok","timestamp":1649181806066,"user_tz":240,"elapsed":156,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Currently in SentimentArcs root directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","Saving Clean Texts to Subdir: ./text_clean/text_clean_novels_new_corpus2/\n","\n","Saving these Texts:\n","  dict_keys(['tmorrison_songofsolomon', 'cliu_threebodyproblem', 'sking_doctorsleep'])\n"]}],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nSaving Clean Texts to Subdir: {SUBDIR_TEXT_CLEAN}')\n","print(f'\\nSaving these Texts:\\n  {global_vars.corpus_texts_dt.keys()}')"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgcmvfbvXqfy","outputId":"81f76563-2b74-4f55-d7f8-6acd8508b580","executionInfo":{"status":"ok","timestamp":1649181807188,"user_tz":240,"elapsed":1124,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving Novel #0 to ./text_clean/text_clean_novels_new_corpus2/tmorrison_songofsolomon.csv\n","Saving Novel #1 to ./text_clean/text_clean_novels_new_corpus2/cliu_threebodyproblem.csv\n","Saving Novel #2 to ./text_clean/text_clean_novels_new_corpus2/sking_doctorsleep.csv\n"]}],"source":["# Save the cleaned Textfiles\n","\n","i = 0\n","for key_novel, anovel_df in global_vars.corpus_texts_dt.items():\n","  anovel_fname = f'{key_novel}.csv'\n","\n","  anovel_fullpath = f'{SUBDIR_TEXT_CLEAN}{anovel_fname}'\n","  print(f'Saving Novel #{i} to {anovel_fullpath}')\n","  global_vars.corpus_texts_dt[key_novel].to_csv(anovel_fullpath)\n","  i += 1"]},{"cell_type":"markdown","metadata":{"id":"_348z09gQKe3"},"source":["# **[END OF NOTEBOOK]**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mGoFJmeFkTxk","ajD8hCbzkStO","umZfB0YCqajW","ZD8Qe-H12p6j","EA1yTaY_9Qod","pjQBAoLjOzDO","WlfujTpEKhCp"],"name":"sentiment_arcs_part1_text_preprocessing.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}