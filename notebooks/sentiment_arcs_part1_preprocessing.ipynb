{"cells":[{"cell_type":"markdown","metadata":{"id":"3i0Fg4SYB7g0"},"source":["# **SentimentArcs (Part 1): Text Preprocessing**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","Welcome! \n","\n","SentimentArcs is a methodlogy and software framework for analyzing narrative in text. Virtually all long text contains narrative elements...(TODO: Insert excerpts from Paper Abstract/Intro Sections here)\n","\n","***\n","\n","* **SentimentArcs: Cloning the Github repository to your gDrive**\n","\n","If this is the first time using SentimentArcs, you will need to copy the software from our Github.com repository (github repo). The default recommended gDrive path is ./gdrive/MyDrive/research/sentiment_arcs/'. \n","\n","The first time you run this notebook and connect your Google gDrive, it will allow to to specify the path to your SentimentArcs subdirectory. If it does not exists, this notebook will copy/clone the SentimentArcs github repository code to your gDrive at the path you specify.\n","\n","\n","***\n","\n","* **NovelText: A Reference Corpus of 24 Diverse Novel**\n","\n","Sentiment Arcs comes with a carefully curated reference corpus of Novels to illustrate the unique diachronic sentiment analysis characteristic of long form fictional narrativeas. This corpus of 24 diverse novels also provides a baseline for exploring and comparing new novels with sentiment analysis using SentimentArcs.\n","\n","***\n","\n","* **Preparing New Novels: Formatting and adding to subdirectory**\n","\n","To analyze new novels with SentimentArcs, the body of the text should consist of plain text organized in to blocks separated by two newlines which visually look like a single blank line between blocks. These blocks are usually paragraphs but can also include title headers, separate lines of dialog or quotes. Please reference any of the 24 novels in the NovelText corpus for examples of this expected format.\n","\n","Once the new novel is correctly formatted as a plain text file, it should follow this standard file naming convention:\n","\n","[first letter of first name]+[full lastname]_[abbreviated book title].txt\n","\n","Examples:\n","\n","* fdouglass_narrativelifeofaslave\n","* fscottfitzgerald_thegreatgatsby.txt\n","* vwoolf_mrsdalloway.txt\n","* homer-ewilson_odyssey.txt (trans. E.Wilson)\n","* mproust-mtreharne_3guermantesway.txt (Book 3, trans. M.Treharne)\n","* staugustine_confessions9end.txt (Upto and incl Book 9)\n","\n","Note the optional author suffix (-translator) and optional title suffix (-selected chapters/books)\n","\n","***\n","\n","* **Adding New Novels: Add file to subdirectory and Update this Notebook**\n","\n","Once you have a cleaned and text file named according the standard rule above, you must move that file to the subdirectory of all input novels and update the global variable in this notebook that defines which novels to analyze.\n","\n","First, copy your cleaned text file to the subdirectory containing all novels read by this notebook. This subdir is defined by the program variable 'subdir_novels' with the default value './in1_novels/'\n","\n","Second, update the program variable 'novels_dt'. This is a Dictionary data structure that following the pattern below:\n","```\n","novels_dt = {\n","  'cdickens_achristmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n","```\n","Where the first string (the dictionary key) must match the filename root without the '.txt' suffix (e.g. cdickens_achristmascarol). The Dictionary value after the ':' is a list of three elements:\n","\n","* A nicely formatted string of the form '(title) by (full first and last name of author)' that should be a human friendly string used to label plots and saved files.\n","\n","* The (publication year) and the (sentence count). Both are optional, but should have placeholder string '0' if unknown. These are intended for future reference and analytics.\n","\n","* Your future self will thank you if you insert new novels into the 'novels_dt' in alphabetic order for faster and more accurate reference.\n","\n","***\n","\n","* **How to Execute SentimentArcs Notebooks:**\n","\n","This is a Jupyter Notebook created to run on Google's free Colab service using only a browers and your exiting Google email account. We chose Google Colab because it is relatively, fast, free, easy to use and makes collaboration as simple as web browsing.\n","\n","A few reminders about using Jupyter Notebooks general and SentimentArcs in particular:\n","\n","* All cells must be run ***in order*** as later code cells often depend upon the output of earlier code cells\n","\n","* ***Cells that take more time to execute*** (> 1 min) usually begin with *%%time* which outputs the *total execution time* of the last run.  This timing output is deleted and recalculated each time the code cell is executed.\n","\n","* **[OPTIONAL]** at the top of a cell indicates you *may* change a setting in that cell to customize behavior.\n","\n","* **[CUSTOMIZE]** at the top of a cell indicates you *must* change a setting in that cell.\n","\n","* **[RESTART REQUIRED]** at the top of a cell indicates you *may* see a *[RESTART REQUIRED] button* at the end of the output. *If you see this button, you must select [Runtime]->[Restart Runtime] from the top menubar.\n","\n","* **[INPUT REQUIRED]** at the top of a cell indicates you will be required to take some action for execution to proceed, usually by clicking a button or entering the response to a prompt.\n","\n","All cells with a top comment prefixed with # [OPTIONAL]: indicates that you can change a setting to customize behavior, the prefix [CUSTOMIZE] indicates you MUST set/change a setting\n","\n","* SentimentArcs divides workflow into a series of chronological Jupyter Notebooks that must be run in order. Here is an overview of the workflow:\n","\n","***\n","\n","**SentimentArcs Notebooks Workflow**\n","1. Notebook #1: Preprocess Text\n","2. Notebook #2: Compute Sentiment Values (Simple Models/CPUs)\n","3. Notebook #3: Compute Sentiment Values (Complex Models/GPUs)\n","4. Notebook #4: Combine all Sentiment Values, perform Time Series analysis, and extract Crux points and surrounding text\n","\n","If you are unfamilar with setting up and using Google Colab or Jupyter Notebooks, here are a series of resources to quickly bring you up to speed. If you are using SentimentArcs with the Cambridge University Press Elements textbook, there are also a series of videos by Prof Elkins and Chun stepping you through these notebooks.\n","\n","***\n","\n","**Additional Resources and Tutorials**\n","\n","\n","**Google Colab and Jupyter Resources:**\n","\n","* Coming...\n","* [IPython, Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/01.00-ipython-beyond-normal-python.html) \n","\n","**Cambridge University Press Videos:**\n","\n","* Coming...\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bfkqjgMiw7T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040194458,"user_tz":240,"elapsed":28672,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"28310047-9193-4d2c-9111-ea214549e3a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n","Mounted at /gdrive\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kskWCX1KyrV_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040194573,"user_tz":240,"elapsed":124,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0ed5eb95-1031-4c9f-e455-ba7c3e3f3ba4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","SUBDIR_TEXT_RAW:\n","  [text_raw_finance_reference]\n","PATH_TEXT_RAW:\n","  [./text_raw/text_raw_finance_reference]\n","SUBDIR_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_reference]\n","PATH_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_reference]\n"]}],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"finance\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 1 #@param {type:\"slider\", min:0, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","PATH_TEXT_RAW = f'./text_raw/{SUBDIR_TEXT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'SUBDIR_TEXT_RAW:\\n  [{SUBDIR_TEXT_RAW}]')\n","print(f'PATH_TEXT_RAW:\\n  [{PATH_TEXT_RAW}]')\n","\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')"]},{"cell_type":"markdown","metadata":{"id":"P00BhwLVyL8X"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRx8mIVxUyXM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040195028,"user_tz":240,"elapsed":458,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"e7c015f4-def2-41a9-8cdb-1a1f8c917d8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n","\n","\n","Contents of Subdirectory [./sentiment_arcs/utils/]\n","\n","config_matplotlib.py   get_subdirs.py\t    sentiment_analysis.py\n","config_seaborn.py      global_constants.py  sentiment_arcs_config.py\n","file_utils.py\t       global_vars.py\t    set_globals.py\n","get_fullpath.py        __init__.py\t    subdir_constants.py\n","get_model_families.py  __pycache__\t    text_cleaners_new.py\n","get_sentimentr.R       read_yaml.py\t    text_cleaners.py\n","get_sentiments.py      sa_config.py\n"]}],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBtWnOBxiw8H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040195430,"user_tz":240,"elapsed":406,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0ca84650-e6e8-415f-e447-e255f825a6b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_ensemble_dt',\n"," 'model_titles_dt']"]},"metadata":{},"execution_count":4}],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_TEXT_RAW = SUBDIR_TEXT_RAW\n","global_vars.PATH_TEXT_RAW = PATH_TEXT_RAW\n","\n","dir(global_vars)"]},{"cell_type":"markdown","metadata":{"id":"CBoEHX9Z9imD"},"source":["## (each time) Custom Libraries & Define Globals"]},{"cell_type":"code","source":["dir(global_vars)"],"metadata":{"id":"sXbFMF7qxkDu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040195574,"user_tz":240,"elapsed":151,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"173bc684-7889-4e75-b3cd-bd94130086ae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_ensemble_dt',\n"," 'model_titles_dt']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Initialize and clean for each iteration of notebook\n","\n","global_vars.corpus_texts_dt = {}\n","global_vars.corpus_titles_dt = {}"],"metadata":{"id":"HYReAP9TxoIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtaPyy4VSohJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040196064,"user_tz":240,"elapsed":493,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"97c9b970-3abb-4e79-c89f-6a9ad4ac3044"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","Objects in sa_config()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_subdirs', 'global_vars', 'set_globals']\n","\n","\n","Verify the Directory Structure:\n","\n","-------------------------------\n","\n","           [Corpus Genre]: finance\n","\n","            [Corpus Type]: reference\n","\n","\n","    [FNAME_SENTIMENT_RAW]: [NONE]\n","\n","\n","\n","\n","INPUTS:\n","-------------------------------\n","\n","   [SUBDIR_SENTIMENTARCS]: /gdrive/MyDrive/sentimentarcs_notebooks/\n","\n","\n","STEP 1: Clean Text\n","--------------------\n","\n","        [SUBDIR_TEXT_RAW]: ./text_raw/text_raw_finance_reference/\n","\n","      [SUBDIR_TEXT_CLEAN]: ./text_clean/text_clean_finance_reference/\n","\n","\n","STEP 2: Get Sentiments\n","--------------------\n","\n","   [SUBDIR_SENTIMENT_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n"," [SUBDIR_SENTIMENT_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","STEP 3: Smooth Time Series and Get Crux Points\n","--------------------\n","\n","  [SUBDIR_TIMESERIES_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n","[SUBDIR_TIMESERIES_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","\n","OUTPUTS:\n","-------------------------------\n","\n","          [SUBDIR_GRAPHS]: ./graphs/graphs_finance/\n","\n","            [SUBDIR_DATA]: ./data/data_finance\n","\n","           [SUBDIR_UTILS]: ./utils/\n","\n"]}],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tx8j_3Y6qQna","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040196065,"user_tz":240,"elapsed":7,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"686078d1-9548-41a1-cca4-7ec369e41cba"},"outputs":[{"output_type":"stream","name":"stdout","text":["MIN_PARAG_LEN: 10\n","STOPWORDS_ADD_EN: ['a', 'the', 'an']\n","TEST_WORDS_LS: ['Love', 'Hate', 'bizarre', 'strange', 'furious', 'elated', 'curious', 'beserk', 'gambaro']\n","SLANG_DT: {'$': ' dollar ', '€': ' euro ', '4ao': 'for adults only', 'a.m': 'before midday', 'a3': 'anytime anywhere anyplace', 'aamof': 'as a matter of fact', 'acct': 'account', 'adih': 'another day in hell', 'afaic': 'as far as i am concerned', 'afaict': 'as far as i can tell', 'afaik': 'as far as i know', 'afair': 'as far as i remember', 'afk': 'away from keyboard', 'app': 'application', 'approx': 'approximately', 'apps': 'applications', 'asap': 'as soon as possible', 'asl': 'age, sex, location', 'atk': 'at the keyboard', 'ave.': 'avenue', 'aymm': 'are you my mother', 'ayor': 'at your own risk', 'b&b': 'bed and breakfast', 'b+b': 'bed and breakfast', 'b.c': 'before christ', 'b2b': 'business to business', 'b2c': 'business to customer', 'b4': 'before', 'b4n': 'bye for now', 'b@u': 'back at you', 'bae': 'before anyone else', 'bak': 'back at keyboard', 'bbbg': 'bye bye be good', 'bbc': 'british broadcasting corporation', 'bbias': 'be back in a second', 'bbl': 'be back later', 'bbs': 'be back soon', 'be4': 'before', 'bfn': 'bye for now', 'blvd': 'boulevard', 'bout': 'about', 'brb': 'be right back', 'bros': 'brothers', 'brt': 'be right there', 'bsaaw': 'big smile and a wink', 'btw': 'by the way', 'bwl': 'bursting with laughter', 'c/o': 'care of', 'cet': 'central european time', 'cf': 'compare', 'cia': 'central intelligence agency', 'csl': 'can not stop laughing', 'cu': 'see you', 'cul8r': 'see you later', 'cv': 'curriculum vitae', 'cwot': 'complete waste of time', 'cya': 'see you', 'cyt': 'see you tomorrow', 'dae': 'does anyone else', 'dbmib': 'do not bother me i am busy', 'diy': 'do it yourself', 'dm': 'direct message', 'dwh': 'during work hours', 'e123': 'easy as one two three', 'eet': 'eastern european time', 'eg': 'example', 'embm': 'early morning business meeting', 'encl': 'enclosed', 'encl.': 'enclosed', 'etc': 'and so on', 'faq': 'frequently asked questions', 'fawc': 'for anyone who cares', 'fb': 'facebook', 'fc': 'fingers crossed', 'fig': 'figure', 'fimh': 'forever in my heart', 'ft.': 'feet', 'ft': 'featuring', 'ftl': 'for the loss', 'ftw': 'for the win', 'fwiw': 'for what it is worth', 'fyi': 'for your information', 'g9': 'genius', 'gahoy': 'get a hold of yourself', 'gal': 'get a life', 'gcse': 'general certificate of secondary education', 'gfn': 'gone for now', 'gg': 'good game', 'gl': 'good luck', 'glhf': 'good luck have fun', 'gmt': 'greenwich mean time', 'gmta': 'great minds think alike', 'gn': 'good night', 'g.o.a.t': 'greatest of all time', 'goat': 'greatest of all time', 'goi': 'get over it', 'gps': 'global positioning system', 'gr8': 'great', 'gratz': 'congratulations', 'gyal': 'girl', 'h&c': 'hot and cold', 'hp': 'horsepower', 'hr': 'hour', 'hrh': 'his royal highness', 'ht': 'height', 'ibrb': 'i will be right back', 'ic': 'i see', 'icq': 'i seek you', 'icymi': 'in case you missed it', 'idc': 'i do not care', 'idgadf': 'i do not give a damn fuck', 'idgaf': 'i do not give a fuck', 'idk': 'i do not know', 'ie': 'that is', 'i.e': 'that is', 'ifyp': 'i feel your pain', 'IG': 'instagram', 'iirc': 'if i remember correctly', 'ilu': 'i love you', 'ily': 'i love you', 'imho': 'in my humble opinion', 'imo': 'in my opinion', 'imu': 'i miss you', 'iow': 'in other words', 'irl': 'in real life', 'j4f': 'just for fun', 'jic': 'just in case', 'jk': 'just kidding', 'jsyk': 'just so you know', 'l8r': 'later', 'lb': 'pound', 'lbs': 'pounds', 'ldr': 'long distance relationship', 'lmao': 'laugh my ass off', 'lmfao': 'laugh my fucking ass off', 'lol': 'laughing out loud', 'ltd': 'limited', 'ltns': 'long time no see', 'm8': 'mate', 'mf': 'motherfucker', 'mfs': 'motherfuckers', 'mfw': 'my face when', 'mofo': 'motherfucker', 'mph': 'miles per hour', 'mr': 'mister', 'mrw': 'my reaction when', 'ms': 'miss', 'mte': 'my thoughts exactly', 'nagi': 'not a good idea', 'nbc': 'national broadcasting company', 'nbd': 'not big deal', 'nfs': 'not for sale', 'ngl': 'not going to lie', 'nhs': 'national health service', 'nrn': 'no reply necessary', 'nsfl': 'not safe for life', 'nsfw': 'not safe for work', 'nth': 'nice to have', 'nvr': 'never', 'nyc': 'new york city', 'oc': 'original content', 'og': 'original', 'ohp': 'overhead projector', 'oic': 'oh i see', 'omdb': 'over my dead body', 'omg': 'oh my god', 'omw': 'on my way', 'p.a': 'per annum', 'p.m': 'after midday', 'pm': 'prime minister', 'poc': 'people of color', 'pov': 'point of view', 'pp': 'pages', 'ppl': 'people', 'prw': 'parents are watching', 'ps': 'postscript', 'pt': 'point', 'ptb': 'please text back', 'pto': 'please turn over', 'qpsa': 'what happens', 'ratchet': 'rude', 'rbtl': 'read between the lines', 'rlrt': 'real life retweet', 'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off', 'rt': 'retweet', 'ruok': 'are you ok', 'sfw': 'safe for work', 'sk8': 'skate', 'smh': 'shake my head', 'sq': 'square', 'srsly': 'seriously', 'ssdd': 'same stuff different day', 'tbh': 'to be honest', 'tbs': 'tablespooful', 'tbsp': 'tablespooful', 'tfw': 'that feeling when', 'thks': 'thank you', 'tho': 'though', 'thx': 'thank you', 'tia': 'thanks in advance', 'til': 'today i learned', 'tl;dr': 'too long i did not read', 'tldr': 'too long i did not read', 'tmb': 'tweet me back', 'tntl': 'trying not to laugh', 'ttyl': 'talk to you later', 'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'utc': 'coordinated universal time', 'w/': 'with', 'w/o': 'without', 'w8': 'wait', 'wassup': 'what is up', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go', 'wtpa': 'where the party at', 'wuf': 'where are you from', 'wuzup': 'what is up', 'wywh': 'wish you were here', 'yd': 'yard', 'ygtr': 'you got that right', 'ynk': 'you never know', 'zzz': 'sleeping bored and tired'}\n"]}],"source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kK8zKENjsyig"},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"Ns5NwArZmush"},"source":["## (each time) Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUveIcUOzYav","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040197003,"user_tz":240,"elapsed":942,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0bb1d345-a923-4677-cf7c-07931d2bca98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Objects in read_yaml()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'global_vars', 'read_corpus_yaml', 'yaml']\n","\n","\n","YAML Directory: text_raw/text_raw_finance_reference\n","YAML File: text_raw_finance_reference_info.yaml\n","SentimentArcs Model Ensemble ------------------------------\n","\n","AutoGluon_Text\n","BERT_2IMDB\n","BERT_Dual_Coding\n","BERT_Multilingual\n","BERT_Yelp\n","CNN_DNN\n","Distilled_BERT\n","FLAML_AutoML\n","Fully_Connected_Network\n","HyperOpt_CNN_Flair_AutoML\n","LSTM_DNN\n","Logistic_Regression\n","Logistic_Regression_CV\n","Multilingual_CNN_Stanza_AutoML\n","Multinomial_Naive_Bayes\n","Pattern\n","Random_Forest\n","RoBERTa_Large_15DB\n","RoBERTa_XML_8Language\n","FinBERT\n","FinBERT_Tone\n","DistilRoBERTa_FinNews\n","SentimentR_JockersRinker\n","SentimentR_Jockers\n","SentimentR_Bing\n","SentimentR_NRC\n","SentimentR_SentiWord\n","SentimentR_SenticNet\n","SentimentR_LMcD\n","SentimentR_SentimentR\n","PySentimentR_JockersRinker\n","PySentimentR_Huliu\n","PySentimentR_NRC\n","PySentimentR_SentiWord\n","PySentimentR_SenticNet\n","PySentimentR_LMcD\n","SyuzhetR_AFINN\n","SyuzhetR_Bing\n","SyuzhetR_NRC\n","SyuzhetR_SyuzhetR\n","T5_IMDB\n","TextBlob\n","VADER\n","AFINN\n","XGBoost\n","\n","\n","Corpus Texts ------------------------------\n","\n","nyfederalreserve_speeches_2007-2009\n","bankofengland_speeches_2007-2009\n","\n","\n","There are 45 Models in the SentimentArcs Ensemble above.\n","\n","\n","There are 2 Texts in the Corpus above.\n","\n","\n","\n"]}],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tz5jGrDYi9Qe"},"outputs":[],"source":["# Library to Read R datafiles from within Python programs\n","\n","# !pip install pyreadr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFXzmfPQouNR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040219479,"user_tz":240,"elapsed":22479,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"be4fe0f7-d759-477d-e29c-45cd2c863866"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 46.4 MB/s \n","\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n","\u001b[K     |████████████████████████████████| 457 kB 39.1 MB/s \n","\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 37.2 MB/s \n","\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n","Collecting typing-extensions<4.0.0.0,>=3.7.4\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Collecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[K     |████████████████████████████████| 653 kB 60.1 MB/s \n","\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 725 kB/s \n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Installing collected packages: typing-extensions, catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.1.1\n","    Uninstalling typing-extensions-4.1.1:\n","      Successfully uninstalled typing-extensions-4.1.1\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n"]}],"source":["# Powerful Industry-Grade NLP Library\n","\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sD_ZVbywJ4_e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040238799,"user_tz":240,"elapsed":19331,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"c5b2354a-b6fe-4858-92b6-52052da3658b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting texthero\n","  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n","Collecting spacy<3.0.0\n","  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.64.0)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n","Collecting nltk>=3.3\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 32.9 MB/s \n","\u001b[?25hRequirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Collecting unidecode>=1.1.1\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.8)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (3.10.0.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Collecting regex>=2021.8.3\n","  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 40.0 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.1)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 52.5 MB/s \n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n","Collecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 45.9 MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Installing collected packages: srsly, catalogue, thinc, regex, unidecode, spacy, nltk, texthero\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.3\n","    Uninstalling srsly-2.4.3:\n","      Successfully uninstalled srsly-2.4.3\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.7\n","    Uninstalling catalogue-2.0.7:\n","      Successfully uninstalled catalogue-2.0.7\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.0.15\n","    Uninstalling thinc-8.0.15:\n","      Successfully uninstalled thinc-8.0.15\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.2.4\n","    Uninstalling spacy-3.2.4:\n","      Successfully uninstalled spacy-3.2.4\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed catalogue-1.0.0 nltk-3.7 regex-2022.3.15 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.4\n"]}],"source":["# NLP Library to Simply Cleaning Text\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T94fr2ymLFgV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040245339,"user_tz":240,"elapsed":6551,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"cce897aa-d1d4-4724-bd2d-3bc989a259bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysbd\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 3.1 MB/s \n","\u001b[?25hInstalling collected packages: pysbd\n","Successfully installed pysbd-0.3.4\n"]}],"source":["# Advanced Sentence Boundry Detection Pythn Library\n","#   for splitting raw text into grammatical sentences\n","#   (can be difficult due to common motifs like Mr., ..., ?!?, etc)\n","\n","!pip install pysbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3ev2lK-MU9E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040252996,"user_tz":240,"elapsed":7664,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"458c03fa-1cb4-4f45-b8a0-bff0b8bc825c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting anyascii\n","  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n","\u001b[K     |████████████████████████████████| 287 kB 5.4 MB/s \n","\u001b[?25hCollecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 9.4 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.1 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}],"source":["# Python Library to expand contractions to aid in Sentiment Analysis\n","#   (e.g. aren't -> are not, can't -> can not)\n","\n","!pip install contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kScSfHO1Q8Y-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040259170,"user_tz":240,"elapsed":6189,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a5bf05c7-2857-40ae-8635-0299600d269c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 19 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}],"source":["# Library for dealing with Emoticons (punctuation) and Emojis (icons)\n","\n","!pip install emot"]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCRgJK2ri9Nx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040260184,"user_tz":240,"elapsed":1018,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"b421b34f-c8b6-4503-e41f-947588c6be77"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-15 16:30:59,139 : INFO : NumExpr defaulting to 2 threads.\n"]}],"source":["# Core Python Libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","import re\n","import string\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import json\n","from pathlib import Path\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pify1umf6A8K"},"outputs":[],"source":["# More advanced Sentence Tokenizier Object from PySBD\n","from pysbd.utils import PySBDFactory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEPQ67KrCO6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040261785,"user_tz":240,"elapsed":1604,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a17c972c-d7e7-45da-a70f-35deaf3d27ec"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}],"source":["# Simplier Sentence Tokenizer Object from NLTK\n","import nltk \n","from nltk.tokenize import sent_tokenize\n","\n","# Download required NLTK tokenizer data\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRYua8r7MP07","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040267397,"user_tz":240,"elapsed":5615,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"2e1180d8-b50a-482e-f805-aeebc17bd609"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-15 16:31:02,313 : INFO : 'pattern' package not found; tag filters are not available for English\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Instantiate and Import Text Cleaning Ojects into Global Variable space\n","import texthero as hero\n","from texthero import preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PBsG4WRMvrN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040268997,"user_tz":240,"elapsed":1605,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"59674e97-fbac-487c-8614-147c04cf3095"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'flag': True,\n"," 'location': [[20, 23], [24, 27], [28, 33]],\n"," 'mean': ['Happy face smiley',\n","  'Frown, sad, andry or pouting',\n","  'Very very Happy face or smiley'],\n"," 'value': [':-)', ':-(', ':-)))']}"]},"metadata":{},"execution_count":21}],"source":["# Expand contractions (e.g. can't -> can not)\n","import contractions\n","\n","# Translate emoticons :0 and emoji icons to text\n","import emot \n","emot_obj = emot.core.emot() \n","\n","from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n","\n","# Test\n","text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\" \n","emot_obj.emoticons(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPFS4MEm6MyF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040270535,"user_tz":240,"elapsed":1541,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"acf95433-2fca-4630-9ca5-88a4660180a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Token Attributes: \n"," token.text, token.pos_, token.tag_, token.dep_, token.lemma_\n","Apples                                          Apples      \n","and                                             and         \n","oranges                                         orange      \n","are                                             be          \n","similar                                         similar     \n",".                                               .           \n","Boots                                           Boots       \n","and                                             and         \n","hippos                                          hippo       \n","are         AUX         VBP                     be          \n","n't         PART        RB                      not         \n",".                                               .           \n","\n","Another Test:\n","\n","Apples      9297668116247400838           Apples      \n","and         2283656566040971221           and         \n","oranges     2208928596161743350           orange      \n","are         10382539506755952630          be          \n","similar     18166476740537071113          similar     \n",".           12646065887601541794          .           \n","Boots       18231591219755621867          Boots       \n","and         2283656566040971221           and         \n","hippos      6542994350242320795           hippo       \n","are         10382539506755952630          be          \n","n't         447765159362469301            not         \n",".           12646065887601541794          .           \n"]}],"source":["# Import spaCy, language model and setup minimal pipeline\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","# nlp.max_length = 1027203\n","nlp.max_length = 2054406\n","nlp.add_pipe(nlp.create_pipe('sentencizer')) # https://stackoverflow.com/questions/51372724/how-to-speed-up-spacy-lemmatization\n","\n","# Test some edge cases, try to find examples that break spaCy\n","doc= nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","print('\\n')\n","print(\"Token Attributes: \\n\", \"token.text, token.pos_, token.tag_, token.dep_, token.lemma_\")\n","for token in doc:\n","    # Print the text and the predicted part-of-speech tag\n","    print(\"{:<12}{:<12}{:<12}{:<12}{:<12}\".format(token.text, token.pos_, token.tag_, token.dep_, token.lemma_))\n","\n","print('\\nAnother Test:\\n')\n","doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","\n","for token in doc:\n","    print(\"{:<12}{:<30}{:<12}\".format(token.text, token.lemma, token.lemma_))"]},{"cell_type":"markdown","metadata":{"id":"umZfB0YCqajW"},"source":["## (each time to customize) Define/Customize Stopwords"]},{"cell_type":"code","source":["# Customize Default SpaCy English Stopword List\n","\n","# Verify English Stopword List\n","\n","stopwords_spacy_en_ls = nlp.Defaults.stop_words\n","\n","','.join([x for x in stopwords_spacy_en_ls])\n","\n","stopwords_en_ls = stopwords_spacy_en_ls\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')\n","\n","# [CUSTOMIZE] Stopwords to ADD or DELETE from default spaCy English stopword list\n","# Define words to keep by removing from stopwords list\n","LOCAL_STOPWORDS_DEL_EN = set(global_vars.STOPWORDS_DEL_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Deleting these stopwords: {LOCAL_STOPWORDS_DEL_EN}')\n","# Define words to remove by adding to stopword list\n","LOCAL_STOPWORDS_ADD_EN = set(global_vars.STOPWORDS_ADD_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Adding these stopwords: {LOCAL_STOPWORDS_ADD_EN}\\n')\n","\n","stopwords_en_ls = list(set(stopwords_spacy_en_ls).difference(set(LOCAL_STOPWORDS_DEL_EN)).union(set(LOCAL_STOPWORDS_ADD_EN)))\n","print(f'Final Count: {len(stopwords_en_ls)} Stopwords')"],"metadata":{"id":"FblMM9egvtw-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040270680,"user_tz":240,"elapsed":148,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a8d5a5a4-42da-4353-8110-bcd2f781f191"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"himself,might,‘ve,'m,yourselves,among,him,them,almost,using,behind,throughout,serious,myself,anywhere,yourself,i,becoming,re,that,by,ever,via,without,meanwhile,yours,two,an,through,while,to,us,next,move,most,everywhere,whoever,eleven,something,many,seem,hundred,’m,enough,were,less,whole,’d,put,whom,around,may,becomes,formerly,'ve,six,twelve,why,whether,because,did,could,hereafter,up,same,fifteen,side,at,twenty,within,ca,get,neither,your,hereby,there,nobody,these,least,further,more,who,thru,forty,one,my,as,’s,empty,indeed,seemed,anyone,for,from,beyond,nowhere,became,onto,however,or,does,go,she,everyone,itself,else,can,will,than,'d,do,'s,keep,her,former,give,so,still,herein,herself,three,be,nevertheless,has,being,must,hereupon,how,in,five,since,n‘t,across,against,you,per,on,doing,am,wherever,whatever,moreover,above,if,themselves,nor,unless,back,under,here,toward,such,fifty,which,been,perhaps,sometime,once,whereupon,never,too,whither,thence,until,it,‘re,except,very,is,ours,every,take,ourselves,those,name,yet,into,what,somehow,some,always,down,with,no,beside,nine,regarding,where,beforehand,part,last,few,of,towards,first,along,over,anything,he,show,all,n’t,not,also,sometimes,ten,latter,whereby,’re,whose,only,thereafter,although,see,again,afterwards,nothing,latterly,any,this,somewhere,have,someone,thereupon,used,'ll,made,other,another,third,when,eight,anyway,mostly,full,thus,‘m,often,would,whence,elsewhere,just,rather,quite,even,seems,due,during,though,well,bottom,before,top,and,already,either,own,its,done,whereas,everything,had,out,‘s,off,each,was,between,amongst,together,about,alone,say,make,‘d,we,otherwise,‘ll,the,become,four,anyhow,seeming,'re,please,amount,are,hers,should,’ve,’ll,his,after,sixty,namely,but,me,thereby,upon,call,cannot,below,whenever,noone,several,front,our,both,mine,wherein,they,n't,besides,really,now,others,much,therein,whereafter,various,therefore,hence,a,then,none,their\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23},{"output_type":"stream","name":"stdout","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n","    Deleting these stopwords: {'an', 'jimmy', 'a', 'but', 'dean', 'the', 'yet'}\n","    Adding these stopwords: {'a', 'an', 'but', 'the', 'yet'}\n","\n","Final Count: 326 Stopwords\n"]}]},{"cell_type":"markdown","metadata":{"id":"xBpIUgstnE62"},"source":["## **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"JXG_G6um4ijG"},"source":["### (each time) Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TO08GFoGlP3y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040270681,"user_tz":240,"elapsed":6,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"daf28e15-9c5b-4737-a381-4180bfe458fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary: corpus_titles_dt\n"]},{"output_type":"execute_result","data":{"text/plain":["{'bankofengland_speeches_2007-2009': ['European Central Bank Speeches (Jan 2007 - Dec 2009)',\n","  datetime.date(2007, 1, 12),\n","  datetime.date(2009, 12, 14)],\n"," 'nyfederalreserve_speeches_2007-2009': ['New York Federal Reserve Bank Speeches (Jan 2007 - Dec 2009)',\n","  datetime.date(2007, 1, 12),\n","  datetime.date(2009, 12, 11)]}"]},"metadata":{},"execution_count":24},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Corpus Texts:\n","  nyfederalreserve_speeches_2007-2009\n","  bankofengland_speeches_2007-2009\n","\n","\n","\n","Natural Corpus Titles:\n","  New York Federal Reserve Bank Speeches (Jan 2007 - Dec 2009)\n","  European Central Bank Speeches (Jan 2007 - Dec 2009)\n"]}],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQNlQr4_Ckb1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040271107,"user_tz":240,"elapsed":247,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"e4012a6e-4839-414f-ffba-311021badc09"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","There are 12 Lexicon Models\n","  Lexicon Model #0: sentimentr_sentimentr\n","  Lexicon Model #1: pysentimentr_jockersrinker\n","  Lexicon Model #2: pysentimentr_huliu\n","  Lexicon Model #3: pysentimentr_nrc\n","  Lexicon Model #4: pysentimentr_sentiword\n","  Lexicon Model #5: pysentimentr_senticnet\n","  Lexicon Model #6: pysentimentr_lmcd\n","  Lexicon Model #7: syuzhetr_afinn\n","  Lexicon Model #8: syuzhetr_bing\n","  Lexicon Model #9: syuzhetr_nrc\n","  Lexicon Model #10: syuzhetr_syuzhetr\n","  Lexicon Model #11: afinn\n","\n","There are 9 Heuristic Models\n","  Heuristic Model #0: pattern\n","  Heuristic Model #1: sentimentr_jockersrinker\n","  Heuristic Model #2: sentimentr_jockers\n","  Heuristic Model #3: sentimentr_bing\n","  Heuristic Model #4: sentimentr_nrc\n","  Heuristic Model #5: sentimentr_sentiword\n","  Heuristic Model #6: sentimentr_senticnet\n","  Heuristic Model #7: sentimentr_lmcd\n","  Heuristic Model #8: vader\n","\n","There are 8 Traditional ML Models\n","  Traditional ML Model #0: autogluon\n","  Traditional ML Model #1: flaml\n","  Traditional ML Model #2: logreg\n","  Traditional ML Model #3: logreg_cv\n","  Traditional ML Model #4: multinb\n","  Traditional ML Model #5: rf\n","  Traditional ML Model #6: textblob\n","  Traditional ML Model #7: xgb\n","\n","There are 5 DNN Models\n","  DNN Model #0: cnn\n","  DNN Model #1: fcn\n","  DNN Model #2: flair\n","  DNN Model #3: lstm\n","  DNN Model #4: stanza\n","\n","There are 11 Transformer Models\n","  Transformer Model #0: imdb2way\n","  Transformer Model #1: hinglish\n","  Transformer Model #2: nlptown\n","  Transformer Model #3: yelp\n","  Transformer Model #4: huggingface\n","  Transformer Model #5: roberta15lg\n","  Transformer Model #6: robertaxml8lang\n","  Transformer Model #7: finbert\n","  Transformer Model #8: finberttone\n","  Transformer Model #9: distilrobertafinnews\n","  Transformer Model #10: t5imdb50k\n","\n","There are 5 Total Models:\n","  Model # 0: lexicon\n","  Model # 1: heuristic\n","  Model # 2: ml\n","  Model # 3: dnn\n","  Model # 4: transformer\n","\n","There are 5 Total Models (+1 for Ensemble Mean)\n","\n","Test: Lexicon Family of Models:\n"]},{"output_type":"execute_result","data":{"text/plain":["['sentimentr_sentimentr',\n"," 'pysentimentr_jockersrinker',\n"," 'pysentimentr_huliu',\n"," 'pysentimentr_nrc',\n"," 'pysentimentr_sentiword',\n"," 'pysentimentr_senticnet',\n"," 'pysentimentr_lmcd',\n"," 'syuzhetr_afinn',\n"," 'syuzhetr_bing',\n"," 'syuzhetr_nrc',\n"," 'syuzhetr_syuzhetr',\n"," 'afinn']"]},"metadata":{},"execution_count":25}],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"]},{"cell_type":"markdown","metadata":{"id":"pjQBAoLjOzDO"},"source":["### Text Cleaning "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpchjKtfy2H4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040271217,"user_tz":240,"elapsed":113,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"72cfd02b-7597-476e-8404-158ac04a133b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<function texthero.preprocessing.fillna>,\n"," <function texthero.preprocessing.lowercase>,\n"," <function texthero.preprocessing.remove_digits>,\n"," <function texthero.preprocessing.remove_punctuation>,\n"," <function texthero.preprocessing.remove_diacritics>,\n"," <function texthero.preprocessing.remove_stopwords>,\n"," <function texthero.preprocessing.remove_whitespace>]"]},"metadata":{},"execution_count":26}],"source":["# [VERIFY]: Texthero preprocessing pipeline\n","\n","hero.preprocessing.get_default_pipeline()\n","\n","\n","\n","# Create Default and Custom Stemming TextHero pipeline\n","\n","# Create a custom cleaning pipeline\n","def_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                # , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace]\n","\n","# Create a custom cleaning pipeline\n","stem_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace\n","                , preprocessing.stem]\n","                   \n","# Test: pass the custom_pipeline to the pipeline argument\n","# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2DLzn1TJ12C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040272527,"user_tz":240,"elapsed":1194,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"b028a395-18f1-4295-e40a-3628dcd655d0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Test #1:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i be run late for a meeting with all the many people .',\n"," 'what time be it when you fall down run away from a grow problem ?',\n"," 'you have get to be kid me - you be joke right ?']"]},"metadata":{},"execution_count":27},{"output_type":"stream","name":"stdout","text":["\n","Test #2:\n","\n","['I', 'will', 'not', 'go', 'and', 'you', 'can', 'not', 'make', 'me', '.']\n","['Billy', 'be', 'run', 'really', 'quickly', 'and', 'with', 'great', 'haste', '.']\n","['Eating', 'freshly', 'catch', 'seafood', '.']\n","\n","Test #3:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i will not go and you can not make me .',\n"," 'billy be run really quickly and with great haste .',\n"," 'eating freshly catch seafood .']"]},"metadata":{},"execution_count":27}],"source":["# Test Text Cleaning Functions\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/text_cleaners.py'\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# test_suite_ls = []\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNiSgwZqElpJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040273341,"user_tz":240,"elapsed":817,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"228adbcb-1d24-4f58-8fd8-1bb702f13db6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":28},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":28},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Test: lemma_pipe()\\nif \\'lemma_pipe\\' in test_suite_ls:\\n  print(\\'\\nTest #1:\\n\\')\\n  test_ls = [\\'I am running late for a meetings with all the many people.\\',\\n            \\'What time is it when you fall down running away from a growing problem?\\',\\n            \"You\\'ve got to be kidding me - you\\'re joking right?\"]\\n  lemma_pipe(test_ls)\\n  print(\\'\\nTest #2:\\n\\')\\n  texts = pd.Series([\"I won\\'t go and you can\\'t make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\\n  for doc in nlp.pipe(texts):\\n    print([tok.lemma_ for tok in doc])\\n  print(\\'\\nTest #3:\\n\\')\\n  lemma_pipe(texts)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["# Test Text Cleaning Functions\n","\n","%run -i './utils/text_cleaners.py'\n","# from utils.text_cleaners import text2lemmas, text_str2sents, emojis2text, expand_slang, clean_text, lemma_pipe\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# Comment out this line to active tests above\n","# test_suite_ls = []\n","\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\"\"\"\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WlfujTpEKhCp"},"source":["### File Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOX-fpiuApL4"},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"Ilz5X9AEbP8r"},"source":["# **[STEP 3] Read in Corpus and Clean**"]},{"cell_type":"markdown","metadata":{"id":"1a-1wyeiGt4Z"},"source":["## Create List of Raw Textfiles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tcl8BtfGfvgZ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1650040273714,"user_tz":240,"elapsed":132,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"67cddeab-31d0-4af8-fb94-923e89f5443b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/gdrive/MyDrive/sentimentarcs_notebooks/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["global_vars.SUBDIR_SENTIMENTARCS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHXNIMtPfZ1j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040273874,"user_tz":240,"elapsed":163,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"137a515f-cebb-4a43-f9d0-5cc8c0f46753"},"outputs":[{"output_type":"stream","name":"stdout","text":["path_text_raw: ./text_raw/text_raw_finance_reference\n","\n","Full Path to Corpus text_raw: /gdrive/MyDrive/sentimentarcs_notebooks/text_raw/text_raw_finance_reference/\n"]}],"source":["# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","path_text_raw = './' + '/'.join(global_vars.SUBDIR_TEXT_RAW.split('/')[1:-1])\n","print(f'path_text_raw: {path_text_raw}\\n')\n","# SUBDIR_TEXT_RAW = path_text_raw + '/'\n","print(f'Full Path to Corpus text_raw: {global_vars.SUBDIR_SENTIMENTARCS}{global_vars.SUBDIR_TEXT_RAW[2:]}')"]},{"cell_type":"code","source":["%whos list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pl1xrvgwrb02","executionInfo":{"status":"ok","timestamp":1650040274016,"user_tz":240,"elapsed":147,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"fb5849a6-0611-429f-d0e6-c33b35ed9f6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Variable           Type    Data/Info\n","------------------------------------\n","corpus_texts_ls    list    n=2\n","corpus_titles_ls   list    n=2\n","def_pipeline       list    n=6\n","stem_pipeline      list    n=8\n","stopwords_en_ls    list    n=326\n","test_ls            list    n=3\n","test_suite_ls      list    n=8\n"]}]},{"cell_type":"code","source":["corpus_texts_ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTU9zjvOrd2c","executionInfo":{"status":"ok","timestamp":1650040274016,"user_tz":240,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"9b7e6d8f-4c35-4f53-c983-36eff45a35f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nyfederalreserve_speeches_2007-2009', 'bankofengland_speeches_2007-2009']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXN4pdZReL4Y"},"outputs":[],"source":["\"\"\"\n","# DELETE: Already created lists of corpus texts/titles in Utility Functions Section above\n","\n","# Get a list of all the Textfile filename roots in Subdir text_raw\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","\n","print(f'Corpus_Genre: {global_vars.Corpus_Genre}')\n","print(f'Corpus_Type: {global_vars.Corpus_Type}\\n')\n","\n","# Build path to Corpus Subdir\n","# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","# path_text_raw = './' + '/'.join(SUBDIR_TEXT_RAW.split('/')[1:-1]) + '/' + SUBDIR_TEXT_RAW\n","# path_text_raw = './text_raw' + global_vars.SUBDIR_TEXT_RAW\n","path_text_raw = global_vars.SUBDIR_TEXT_RAW\n","print(f'Corpus Subdir: {path_text_raw}')\n","\n","# Create a List (preprocessed_ls) of all preprocessed text files\n","try:\n","  # texts_raw_ls = glob.glob(f'{SUBDIR_TEXT_RAW}*.txt')\n","  texts_raw_root_ls = glob.glob(f'{path_text_raw}/*.txt')\n","  texts_raw_root_ls = [x.split('/')[-1] for x in texts_raw_root_ls]\n","  texts_raw_root_ls = [x.split('.')[0] for x in texts_raw_root_ls]\n","except IndexError:\n","  raise RuntimeError('No *.txt files found')\n","\n","print(f'\\ntexts_raw_root_ls:\\n  {texts_raw_root_ls}\\n')\n","\n","text_ct = 0\n","for afile_root in texts_raw_root_ls:\n","  # file_root = file_fullpath.split('/')[-1].split('.')[0]\n","  text_ct += 1\n","  print(f'{afile_root}: ') # {corpus_titles_dt[afile_root]}')\n","\n","print(f'\\nThere are {text_ct} Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]')\n","\n","\"\"\";"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvqS1TQthfxF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040275470,"user_tz":240,"elapsed":1455,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"036cdf77-a3dd-4e7b-aabc-352709e343d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./text_raw/text_raw_finance_reference/bankofengland_speeches_2007-2009.txt',\n"," './text_raw/text_raw_finance_reference/nyfederalreserve_speeches_2007-2009.txt']"]},"metadata":{},"execution_count":35}],"source":["glob.glob(f'{path_text_raw}/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"KkXBipRrGoCQ"},"source":["## Read and Segment into Sentences"]},{"cell_type":"code","source":["corpus_texts_ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbU4nPXWxLez","executionInfo":{"status":"ok","timestamp":1650040275574,"user_tz":240,"elapsed":106,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"81dbf471-15e4-4a30-c110-1a912f0adebc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['nyfederalreserve_speeches_2007-2009', 'bankofengland_speeches_2007-2009']"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq50LyAMHKYX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040308520,"user_tz":240,"elapsed":32950,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"80ae3884-d060-4223-8ee3-034f164beafd"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 33.3 s, sys: 151 ms, total: 33.4 s\n","Wall time: 34.1 s\n"]}],"source":["%%time\n","%%capture\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE:   3m30s Entire Corpus of 25 \n","#         7m30s Ref Corpus 32 Novels\n","#         7m24s Ref Corpus 32 Novels\n","#         1m00s New Corpus 2 Novels\n","\n","#        13m55s 17:00 @20220405 Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","#        16m59s 19:49 @20220405 Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","\n","\n","#           23s 22:21 @20220405 New Corpus1 2 Novels\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","\n","for i, file_root in enumerate(corpus_texts_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  global_vars.corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","\n","# global_vars.corpus_texts_dt[corpus_titles_ls[0]].head()\n","text_no = 0\n","print(f'Verify sample segmented Text: \\n    {corpus_texts_ls[text_no]}\\n')\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].head()\n"]},{"cell_type":"markdown","metadata":{"id":"tw-Ll-fdI_yb"},"source":["## Clean Sentences"]},{"cell_type":"code","source":["global_vars.corpus_texts_dt.keys()"],"metadata":{"id":"AWYdztOuwygY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040308521,"user_tz":240,"elapsed":7,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"40a14c46-c3f6-4731-ed22-60a5c223c1ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['nyfederalreserve_speeches_2007-2009', 'bankofengland_speeches_2007-2009'])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPrpL1wyNPva","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040318970,"user_tz":240,"elapsed":10454,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"ce3e5fe5-c218-4bdb-a3a7-72a4a5f92196"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Novel #0: nyfederalreserve_speeches_2007-2009...\n","  shape: (3397, 2)\n","Processing Novel #1: bankofengland_speeches_2007-2009...\n","  shape: (15054, 2)\n","CPU times: user 9.3 s, sys: 181 ms, total: 9.48 s\n","Wall time: 10.3 s\n"]}],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s (24 Novels)\n","#       (w/ stem) 4m24s (24 Novels)\n","\n","\n","#         4m10s 17:00 @20220405 Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","#         4m56s 19:49 @20220405 Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","\n","#           23s 22:21 @20220405 New Corpus1 2 Novels\n","\n","\n","i = 0\n","\n","for key_novel, atext_df in global_vars.corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('empty_placeholder')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXuwwg2_XgZu","colab":{"base_uri":"https://localhost:8080/","height":871},"executionInfo":{"status":"ok","timestamp":1650040318970,"user_tz":240,"elapsed":9,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"9efcb942-33b5-4ab4-dda9-1a7c71bf9148"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            text_raw  \\\n","0  Paul Tucker: Macro, asset price, and financial...   \n","1  It is a great privilege to give this lecture, ...   \n","2  The world in which Bridge worked was so very d...   \n","3  First, while monetary authorities are commonly...   \n","4  Second, while some distinguished commentators ...   \n","5  And third, while central bankers and others in...   \n","6  These three arenas of uncertainty  macroeconom...   \n","7  Macroeconomic and monetary policy uncertaintie...   \n","8  Essentially, low inflation on average; much le...   \n","9  Some of the credit is typically given to bette...   \n","\n","                                          text_clean  \n","0  paul tucker macro asset price and financial sy...  \n","1  it be a great privilege to give this lecture n...  \n","2  the world in which bridge work be so very diff...  \n","3  ﻿1 while monetary authority be commonly give s...  \n","4  2 while some distinguish commentator see a puz...  \n","5  and 3 while central banker and other in the of...  \n","6  this three arena of uncertainty macroeconomic ...  \n","7  macroeconomic and monetary policy uncertainty ...  \n","8  essentially low inflation on average much litt...  \n","9  some of the credit be typically give to well m...  "],"text/html":["\n","  <div id=\"df-d7e29ad5-e8c1-4dd8-bb33-9921340ecf26\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_raw</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Paul Tucker: Macro, asset price, and financial...</td>\n","      <td>paul tucker macro asset price and financial sy...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>It is a great privilege to give this lecture, ...</td>\n","      <td>it be a great privilege to give this lecture n...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The world in which Bridge worked was so very d...</td>\n","      <td>the world in which bridge work be so very diff...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>First, while monetary authorities are commonly...</td>\n","      <td>﻿1 while monetary authority be commonly give s...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Second, while some distinguished commentators ...</td>\n","      <td>2 while some distinguish commentator see a puz...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>And third, while central bankers and others in...</td>\n","      <td>and 3 while central banker and other in the of...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>These three arenas of uncertainty  macroeconom...</td>\n","      <td>this three arena of uncertainty macroeconomic ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Macroeconomic and monetary policy uncertaintie...</td>\n","      <td>macroeconomic and monetary policy uncertainty ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Essentially, low inflation on average; much le...</td>\n","      <td>essentially low inflation on average much litt...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Some of the credit is typically given to bette...</td>\n","      <td>some of the credit be typically give to well m...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7e29ad5-e8c1-4dd8-bb33-9921340ecf26')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d7e29ad5-e8c1-4dd8-bb33-9921340ecf26 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d7e29ad5-e8c1-4dd8-bb33-9921340ecf26');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":40},{"output_type":"execute_result","data":{"text/plain":["                                                text_raw  \\\n","15044    They are high profile in the eye of the public.   \n","15045  There is an apparent paradox in that the value...   \n","15046  I have offered some explanations for this para...   \n","15047  But at the same time the objectives of our cen...   \n","15048  The last of these  denominational mix  poses t...   \n","15049  And, finally, I am sometimes asked why this ma...   \n","15050  After all, I can assure you that in the curren...   \n","15051                              The answer is simple.   \n","15052  We should not forget that our job is to ensure...   \n","15053                                BIS Review 162/2009   \n","\n","                                              text_clean  \n","15044      they be high profile in the eye of the public  \n","15045  there be a apparent paradox in that the value ...  \n","15046  i have offer some explanation for this paradox...  \n","15047  but at the same time the objective of our cent...  \n","15048  the last of this denominational mix pose the b...  \n","15049  and finally i be sometimes ask why this matter...  \n","15050  after all i can assure you that in the current...  \n","15051                               the answer be simple  \n","15052  we should not forget that our job be to ensure...  \n","15053                                         bis review  "],"text/html":["\n","  <div id=\"df-a211a65b-5729-44fa-b2b5-a08aeb82c224\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_raw</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>15044</th>\n","      <td>They are high profile in the eye of the public.</td>\n","      <td>they be high profile in the eye of the public</td>\n","    </tr>\n","    <tr>\n","      <th>15045</th>\n","      <td>There is an apparent paradox in that the value...</td>\n","      <td>there be a apparent paradox in that the value ...</td>\n","    </tr>\n","    <tr>\n","      <th>15046</th>\n","      <td>I have offered some explanations for this para...</td>\n","      <td>i have offer some explanation for this paradox...</td>\n","    </tr>\n","    <tr>\n","      <th>15047</th>\n","      <td>But at the same time the objectives of our cen...</td>\n","      <td>but at the same time the objective of our cent...</td>\n","    </tr>\n","    <tr>\n","      <th>15048</th>\n","      <td>The last of these  denominational mix  poses t...</td>\n","      <td>the last of this denominational mix pose the b...</td>\n","    </tr>\n","    <tr>\n","      <th>15049</th>\n","      <td>And, finally, I am sometimes asked why this ma...</td>\n","      <td>and finally i be sometimes ask why this matter...</td>\n","    </tr>\n","    <tr>\n","      <th>15050</th>\n","      <td>After all, I can assure you that in the curren...</td>\n","      <td>after all i can assure you that in the current...</td>\n","    </tr>\n","    <tr>\n","      <th>15051</th>\n","      <td>The answer is simple.</td>\n","      <td>the answer be simple</td>\n","    </tr>\n","    <tr>\n","      <th>15052</th>\n","      <td>We should not forget that our job is to ensure...</td>\n","      <td>we should not forget that our job be to ensure...</td>\n","    </tr>\n","    <tr>\n","      <th>15053</th>\n","      <td>BIS Review 162/2009</td>\n","      <td>bis review</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a211a65b-5729-44fa-b2b5-a08aeb82c224')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a211a65b-5729-44fa-b2b5-a08aeb82c224 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a211a65b-5729-44fa-b2b5-a08aeb82c224');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":40},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 15054 entries, 0 to 15053\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   text_raw    15054 non-null  object\n"," 1   text_clean  15054 non-null  string\n","dtypes: object(1), string(1)\n","memory usage: 235.3+ KB\n"]}],"source":["# Verify the first Text in Corpus is cleaned\n","\n","text_no = 1\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].head(10)\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].tail(10)\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].info()"]},{"cell_type":"markdown","metadata":{"id":"WAjjOEFx7F5J"},"source":["## Save Cleaned Corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CrH24Dv7YwK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040319326,"user_tz":240,"elapsed":361,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"5f4b3ef2-94b6-4062-b40d-96474a8f276a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Currently in SentimentArcs root directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","Saving Clean Texts to Subdir: ./text_clean/text_clean_finance_reference\n","\n","Saving these Texts:\n","  dict_keys(['nyfederalreserve_speeches_2007-2009', 'bankofengland_speeches_2007-2009'])\n"]}],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nSaving Clean Texts to Subdir: {SUBDIR_TEXT_CLEAN}')\n","print(f'\\nSaving these Texts:\\n  {global_vars.corpus_texts_dt.keys()}')"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7lo3Lops1Zs","executionInfo":{"status":"ok","timestamp":1650040319326,"user_tz":240,"elapsed":6,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"8fdc2e4f-8828-426e-e28c-047565e326df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n"]}]},{"cell_type":"code","source":["!ls ./text_clean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eD4ucXFgsI4e","executionInfo":{"status":"ok","timestamp":1650040319526,"user_tz":240,"elapsed":203,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"7f493da1-0668-49ea-f4c5-675ff6890b4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["text_clean_finance_new_corpus1\ttext_clean_novels_reference\n","text_clean_finance_reference\ttext_clean_social_new_corpus1\n","text_clean_novels_new_corpus1\ttext_clean_social_reference\n","text_clean_novels_new_corpus2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgcmvfbvXqfy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040395941,"user_tz":240,"elapsed":317,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"fabc4c68-043f-4a83-a2b6-0ac4e992e66f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving Novel #0 to ./text_clean/text_clean_finance_reference/nyfederalreserve_speeches_2007-2009.csv\n","Saving Novel #1 to ./text_clean/text_clean_finance_reference/bankofengland_speeches_2007-2009.csv\n"]}],"source":["# Save the cleaned Textfiles\n","\n","i = 0\n","for key_novel, anovel_df in global_vars.corpus_texts_dt.items():\n","  anovel_fname = f'{key_novel}.csv'\n","\n","  anovel_fullpath = f'{SUBDIR_TEXT_CLEAN}/{anovel_fname}'\n","  print(f'Saving Novel #{i} to {anovel_fullpath}')\n","  global_vars.corpus_texts_dt[key_novel].to_csv(anovel_fullpath)\n","  i += 1"]},{"cell_type":"code","source":["# Verify files were written\n","\n","!ls -altr $PATH_TEXT_CLEAN"],"metadata":{"id":"bWqfKN2uhFkD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650040320011,"user_tz":240,"elapsed":264,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"ef8d3a31-a15b-4caa-8354-047b84ff93ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 139346\n","-rw------- 1 root root        0 Apr 10 17:52 test.txt\n","-rw------- 1 root root 77124518 Apr 10 17:52 eucentralbank_speeches_1998-2022.csv\n","-rw------- 1 root root 65565477 Apr 10 17:52 bogfederalreserve_speech_1997-2022.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"_348z09gQKe3"},"source":["# **[END OF NOTEBOOK]**"]}],"metadata":{"colab":{"collapsed_sections":["CBoEHX9Z9imD","mGoFJmeFkTxk","Ns5NwArZmush","o5GqEXyRkPjj","umZfB0YCqajW","JXG_G6um4ijG","pjQBAoLjOzDO","WlfujTpEKhCp"],"name":"sentiment_arcs_part1_preprocessing.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}