{"cells":[{"cell_type":"markdown","metadata":{"id":"3i0Fg4SYB7g0"},"source":["# **SentimentArcs (Part 1): Text Preprocessing**\n","\n","```\n","Jon Chun\n","12 Jun 2021: Started\n","04 Mar 2022: Last Update\n","```\n","\n","Welcome! \n","\n","SentimentArcs is a methodlogy and software framework for analyzing narrative in text. Virtually all long text contains narrative elements...(TODO: Insert excerpts from Paper Abstract/Intro Sections here)\n","\n","***\n","\n","* **SentimentArcs: Cloning the Github repository to your gDrive**\n","\n","If this is the first time using SentimentArcs, you will need to copy the software from our Github.com repository (github repo). The default recommended gDrive path is ./gdrive/MyDrive/research/sentiment_arcs/'. \n","\n","The first time you run this notebook and connect your Google gDrive, it will allow to to specify the path to your SentimentArcs subdirectory. If it does not exists, this notebook will copy/clone the SentimentArcs github repository code to your gDrive at the path you specify.\n","\n","\n","***\n","\n","* **NovelText: A Reference Corpus of 24 Diverse Novel**\n","\n","Sentiment Arcs comes with a carefully curated reference corpus of Novels to illustrate the unique diachronic sentiment analysis characteristic of long form fictional narrativeas. This corpus of 24 diverse novels also provides a baseline for exploring and comparing new novels with sentiment analysis using SentimentArcs.\n","\n","***\n","\n","* **Preparing New Novels: Formatting and adding to subdirectory**\n","\n","To analyze new novels with SentimentArcs, the body of the text should consist of plain text organized in to blocks separated by two newlines which visually look like a single blank line between blocks. These blocks are usually paragraphs but can also include title headers, separate lines of dialog or quotes. Please reference any of the 24 novels in the NovelText corpus for examples of this expected format.\n","\n","Once the new novel is correctly formatted as a plain text file, it should follow this standard file naming convention:\n","\n","[first letter of first name]+[full lastname]_[abbreviated book title].txt\n","\n","Examples:\n","\n","* fdouglass_narrativelifeofaslave\n","* fscottfitzgerald_thegreatgatsby.txt\n","* vwoolf_mrsdalloway.txt\n","* homer-ewilson_odyssey.txt (trans. E.Wilson)\n","* mproust-mtreharne_3guermantesway.txt (Book 3, trans. M.Treharne)\n","* staugustine_confessions9end.txt (Upto and incl Book 9)\n","\n","Note the optional author suffix (-translator) and optional title suffix (-selected chapters/books)\n","\n","***\n","\n","* **Adding New Novels: Add file to subdirectory and Update this Notebook**\n","\n","Once you have a cleaned and text file named according the standard rule above, you must move that file to the subdirectory of all input novels and update the global variable in this notebook that defines which novels to analyze.\n","\n","First, copy your cleaned text file to the subdirectory containing all novels read by this notebook. This subdir is defined by the program variable 'subdir_novels' with the default value './in1_novels/'\n","\n","Second, update the program variable 'novels_dt'. This is a Dictionary data structure that following the pattern below:\n","```\n","novels_dt = {\n","  'cdickens_achristmascarol':['A Christmas Carol by Charles Dickens ',1843,1399],\n","```\n","Where the first string (the dictionary key) must match the filename root without the '.txt' suffix (e.g. cdickens_achristmascarol). The Dictionary value after the ':' is a list of three elements:\n","\n","* A nicely formatted string of the form '(title) by (full first and last name of author)' that should be a human friendly string used to label plots and saved files.\n","\n","* The (publication year) and the (sentence count). Both are optional, but should have placeholder string '0' if unknown. These are intended for future reference and analytics.\n","\n","* Your future self will thank you if you insert new novels into the 'novels_dt' in alphabetic order for faster and more accurate reference.\n","\n","***\n","\n","* **How to Execute SentimentArcs Notebooks:**\n","\n","This is a Jupyter Notebook created to run on Google's free Colab service using only a browers and your exiting Google email account. We chose Google Colab because it is relatively, fast, free, easy to use and makes collaboration as simple as web browsing.\n","\n","A few reminders about using Jupyter Notebooks general and SentimentArcs in particular:\n","\n","* All cells must be run ***in order*** as later code cells often depend upon the output of earlier code cells\n","\n","* ***Cells that take more time to execute*** (> 1 min) usually begin with *%%time* which outputs the *total execution time* of the last run.  This timing output is deleted and recalculated each time the code cell is executed.\n","\n","* **[OPTIONAL]** at the top of a cell indicates you *may* change a setting in that cell to customize behavior.\n","\n","* **[CUSTOMIZE]** at the top of a cell indicates you *must* change a setting in that cell.\n","\n","* **[RESTART REQUIRED]** at the top of a cell indicates you *may* see a *[RESTART REQUIRED] button* at the end of the output. *If you see this button, you must select [Runtime]->[Restart Runtime] from the top menubar.\n","\n","* **[INPUT REQUIRED]** at the top of a cell indicates you will be required to take some action for execution to proceed, usually by clicking a button or entering the response to a prompt.\n","\n","All cells with a top comment prefixed with # [OPTIONAL]: indicates that you can change a setting to customize behavior, the prefix [CUSTOMIZE] indicates you MUST set/change a setting\n","\n","* SentimentArcs divides workflow into a series of chronological Jupyter Notebooks that must be run in order. Here is an overview of the workflow:\n","\n","***\n","\n","**SentimentArcs Notebooks Workflow**\n","1. Notebook #1: Preprocess Text\n","2. Notebook #2: Compute Sentiment Values (Simple Models/CPUs)\n","3. Notebook #3: Compute Sentiment Values (Complex Models/GPUs)\n","4. Notebook #4: Combine all Sentiment Values, perform Time Series analysis, and extract Crux points and surrounding text\n","\n","If you are unfamilar with setting up and using Google Colab or Jupyter Notebooks, here are a series of resources to quickly bring you up to speed. If you are using SentimentArcs with the Cambridge University Press Elements textbook, there are also a series of videos by Prof Elkins and Chun stepping you through these notebooks.\n","\n","***\n","\n","**Additional Resources and Tutorials**\n","\n","\n","**Google Colab and Jupyter Resources:**\n","\n","* Coming...\n","* [IPython, Python Data Science Handbook by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/01.00-ipython-beyond-normal-python.html) \n","\n","**Cambridge University Press Videos:**\n","\n","* Coming...\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vgvTrI7bevn2"},"source":["# **[STEP 1] Manual Configuration/Setup**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qkcsI681TaDM"},"source":["## (Popups) Connect Google gDrive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bfkqjgMiw7T","executionInfo":{"status":"ok","timestamp":1649188487886,"user_tz":240,"elapsed":20687,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"1e7b5b38-68d0-46f8-8e99-f06835178fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to attach your Google gDrive to this Colab Jupyter Notebook\n","Mounted at /gdrive\n"]}],"source":["# [INPUT REQUIRED]: Authorize access to Google gDrive\n","\n","# Connect this Notebook to your permanent Google Drive\n","#   so all generated output is saved to permanent storage there\n","\n","try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"Attempting to attach your Google gDrive to this Colab Jupyter Notebook\")\n","  drive.mount('/gdrive', force_remount=True)\n","else:\n","  print(\"Your Google gDrive is attached to this Colab Jupyter Notebook\")"]},{"cell_type":"markdown","metadata":{"id":"XVWagkv16GKQ"},"source":["## (3 Inputs) Define Directory Tree"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1649192260488,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"kskWCX1KyrV_","outputId":"e22146f7-836c-4d40-888b-547e41c6967c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current Working Directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","SUBDIR_TEXT_RAW:\n","  [text_raw_finance_reference]\n","PATH_TEXT_RAW:\n","  [./text_raw/text_raw_finance_reference]\n","SUBDIR_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_reference]\n","PATH_TEXT_CLEAN:\n","  [./text_clean/text_clean_finance_reference]\n"]}],"source":["# [CUSTOMIZE]: Change the text after the Unix '%cd ' command below (change directory)\n","#              to math the full path to your gDrive subdirectory which should be the \n","#              root directory cloned from the SentimentArcs github repo.\n","\n","# NOTE: Make sure this subdirectory already exists and there are \n","#       no typos, spaces or illegals characters (e.g. periods) in the full path after %cd\n","\n","# NOTE: In Python all strings must begin with an upper or lowercase letter, and only\n","#         letter, number and underscores ('_') characters should appear afterwards.\n","#         Make sure your full path after %cd obeys this constraint or errors may appear.\n","\n","# #@markdown **Instructions**\n","\n","# #@markdown Set Directory and Corpus names:\n","# #@markdown <li> Set <b>Path_to_SentimentArcs</b> to the project root in your **GDrive folder**\n","# #@markdown <li> Set <b>Corpus_Genre</b> = [novels, finance, social_media]\n","# #@markdown <li> <b>Corpus_Type</b> = [reference_corpus, new_corpus]\n","# #@markdown <li> <b>Corpus_Number</b> = [1-20] (id nunmber if a new_corpus)\n","\n","#@markdown <hr>\n","\n","# Step #1: Get full path to SentimentArcs subdir on gDrive\n","# =======\n","#@markdown **Accept default path on gDrive or Enter new one:**\n","\n","Path_to_SentimentArcs = \"/gdrive/MyDrive/sentimentarcs_notebooks/\" #@param [\"/gdrive/MyDrive/sentiment_arcs/\"] {allow-input: true}\n","\n","\n","#@markdown Set this to the project root in your <b>GDrive folder</b>\n","#@markdown <br> (e.g. /<wbr><b>gdrive/MyDrive/research/sentiment_arcs/</b>)\n","\n","#@markdown <hr>\n","\n","#@markdown **Which type of texts are you cleaning?** \\\n","\n","Corpus_Genre = \"finance\" #@param [\"novels\", \"social_media\", \"finance\"]\n","\n","# Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","Corpus_Type = \"reference\" #@param [\"new\", \"reference\"]\n","\n","\n","Corpus_Number = 2 #@param {type:\"slider\", min:0, max:10, step:1}\n","\n","\n","#@markdown Put in the corresponding Subdirectory under **./text_raw**:\n","#@markdown <li> All Texts as clean <b>plaintext *.txt</b> files \n","#@markdown <li> A <b>YAML Configuration File</b> describing each Texts\n","\n","#@markdown Please verify the required textfiles and YAML file exist in the correct subdirectories before continuing.\n","\n","print('Current Working Directory:')\n","%cd $Path_to_SentimentArcs\n","\n","print('\\n')\n","\n","if Corpus_Type == 'reference':\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_reference'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_reference'\n","else:\n","  SUBDIR_TEXT_RAW = f'text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","  SUBDIR_TEXT_CLEAN = f'text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/'\n","\n","PATH_TEXT_RAW = f'./text_raw/{SUBDIR_TEXT_RAW}'\n","PATH_TEXT_CLEAN = f'./text_clean/{SUBDIR_TEXT_CLEAN}'\n","\n","# TODO: Clean up\n","SUBDIR_TEXT_CLEAN = PATH_TEXT_CLEAN\n","\n","print(f'SUBDIR_TEXT_RAW:\\n  [{SUBDIR_TEXT_RAW}]')\n","print(f'PATH_TEXT_RAW:\\n  [{PATH_TEXT_RAW}]')\n","\n","print(f'SUBDIR_TEXT_CLEAN:\\n  [{SUBDIR_TEXT_CLEAN}]')\n","print(f'PATH_TEXT_CLEAN:\\n  [{PATH_TEXT_CLEAN}]')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1008,"status":"ok","timestamp":1649188494420,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"uRx8mIVxUyXM","outputId":"76ce6393-6ec2-4a0c-e53a-5be62502b898"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.13\n","\n","\n","Contents of Subdirectory [./sentiment_arcs/utils/]\n","\n","config_matplotlib.py   global_constants.py    sentiment_arcs_config.py\n","config_seaborn.py      global_vars.py\t      set_globals.py\n","file_utils.py\t       __init__.py\t      subdir_constants.py\n","get_fullpath.py        __pycache__\t      test.py\n","get_model_families.py  read_yaml.py\t      text_cleaners_new.py\n","get_sentimentr.R       sa_config_20220404.py  text_cleaners.py\n","get_sentiments.py      sa_config.py\n","get_subdirs.py\t       sentiment_analysis.py\n"]}],"source":["# Add PATH for ./utils subdirectory\n","\n","import sys\n","import os\n","\n","!python --version\n","\n","print('\\n')\n","\n","PATH_UTILS = f'{Path_to_SentimentArcs}utils'\n","PATH_UTILS\n","\n","sys.path.append(PATH_UTILS)\n","\n","print('Contents of Subdirectory [./sentiment_arcs/utils/]\\n')\n","!ls $PATH_UTILS\n","\n","# More Specific than PATH for searching libraries\n","# !echo $PYTHONPATH"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1649188495997,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"RBtWnOBxiw8H","outputId":"480359b0-12f5-44d2-edc1-a82aa9357436"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt']"]},"metadata":{},"execution_count":4}],"source":["# Review Global Variables and set the first few\n","\n","import global_vars as global_vars\n","\n","global_vars.SUBDIR_SENTIMENTARCS = Path_to_SentimentArcs\n","global_vars.Corpus_Genre = Corpus_Genre\n","global_vars.Corpus_Type = Corpus_Type\n","global_vars.Corpus_Number = Corpus_Number\n","\n","global_vars.SUBDIR_TEXT_RAW = SUBDIR_TEXT_RAW\n","global_vars.PATH_TEXT_RAW = PATH_TEXT_RAW\n","\n","dir(global_vars)"]},{"cell_type":"markdown","metadata":{"id":"P00BhwLVyL8X"},"source":["# **[STEP 2] Automatic Configuration/Setup**"]},{"cell_type":"markdown","metadata":{"id":"CBoEHX9Z9imD"},"source":["## Custom Libraries & Define Globals"]},{"cell_type":"code","source":["dir(global_vars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mfm1mYioS4jT","executionInfo":{"status":"ok","timestamp":1649188561530,"user_tz":240,"elapsed":175,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"e786c4b6-2f44-400d-8ce3-5bb5fe502b43"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Define Global Dict to hold cleaned Texts\n","\n","global_vars.corpus_texts_dt = {}"],"metadata":{"id":"qzJSUvadztx9","executionInfo":{"status":"ok","timestamp":1649187533815,"user_tz":240,"elapsed":201,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["!ls utils/*.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUzefhTQSHFs","executionInfo":{"status":"ok","timestamp":1649188600114,"user_tz":240,"elapsed":225,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"d897a4e5-6faa-4590-ca2b-f23f966854af"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["utils/config_matplotlib.py   utils/read_yaml.py\n","utils/config_seaborn.py      utils/sa_config_20220404.py\n","utils/file_utils.py\t     utils/sa_config.py\n","utils/get_fullpath.py\t     utils/sentiment_analysis.py\n","utils/get_model_families.py  utils/sentiment_arcs_config.py\n","utils/get_sentiments.py      utils/set_globals.py\n","utils/get_subdirs.py\t     utils/subdir_constants.py\n","utils/global_constants.py    utils/test.py\n","utils/global_vars.py\t     utils/text_cleaners_new.py\n","utils/__init__.py\t     utils/text_cleaners.py\n"]}]},{"cell_type":"code","source":["!head -n 40 ./utils/sa_config.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWRM6xbWR9Jx","executionInfo":{"status":"ok","timestamp":1649188601523,"user_tz":240,"elapsed":383,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"b3deca99-5798-40b3-9bf5-1ee225460d64"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","import global_vars\n","\n","def get_subdirs(SA_root,Corpus_Genre, Corpus_Type, Corpus_Number, NotebookModels):\n","    '''\n","    Given a two strings: Corpus, Text_type\n","    Set all global SUB/DIR constants\n","    '''\n","\n","    # NotebookModels indicates which notebook is currently running that imported this get_subdirs() function\n","    if global_vars.NotebookModels == 'syuzhetr2sentimentr':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_syuzhetr2sentimentr.json'\n","    elif NotebookModels == 'lex2ml':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_lex2ml.json'\n","    elif NotebookModels == 'dnn2transformers':\n","        global_vars.FNAME_SENTIMENT_RAW = f'sentiment_raw_{Corpus_Genre}_{Corpus_Type}_dnn2transformers.json'\n","    elif NotebookModels == 'none':\n","        global_vars.FNAME_SENTIMENT_RAW = f'[NONE]'\n","    else:\n","        print(f'ERROR: Illegal value for NotebookModels: {global_vars.NotebookModels}')\n","        return\n","\n","    # Define a universal syntax for a common directory structure across all notebooks\n","    # global_vars.SUBDIR_SENTIMENTARCS = '/gdrive/MyDrive/cdh/sentiment_arcs'\n","    global_vars.SUBDIR_SENTIMENTARCS = SA_root\n","    if Corpus_Type == 'new':\n","        global_vars.SUBDIR_TEXT_RAW = f\"./text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TEXT_CLEAN = f\"./text_clean/text_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/sentiment_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/sentiemnt_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TIMESERIES_RAW = f\"./timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","        global_vars.SUBDIR_TIMESERIES_CLEAN = f\"./timeseries_clean/timeseries_clean_{Corpus_Genre}_{Corpus_Type}_corpus{Corpus_Number}/\"\n","    elif Corpus_Type == 'reference':\n","        global_vars.SUBDIR_TEXT_RAW = f\"./text_raw/text_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TEXT_CLEAN = f\"./text_clean/text_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_SENTIMENT_RAW = f\"./sentiment_raw/sentiment_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_SENTIMENT_CLEAN = f\"./sentiment_clean/sentiemnt_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TIMESERIES_RAW = f\"./timeseries_raw/timeseries_raw_{Corpus_Genre}_{Corpus_Type}/\"\n","        global_vars.SUBDIR_TIMESERIES_CLEAN = f\"./timeseries_clean/timeseries_clean_{Corpus_Genre}_{Corpus_Type}/\"\n","    else:\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1649188610568,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"VtaPyy4VSohJ","outputId":"58c4d771-5e0b-41d8-aebf-e762712453e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","\n","Objects in sa_config()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_subdirs', 'global_vars', 'set_globals']\n","\n","\n","Verify the Directory Structure:\n","\n","-------------------------------\n","\n","           [Corpus Genre]: finance\n","\n","            [Corpus Type]: reference\n","\n","\n","    [FNAME_SENTIMENT_RAW]: [NONE]\n","\n","\n","\n","\n","INPUTS:\n","-------------------------------\n","\n","   [SUBDIR_SENTIMENTARCS]: /gdrive/MyDrive/sentimentarcs_notebooks/\n","\n","\n","STEP 1: Clean Text\n","--------------------\n","\n","        [SUBDIR_TEXT_RAW]: ./text_raw/text_raw_finance_reference/\n","\n","      [SUBDIR_TEXT_CLEAN]: ./text_clean/text_clean_finance_reference/\n","\n","\n","STEP 2: Get Sentiments\n","--------------------\n","\n","   [SUBDIR_SENTIMENT_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n"," [SUBDIR_SENTIMENT_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","STEP 3: Smooth Time Series and Get Crux Points\n","--------------------\n","\n","  [SUBDIR_TIMESERIES_RAW]: ./sentiment_raw/sentiment_raw_finance_reference/\n","\n","[SUBDIR_TIMESERIES_CLEAN]: ./sentiment_clean/sentiemnt_clean_finance_reference/\n","\n","\n","\n","OUTPUTS:\n","-------------------------------\n","\n","          [SUBDIR_GRAPHS]: ./graphs/graphs_finance/\n","\n","            [SUBDIR_DATA]: ./data/data_finance\n","\n","           [SUBDIR_UTILS]: ./utils/\n","\n"]}],"source":["# Import SentimentArcs Utilities to define Directory Structure\n","#   based the Selected Corpus Genre, Type and Number\n","\n","!pwd \n","print('\\n')\n","\n","# from utils import sa_config # .sentiment_arcs_utils\n","from utils import sa_config\n","\n","print('Objects in sa_config()')\n","print(dir(sa_config))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","sa_config.get_subdirs(Path_to_SentimentArcs, Corpus_Genre, Corpus_Type, Corpus_Number, 'none')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1649188619964,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tx8j_3Y6qQna","outputId":"97640f1c-affd-4be9-b5cc-d412622a6430"},"outputs":[{"output_type":"stream","name":"stdout","text":["MIN_PARAG_LEN: 10\n","STOPWORDS_ADD_EN: ['a', 'the', 'an']\n","TEST_WORDS_LS: ['Love', 'Hate', 'bizarre', 'strange', 'furious', 'elated', 'curious', 'beserk', 'gambaro']\n","SLANG_DT: {'$': ' dollar ', '€': ' euro ', '4ao': 'for adults only', 'a.m': 'before midday', 'a3': 'anytime anywhere anyplace', 'aamof': 'as a matter of fact', 'acct': 'account', 'adih': 'another day in hell', 'afaic': 'as far as i am concerned', 'afaict': 'as far as i can tell', 'afaik': 'as far as i know', 'afair': 'as far as i remember', 'afk': 'away from keyboard', 'app': 'application', 'approx': 'approximately', 'apps': 'applications', 'asap': 'as soon as possible', 'asl': 'age, sex, location', 'atk': 'at the keyboard', 'ave.': 'avenue', 'aymm': 'are you my mother', 'ayor': 'at your own risk', 'b&b': 'bed and breakfast', 'b+b': 'bed and breakfast', 'b.c': 'before christ', 'b2b': 'business to business', 'b2c': 'business to customer', 'b4': 'before', 'b4n': 'bye for now', 'b@u': 'back at you', 'bae': 'before anyone else', 'bak': 'back at keyboard', 'bbbg': 'bye bye be good', 'bbc': 'british broadcasting corporation', 'bbias': 'be back in a second', 'bbl': 'be back later', 'bbs': 'be back soon', 'be4': 'before', 'bfn': 'bye for now', 'blvd': 'boulevard', 'bout': 'about', 'brb': 'be right back', 'bros': 'brothers', 'brt': 'be right there', 'bsaaw': 'big smile and a wink', 'btw': 'by the way', 'bwl': 'bursting with laughter', 'c/o': 'care of', 'cet': 'central european time', 'cf': 'compare', 'cia': 'central intelligence agency', 'csl': 'can not stop laughing', 'cu': 'see you', 'cul8r': 'see you later', 'cv': 'curriculum vitae', 'cwot': 'complete waste of time', 'cya': 'see you', 'cyt': 'see you tomorrow', 'dae': 'does anyone else', 'dbmib': 'do not bother me i am busy', 'diy': 'do it yourself', 'dm': 'direct message', 'dwh': 'during work hours', 'e123': 'easy as one two three', 'eet': 'eastern european time', 'eg': 'example', 'embm': 'early morning business meeting', 'encl': 'enclosed', 'encl.': 'enclosed', 'etc': 'and so on', 'faq': 'frequently asked questions', 'fawc': 'for anyone who cares', 'fb': 'facebook', 'fc': 'fingers crossed', 'fig': 'figure', 'fimh': 'forever in my heart', 'ft.': 'feet', 'ft': 'featuring', 'ftl': 'for the loss', 'ftw': 'for the win', 'fwiw': 'for what it is worth', 'fyi': 'for your information', 'g9': 'genius', 'gahoy': 'get a hold of yourself', 'gal': 'get a life', 'gcse': 'general certificate of secondary education', 'gfn': 'gone for now', 'gg': 'good game', 'gl': 'good luck', 'glhf': 'good luck have fun', 'gmt': 'greenwich mean time', 'gmta': 'great minds think alike', 'gn': 'good night', 'g.o.a.t': 'greatest of all time', 'goat': 'greatest of all time', 'goi': 'get over it', 'gps': 'global positioning system', 'gr8': 'great', 'gratz': 'congratulations', 'gyal': 'girl', 'h&c': 'hot and cold', 'hp': 'horsepower', 'hr': 'hour', 'hrh': 'his royal highness', 'ht': 'height', 'ibrb': 'i will be right back', 'ic': 'i see', 'icq': 'i seek you', 'icymi': 'in case you missed it', 'idc': 'i do not care', 'idgadf': 'i do not give a damn fuck', 'idgaf': 'i do not give a fuck', 'idk': 'i do not know', 'ie': 'that is', 'i.e': 'that is', 'ifyp': 'i feel your pain', 'IG': 'instagram', 'iirc': 'if i remember correctly', 'ilu': 'i love you', 'ily': 'i love you', 'imho': 'in my humble opinion', 'imo': 'in my opinion', 'imu': 'i miss you', 'iow': 'in other words', 'irl': 'in real life', 'j4f': 'just for fun', 'jic': 'just in case', 'jk': 'just kidding', 'jsyk': 'just so you know', 'l8r': 'later', 'lb': 'pound', 'lbs': 'pounds', 'ldr': 'long distance relationship', 'lmao': 'laugh my ass off', 'lmfao': 'laugh my fucking ass off', 'lol': 'laughing out loud', 'ltd': 'limited', 'ltns': 'long time no see', 'm8': 'mate', 'mf': 'motherfucker', 'mfs': 'motherfuckers', 'mfw': 'my face when', 'mofo': 'motherfucker', 'mph': 'miles per hour', 'mr': 'mister', 'mrw': 'my reaction when', 'ms': 'miss', 'mte': 'my thoughts exactly', 'nagi': 'not a good idea', 'nbc': 'national broadcasting company', 'nbd': 'not big deal', 'nfs': 'not for sale', 'ngl': 'not going to lie', 'nhs': 'national health service', 'nrn': 'no reply necessary', 'nsfl': 'not safe for life', 'nsfw': 'not safe for work', 'nth': 'nice to have', 'nvr': 'never', 'nyc': 'new york city', 'oc': 'original content', 'og': 'original', 'ohp': 'overhead projector', 'oic': 'oh i see', 'omdb': 'over my dead body', 'omg': 'oh my god', 'omw': 'on my way', 'p.a': 'per annum', 'p.m': 'after midday', 'pm': 'prime minister', 'poc': 'people of color', 'pov': 'point of view', 'pp': 'pages', 'ppl': 'people', 'prw': 'parents are watching', 'ps': 'postscript', 'pt': 'point', 'ptb': 'please text back', 'pto': 'please turn over', 'qpsa': 'what happens', 'ratchet': 'rude', 'rbtl': 'read between the lines', 'rlrt': 'real life retweet', 'rofl': 'rolling on the floor laughing', 'roflol': 'rolling on the floor laughing out loud', 'rotflmao': 'rolling on the floor laughing my ass off', 'rt': 'retweet', 'ruok': 'are you ok', 'sfw': 'safe for work', 'sk8': 'skate', 'smh': 'shake my head', 'sq': 'square', 'srsly': 'seriously', 'ssdd': 'same stuff different day', 'tbh': 'to be honest', 'tbs': 'tablespooful', 'tbsp': 'tablespooful', 'tfw': 'that feeling when', 'thks': 'thank you', 'tho': 'though', 'thx': 'thank you', 'tia': 'thanks in advance', 'til': 'today i learned', 'tl;dr': 'too long i did not read', 'tldr': 'too long i did not read', 'tmb': 'tweet me back', 'tntl': 'trying not to laugh', 'ttyl': 'talk to you later', 'u': 'you', 'u2': 'you too', 'u4e': 'yours for ever', 'utc': 'coordinated universal time', 'w/': 'with', 'w/o': 'without', 'w8': 'wait', 'wassup': 'what is up', 'wb': 'welcome back', 'wtf': 'what the fuck', 'wtg': 'way to go', 'wtpa': 'where the party at', 'wuf': 'where are you from', 'wuzup': 'what is up', 'wywh': 'wish you were here', 'yd': 'yard', 'ygtr': 'you got that right', 'ynk': 'you never know', 'zzz': 'sleeping bored and tired'}\n"]}],"source":["# Call SentimentArcs Utility to define Global Variables\n","\n","sa_config.set_globals()\n","\n","# Verify sample global var set\n","print(f'MIN_PARAG_LEN: {global_vars.MIN_PARAG_LEN}')\n","print(f'STOPWORDS_ADD_EN: {global_vars.STOPWORDS_ADD_EN}')\n","print(f'TEST_WORDS_LS: {global_vars.TEST_WORDS_LS}')\n","print(f'SLANG_DT: {global_vars.SLANG_DT}')"]},{"cell_type":"markdown","metadata":{"id":"mGoFJmeFkTxk"},"source":["## Configure Jupyter Notebook"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kK8zKENjsyig","executionInfo":{"status":"ok","timestamp":1649188622230,"user_tz":240,"elapsed":190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Configure Jupyter\n","\n","# To reload modules under development\n","\n","# Option (a)\n","%load_ext autoreload\n","%autoreload 2\n","# Option (b)\n","# import importlib\n","# importlib.reload(functions.readfunctions)\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Enable multiple outputs from one code cell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from IPython.display import display\n","from IPython.display import Image\n","from ipywidgets import widgets, interactive\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"Ns5NwArZmush"},"source":["## Read YAML Configuration for Corpus and Models "]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1649191566757,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"mUveIcUOzYav","outputId":"1e64f6bc-332c-458e-f776-87fa533e7311"},"outputs":[{"output_type":"stream","name":"stdout","text":["Objects in read_yaml()\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'global_vars', 'read_corpus_yaml', 'yaml']\n","\n","\n","YAML Directory: text_raw/text_raw_finance_reference\n","YAML File: text_raw_finance_reference_info.yaml\n","SentimentArcs Model Ensemble ------------------------------\n","\n","AutoGluon_Text\n","BERT_2IMDB\n","BERT_Dual_Coding\n","BERT_Multilingual\n","BERT_Yelp\n","CNN_DNN\n","Distilled_BERT\n","FLAML_AutoML\n","Fully_Connected_Network\n","HyperOpt_CNN_Flair_AutoML\n","LSTM_DNN\n","Logistic_Regression\n","Logistic_Regression_CV\n","Multilingual_CNN_Stanza_AutoML\n","Multinomial_Naive_Bayes\n","Pattern\n","Random_Forest\n","RoBERTa_Large_15DB\n","RoBERTa_XML_8Language\n","SentimentR_JockersRinker\n","SentimentR_Jockers\n","SentimentR_Bing\n","SentimentR_NRC\n","SentimentR_SentiWord\n","SentimentR_SenticNet\n","SentimentR_LMcD\n","SentimentR_SentimentR\n","PySentimentR_JockersRinker\n","PySentimentR_Huliu\n","PySentimentR_NRC\n","PySentimentR_SentiWord\n","PySentimentR_SenticNet\n","PySentimentR_LMcD\n","SyuzhetR_AFINN\n","SyuzhetR_Bing\n","SyuzhetR_NRC\n","SyuzhetR_SyuzhetR\n","T5_IMDB\n","TextBlob\n","VADER\n","AFINN\n","XGBoost\n","\n","\n","Corpus Texts ------------------------------\n","\n","bogfederalreserve_speech_1997-2022\n","eucentralbank_speeches_1998-2022\n","\n","\n","There are 42 Models in the SentimentArcs Ensemble above.\n","\n","\n","There are 2 Texts in the Corpus above.\n","\n","\n","\n"]}],"source":["# from utils import sa_config # .sentiment_arcs_utils\n","\n","import yaml\n","\n","from utils import read_yaml\n","\n","print('Objects in read_yaml()')\n","print(dir(read_yaml))\n","print('\\n')\n","\n","# Directory Structure for the Selected Corpus Genre, Type and Number\n","read_yaml.read_corpus_yaml(Corpus_Genre, Corpus_Type, Corpus_Number)\n","\n","print('SentimentArcs Model Ensemble ------------------------------\\n')\n","model_titles_ls = global_vars.models_titles_dt.keys()\n","print('\\n'.join(model_titles_ls))\n","\n","\n","print('\\n\\nCorpus Texts ------------------------------\\n')\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","print('\\n'.join(corpus_titles_ls))\n","\n","\n","print(f'\\n\\nThere are {len(model_titles_ls)} Models in the SentimentArcs Ensemble above.\\n')\n","print(f'\\nThere are {len(corpus_titles_ls)} Texts in the Corpus above.\\n')\n","print('\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"o5GqEXyRkPjj"},"source":["## Install Libraries"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3626,"status":"ok","timestamp":1649189127200,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tz5jGrDYi9Qe","outputId":"7ea037fe-1207-4fe3-da62-6daddc1c755a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyreadr\n","  Downloading pyreadr-0.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 40 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 92 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 215 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 225 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 235 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 245 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 256 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 266 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 276 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 286 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 296 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 307 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 317 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 327 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 337 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 348 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 358 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 361 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyreadr) (1.3.5)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (1.21.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.15.0)\n","Installing collected packages: pyreadr\n","Successfully installed pyreadr-0.4.4\n"]}],"source":["# Library to Read R datafiles from within Python programs\n","\n","!pip install pyreadr"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19192,"status":"ok","timestamp":1649189146388,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"WFXzmfPQouNR","outputId":"a8432c83-becc-4ee7-bc3f-e0aa0e3d1df1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n","Collecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 51.0 MB/s \n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 50.4 MB/s \n","\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 59.1 MB/s \n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Collecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[K     |████████████████████████████████| 653 kB 68.8 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.2 thinc-8.0.15 typer-0.4.1\n"]}],"source":["# Powerful Industry-Grade NLP Library\n","\n","!pip install -U spacy"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15498,"status":"ok","timestamp":1649189161882,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"sD_ZVbywJ4_e","outputId":"2d9c6896-b355-436c-f6b4-478fb9021391"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting texthero\n","  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n","Collecting spacy<3.0.0\n","  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 10.4 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.0.2)\n","Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (5.5.0)\n","Requirement already satisfied: gensim<4.0,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.6.0)\n","Collecting nltk>=3.3\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 54.7 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.3.5)\n","Collecting unidecode>=1.1.1\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from texthero) (1.21.5)\n","Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.7/dist-packages (from texthero) (4.63.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.6.0->texthero) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (1.4.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.0->texthero) (3.10.0.2)\n","Collecting regex>=2021.8.3\n","  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 35.1 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.2.0->texthero) (8.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->texthero) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.9.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (3.0.6)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 53.8 MB/s \n","\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0->texthero) (2.23.0)\n","Collecting thinc<7.5.0,>=7.4.1\n","  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[K     |████████████████████████████████| 1.0 MB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud>=1.5.0->texthero) (7.1.2)\n","Installing collected packages: srsly, catalogue, thinc, regex, unidecode, spacy, nltk, texthero\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.2\n","    Uninstalling srsly-2.4.2:\n","      Successfully uninstalled srsly-2.4.2\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.7\n","    Uninstalling catalogue-2.0.7:\n","      Successfully uninstalled catalogue-2.0.7\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.0.15\n","    Uninstalling thinc-8.0.15:\n","      Successfully uninstalled thinc-8.0.15\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.2.4\n","    Uninstalling spacy-3.2.4:\n","      Successfully uninstalled spacy-3.2.4\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed catalogue-1.0.0 nltk-3.7 regex-2022.3.15 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.4\n"]}],"source":["# NLP Library to Simply Cleaning Text\n","\n","!pip install texthero"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3184,"status":"ok","timestamp":1649189165062,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"T94fr2ymLFgV","outputId":"0cb0d814-c727-462b-bcf9-e3cea8d20b1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysbd\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: pysbd\n","Successfully installed pysbd-0.3.4\n"]}],"source":["# Advanced Sentence Boundry Detection Pythn Library\n","#   for splitting raw text into grammatical sentences\n","#   (can be difficult due to common motifs like Mr., ..., ?!?, etc)\n","\n","!pip install pysbd"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4461,"status":"ok","timestamp":1649189169515,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"E3ev2lK-MU9E","outputId":"6471a492-bf0b-479e-ac9f-e3f1986225c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 8.4 MB/s \n","\u001b[?25hCollecting anyascii\n","  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n","\u001b[K     |████████████████████████████████| 284 kB 57.4 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.0 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n"]}],"source":["# Python Library to expand contractions to aid in Sentiment Analysis\n","#   (e.g. aren't -> are not, can't -> can not)\n","\n","!pip install contractions"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4355,"status":"ok","timestamp":1649189173865,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"kScSfHO1Q8Y-","outputId":"c565ee2e-6180-4766-97e9-851a11321270"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 21 kB/s \n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n"]}],"source":["# Library for dealing with Emoticons (punctuation) and Emojis (icons)\n","\n","!pip install emot"]},{"cell_type":"markdown","metadata":{"id":"ajD8hCbzkStO"},"source":["## Load Libraries"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"oCRgJK2ri9Nx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649189174784,"user_tz":240,"elapsed":923,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a0477706-6da8-4944-d797-ca7c41089f57"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-05 20:06:10,057 : INFO : NumExpr defaulting to 2 threads.\n"]}],"source":["# Core Python Libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline\n","\n","import re\n","import string\n","from datetime import datetime\n","import os\n","import sys\n","import glob\n","import json\n","from pathlib import Path\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"pify1umf6A8K","executionInfo":{"status":"ok","timestamp":1649189174784,"user_tz":240,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# More advanced Sentence Tokenizier Object from PySBD\n","from pysbd.utils import PySBDFactory"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1649189176073,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"EEPQ67KrCO6f","outputId":"90663b74-c184-4005-c2c8-d096415ea6c8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":29}],"source":["# Simplier Sentence Tokenizer Object from NLTK\n","import nltk \n","from nltk.tokenize import sent_tokenize\n","\n","# Download required NLTK tokenizer data\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5184,"status":"ok","timestamp":1649189181253,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"NRYua8r7MP07","outputId":"55c3f6bf-c3b9-4faf-ccc2-43c1e1a8644a"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-04-05 20:06:12,472 : INFO : 'pattern' package not found; tag filters are not available for English\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Instantiate and Import Text Cleaning Ojects into Global Variable space\n","import texthero as hero\n","from texthero import preprocessing"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1626,"status":"ok","timestamp":1649189182873,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"7PBsG4WRMvrN","outputId":"6de8f82a-58e1-49a4-a3be-c3013326d0c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'flag': True,\n"," 'location': [[20, 23], [24, 27], [28, 33]],\n"," 'mean': ['Happy face smiley',\n","  'Frown, sad, andry or pouting',\n","  'Very very Happy face or smiley'],\n"," 'value': [':-)', ':-(', ':-)))']}"]},"metadata":{},"execution_count":31}],"source":["# Expand contractions (e.g. can't -> can not)\n","import contractions\n","\n","# Translate emoticons :0 and emoji icons to text\n","import emot \n","emot_obj = emot.core.emot() \n","\n","from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n","\n","# Test\n","text = \"I love python ☮ 🙂 ❤ :-) :-( :-)))\" \n","emot_obj.emoticons(text)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1649189183997,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"JPFS4MEm6MyF","outputId":"906ee72c-c3a8-45e7-cb52-27732a30b32d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Token Attributes: \n"," token.text, token.pos_, token.tag_, token.dep_, token.lemma_\n","Apples                                          Apples      \n","and                                             and         \n","oranges                                         orange      \n","are                                             be          \n","similar                                         similar     \n",".                                               .           \n","Boots                                           Boots       \n","and                                             and         \n","hippos                                          hippo       \n","are         AUX         VBP                     be          \n","n't         PART        RB                      not         \n",".                                               .           \n","\n","Another Test:\n","\n","Apples      9297668116247400838           Apples      \n","and         2283656566040971221           and         \n","oranges     2208928596161743350           orange      \n","are         10382539506755952630          be          \n","similar     18166476740537071113          similar     \n",".           12646065887601541794          .           \n","Boots       18231591219755621867          Boots       \n","and         2283656566040971221           and         \n","hippos      6542994350242320795           hippo       \n","are         10382539506755952630          be          \n","n't         447765159362469301            not         \n",".           12646065887601541794          .           \n"]}],"source":["# Import spaCy, language model and setup minimal pipeline\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n","# nlp.max_length = 1027203\n","nlp.max_length = 2054406\n","nlp.add_pipe(nlp.create_pipe('sentencizer')) # https://stackoverflow.com/questions/51372724/how-to-speed-up-spacy-lemmatization\n","\n","# Test some edge cases, try to find examples that break spaCy\n","doc= nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","print('\\n')\n","print(\"Token Attributes: \\n\", \"token.text, token.pos_, token.tag_, token.dep_, token.lemma_\")\n","for token in doc:\n","    # Print the text and the predicted part-of-speech tag\n","    print(\"{:<12}{:<12}{:<12}{:<12}{:<12}\".format(token.text, token.pos_, token.tag_, token.dep_, token.lemma_))\n","\n","print('\\nAnother Test:\\n')\n","doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n","\n","for token in doc:\n","    print(\"{:<12}{:<30}{:<12}\".format(token.text, token.lemma, token.lemma_))"]},{"cell_type":"markdown","metadata":{"id":"umZfB0YCqajW"},"source":["## Define/Customize Stopwords"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"e8_qLJBbtoOA","executionInfo":{"status":"ok","timestamp":1649189545671,"user_tz":240,"elapsed":159,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Define Globals\n","\"\"\"\n","# Main data structure: Dictionary (key=text_name) of DataFrames (cols: text_raw, text_clean)\n","corpus_texts_dt = {}\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir('/gdrive/MyDrive/cdh/sentiment_arcs/')\n","\n","%run -i './utils/get_globals.py'\n","\n","SLANG_DT.keys()\n","\"\"\";"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"DWFx7-P-Ai5D","outputId":"61a1196e-7d33-494d-9fbb-9d7ab20c559b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['$', '€', '4ao', 'a.m', 'a3', 'aamof', 'acct', 'adih', 'afaic', 'afaict', 'afaik', 'afair', 'afk', 'app', 'approx', 'apps', 'asap', 'asl', 'atk', 'ave.', 'aymm', 'ayor', 'b&b', 'b+b', 'b.c', 'b2b', 'b2c', 'b4', 'b4n', 'b@u', 'bae', 'bak', 'bbbg', 'bbc', 'bbias', 'bbl', 'bbs', 'be4', 'bfn', 'blvd', 'bout', 'brb', 'bros', 'brt', 'bsaaw', 'btw', 'bwl', 'c/o', 'cet', 'cf', 'cia', 'csl', 'cu', 'cul8r', 'cv', 'cwot', 'cya', 'cyt', 'dae', 'dbmib', 'diy', 'dm', 'dwh', 'e123', 'eet', 'eg', 'embm', 'encl', 'encl.', 'etc', 'faq', 'fawc', 'fb', 'fc', 'fig', 'fimh', 'ft.', 'ft', 'ftl', 'ftw', 'fwiw', 'fyi', 'g9', 'gahoy', 'gal', 'gcse', 'gfn', 'gg', 'gl', 'glhf', 'gmt', 'gmta', 'gn', 'g.o.a.t', 'goat', 'goi', 'gps', 'gr8', 'gratz', 'gyal', 'h&c', 'hp', 'hr', 'hrh', 'ht', 'ibrb', 'ic', 'icq', 'icymi', 'idc', 'idgadf', 'idgaf', 'idk', 'ie', 'i.e', 'ifyp', 'IG', 'iirc', 'ilu', 'ily', 'imho', 'imo', 'imu', 'iow', 'irl', 'j4f', 'jic', 'jk', 'jsyk', 'l8r', 'lb', 'lbs', 'ldr', 'lmao', 'lmfao', 'lol', 'ltd', 'ltns', 'm8', 'mf', 'mfs', 'mfw', 'mofo', 'mph', 'mr', 'mrw', 'ms', 'mte', 'nagi', 'nbc', 'nbd', 'nfs', 'ngl', 'nhs', 'nrn', 'nsfl', 'nsfw', 'nth', 'nvr', 'nyc', 'oc', 'og', 'ohp', 'oic', 'omdb', 'omg', 'omw', 'p.a', 'p.m', 'pm', 'poc', 'pov', 'pp', 'ppl', 'prw', 'ps', 'pt', 'ptb', 'pto', 'qpsa', 'ratchet', 'rbtl', 'rlrt', 'rofl', 'roflol', 'rotflmao', 'rt', 'ruok', 'sfw', 'sk8', 'smh', 'sq', 'srsly', 'ssdd', 'tbh', 'tbs', 'tbsp', 'tfw', 'thks', 'tho', 'thx', 'tia', 'til', 'tl;dr', 'tldr', 'tmb', 'tntl', 'ttyl', 'u', 'u2', 'u4e', 'utc', 'w/', 'w/o', 'w8', 'wassup', 'wb', 'wtf', 'wtg', 'wtpa', 'wuf', 'wuzup', 'wywh', 'yd', 'ygtr', 'ynk', 'zzz'])"]},"metadata":{},"execution_count":44}],"source":["global_vars.SLANG_DT.keys()"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"zqrk5TEuAzzq","outputId":"389b972b-12c9-41c8-dc05-a21132b34d89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Corpus_Genre',\n"," 'Corpus_Number',\n"," 'Corpus_Type',\n"," 'FNAME_SENTIMENT_RAW',\n"," 'MIN_PARAG_LEN',\n"," 'MIN_SENT_LEN',\n"," 'NotebookModels',\n"," 'PATH_TEXT_RAW',\n"," 'PATH_TEXT_RAW_CORPUS',\n"," 'SLANG_DT',\n"," 'STOPWORDS_ADD_EN',\n"," 'STOPWORDS_DEL_EN',\n"," 'SUBDIR_CRUXES',\n"," 'SUBDIR_DATA',\n"," 'SUBDIR_GRAPHS',\n"," 'SUBDIR_SENTIMENTARCS',\n"," 'SUBDIR_SENTIMENT_CLEAN',\n"," 'SUBDIR_SENTIMENT_RAW',\n"," 'SUBDIR_TEXT_CLEAN',\n"," 'SUBDIR_TEXT_RAW',\n"," 'SUBDIR_TIMESERIES_CLEAN',\n"," 'SUBDIR_TIMESERIES_RAW',\n"," 'SUBDIR_UTILS',\n"," 'TEST_SENTENCES_LS',\n"," 'TEST_WORDS_LS',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__spec__',\n"," 'corpus_texts_dt',\n"," 'corpus_titles_dt',\n"," 'corpus_titles_ls',\n"," 'lexicons_dt',\n"," 'model_titles_dt',\n"," 'models_ensemble_dt',\n"," 'models_titles_dt']"]},"metadata":{},"execution_count":45}],"source":["dir(global_vars)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"wdTV7rVOAsNO","outputId":"059aa626-a280-46a9-93f8-740f9906a5fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Variable                Type             Data/Info\n","--------------------------------------------------\n","Corpus_Genre            str              finance\n","Corpus_Number           int              2\n","Corpus_Type             str              reference\n","EMOTICONS_EMO           dict             n=221\n","IN_COLAB                bool             True\n","Image                   type             <class 'IPython.core.display.Image'>\n","InteractiveShell        MetaHasTraits    <class 'IPython.core.inte<...>eshell.InteractiveShell'>\n","PATH_TEXT_CLEAN         str              ./text_clean/text_clean_finance_ref\n","PATH_TEXT_RAW           str              ./text_raw/text_raw_finance_ref\n","PATH_UTILS              str              /gdrive/MyDrive/sentimentarcs_notebooks/utils\n","Path                    type             <class 'pathlib.Path'>\n","Path_to_SentimentArcs   str              /gdrive/MyDrive/sentimentarcs_notebooks/\n","PySBDFactory            type             <class 'pysbd.utils.PySBDFactory'>\n","SUBDIR_TEXT_CLEAN       str              ./text_clean/text_clean_finance_ref\n","SUBDIR_TEXT_RAW         str              text_raw_finance_ref\n","UNICODE_EMOJI           dict             n=3521\n","contractions            module           <module 'contractions' fr<...>ontractions/__init__.py'>\n","corpus_titles_ls        dict_keys        dict_keys(['federalreserv<...>'european_central_bank'])\n","datetime                type             <class 'datetime.datetime'>\n","deepcopy                function         <function deepcopy at 0x7fc806412830>\n","display                 function         <function display at 0x7fc804d14290>\n","doc                     Doc              Apples and oranges are si<...> Boots and hippos aren't.\n","drive                   module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n","emot                    module           <module 'emot' from '/usr<...>ckages/emot/__init__.py'>\n","emot_obj                emot             <emot.core.emot object at 0x7fc750907550>\n","glob                    module           <module 'glob' from '/usr/lib/python3.7/glob.py'>\n","global_vars             module           <module 'global_vars' fro<...>ks/utils/global_vars.py'>\n","hero                    module           <module 'texthero' from '<...>es/texthero/__init__.py'>\n","interactive             MetaHasTraits    <class 'ipywidgets.widget<...>interaction.interactive'>\n","json                    module           <module 'json' from '/usr<...>hon3.7/json/__init__.py'>\n","logging                 module           <module 'logging' from '/<...>3.7/logging/__init__.py'>\n","model_titles_ls         dict_keys        dict_keys(['AutoGluon_Tex<...>ER', 'AFINN', 'XGBoost'])\n","nlp                     English          <spacy.lang.en.English object at 0x7fc74cb32dd0>\n","nltk                    module           <module 'nltk' from '/usr<...>ckages/nltk/__init__.py'>\n","np                      module           <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n","os                      module           <module 'os' from '/usr/lib/python3.7/os.py'>\n","pd                      module           <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n","plt                     module           <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n","preprocessing           module           <module 'texthero.preproc<...>xthero/preprocessing.py'>\n","re                      module           <module 're' from '/usr/lib/python3.7/re.py'>\n","read_yaml               module           <module 'utils.read_yaml'<...>ooks/utils/read_yaml.py'>\n","sa_config               module           <module 'utils.sa_config'<...>ooks/utils/sa_config.py'>\n","sent_tokenize           function         <function sent_tokenize at 0x7fc7d5899d40>\n","sns                     module           <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n","spacy                   module           <module 'spacy' from '/us<...>kages/spacy/__init__.py'>\n","string                  module           <module 'string' from '/u<...>lib/python3.7/string.py'>\n","sys                     module           <module 'sys' (built-in)>\n","text                    str              I love python ☮ 🙂 ❤ :-) :-( :-)))\n","token                   Token            .\n","warnings                module           <module 'warnings' from '<...>b/python3.7/warnings.py'>\n","widgets                 module           <module 'ipywidgets.widge<...>ets/widgets/__init__.py'>\n","yaml                    module           <module 'yaml' from '/usr<...>ckages/yaml/__init__.py'>\n"]}],"source":["%whos"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189545827,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"j1Lp4GLndZhY","outputId":"6bdd18a2-6a03-4304-9563-d5dd2deb589b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"mine,will,that,made,until,our,both,off,are,’ll,this,many,whether,but,herself,by,get,since,move,two,’m,‘ll,as,thus,with,much,me,doing,it,do,him,anyway,‘s,so,already,full,sometimes,might,whoever,seeming,please,after,everything,none,bottom,anyhow,become,across,very,always,’d,mostly,eleven,most,quite,empty,although,n‘t,yours,seems,not,then,n’t,really,also,nor,these,never,own,front,becoming,who,every,various,amongst,seemed,its,'ve,indeed,am,perhaps,few,thence,together,myself,through,a,ever,forty,above,down,via,on,side,others,ours,itself,however,here,himself,throughout,behind,amount,four,i,while,meanwhile,anyone,where,were,can,other,all,back,see,three,up,nevertheless,be,hereafter,make,serious,go,‘m,top,may,now,nothing,how,take,further,there,in,whence,once,show,if,formerly,beyond,wherein,during,had,becomes,no,'m,keep,next,whatever,could,somehow,last,even,latterly,twelve,everywhere,due,’s,to,seem,should,name,along,part,being,did,'s,from,upon,still,something,whereupon,them,under,which,elsewhere,whom,against,you,than,namely,someone,except,done,she,ca,first,why,wherever,third,whereby,sixty,whose,those,is,he,nine,hundred,they,onto,whither,though,enough,'ll,what,her,anywhere,thereby,unless,same,over,six,'d,of,ten,beforehand,whenever,often,eight,toward,my,therefore,beside,‘re,’ve,around,re,noone,before,became,yourselves,several,anything,the,whereas,almost,because,when,thru,out,themselves,yourself,one,any,hers,fifteen,at,sometime,their,former,call,besides,into,give,’re,has,well,afterwards,‘ve,everyone,among,nobody,within,otherwise,either,n't,your,just,hereupon,or,thereafter,again,was,have,another,regarding,'re,each,must,whole,we,some,per,thereupon,neither,fifty,‘d,towards,say,least,us,too,yet,rather,put,five,nowhere,hence,below,using,his,twenty,only,used,alone,such,an,and,without,hereby,else,herein,more,ourselves,moreover,for,latter,about,therein,would,whereafter,less,somewhere,cannot,been,does,between\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47},{"output_type":"stream","name":"stdout","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n"]}],"source":["# Verify English Stopword List\n","\n","stopwords_spacy_en_ls = nlp.Defaults.stop_words\n","\n","','.join([x for x in stopwords_spacy_en_ls])\n","\n","stopwords_en_ls = stopwords_spacy_en_ls\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')"]},{"cell_type":"markdown","metadata":{"id":"ZD8Qe-H12p6j"},"source":["## (Optional) Customize Stopword List (add/del)"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189546521,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Q_438Act0jds","outputId":"5b86f992-2f82-4a90-abc0-3a0158d58b52"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","There are 326 default English Stopwords from spaCy\n","\n","    Deleting these stopwords: {'a', 'jimmy', 'but', 'an', 'the', 'dean', 'yet'}\n","    Adding these stopwords: {'a', 'an', 'but', 'the', 'yet'}\n","\n","Final Count: 326 Stopwords\n"]}],"source":["# Customize Default SpaCy English Stopword List\n","\n","print(f'\\n\\nThere are {len(stopwords_spacy_en_ls)} default English Stopwords from spaCy\\n')\n","\n","# [CUSTOMIZE] Stopwords to ADD or DELETE from default spaCy English stopword list\n","LOCAL_STOPWORDS_DEL_EN = set(global_vars.STOPWORDS_DEL_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Deleting these stopwords: {LOCAL_STOPWORDS_DEL_EN}')\n","LOCAL_STOPWORDS_ADD_EN = set(global_vars.STOPWORDS_ADD_EN).union(set(['a','an','the','but','yet']))\n","print(f'    Adding these stopwords: {LOCAL_STOPWORDS_ADD_EN}\\n')\n","\n","stopwords_en_ls = list(set(stopwords_spacy_en_ls).difference(set(LOCAL_STOPWORDS_DEL_EN)).union(set(LOCAL_STOPWORDS_ADD_EN)))\n","print(f'Final Count: {len(stopwords_en_ls)} Stopwords')"]},{"cell_type":"markdown","metadata":{"id":"EA1yTaY_9Qod"},"source":["## Setup Matplotlib Style"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1649189547999,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"xmf7SFb7PnUi","outputId":"d8a8b8ac-ac9e-4425-8009-6833364fb35a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n"," New figure size:  (20, 10)\n","Matplotlib Configuration ------------------------------\n","\n","  (Uncomment to view)\n","\n","  Edit ./utils/config_matplotlib.py to change\n"]}],"source":["# Configure Matplotlib\n","\n","# View available styles\n","# plt.style.available\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_matplotlib.py'\n","\n","config_matplotlib()\n","\n","print('Matplotlib Configuration ------------------------------')\n","print('\\n  (Uncomment to view)')\n","# plt.rcParams.keys()\n","print('\\n  Edit ./utils/config_matplotlib.py to change')"]},{"cell_type":"markdown","metadata":{"id":"7dPPrZwyIIze"},"source":["## Setup Seaborn Style"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1649189548709,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"hM3oRY-UOmzX","outputId":"c7a2e502-64a2-43bb-b180-cf575d968c31"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Seaborn Configuration ------------------------------\n","\n"]}],"source":["# Configure Seaborn\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/config_seaborn.py'\n","\n","config_seaborn()\n","\n","print('Seaborn Configuration ------------------------------\\n')\n","# print('\\n  Update ./utils/config_seaborn.py to display seaborn settings')"]},{"cell_type":"markdown","metadata":{"id":"xBpIUgstnE62"},"source":["## **Utility Functions**"]},{"cell_type":"markdown","metadata":{"id":"JXG_G6um4ijG"},"source":["### Generate Convenient Data Lists"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136,"status":"ok","timestamp":1649191588970,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"TO08GFoGlP3y","outputId":"73885aff-9703-4b58-b9a0-e5f531b0a799"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary: corpus_titles_dt\n"]},{"output_type":"execute_result","data":{"text/plain":["{'bogfederalreserve_speech_1997-2022': ['Federal Reserve Board of Governor Speeches (Jan 1997 - Feb 2022)',\n","  datetime.date(1997, 1, 8),\n","  datetime.date(2022, 2, 25)],\n"," 'eucentralbank_speeches_1998-2022': ['European Central Bank Speeches (Jul 1998 - Mar 2022)',\n","  datetime.date(1998, 7, 17),\n","  datetime.date(2022, 3, 1)]}"]},"metadata":{},"execution_count":83},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Corpus Texts:\n","  bogfederalreserve_speech_1997-2022\n","  eucentralbank_speeches_1998-2022\n","\n","\n","\n","Natural Corpus Titles:\n","  Federal Reserve Board of Governor Speeches (Jan 1997 - Feb 2022)\n","  European Central Bank Speeches (Jul 1998 - Mar 2022)\n"]}],"source":["# Derive List of Texts in Corpus a)keys and b)full author and titles\n","\n","print('Dictionary: corpus_titles_dt')\n","global_vars.corpus_titles_dt\n","print('\\n')\n","\n","corpus_texts_ls = list(global_vars.corpus_titles_dt.keys())\n","print(f'\\nCorpus Texts:')\n","for akey in corpus_texts_ls:\n","  print(f'  {akey}')\n","print('\\n')\n","\n","print(f'\\nNatural Corpus Titles:')\n","corpus_titles_ls = [x[0] for x in list(global_vars.corpus_titles_dt.values())]\n","for akey in corpus_titles_ls:\n","  print(f'  {akey}')\n"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1649191593177,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"rQNlQr4_Ckb1","outputId":"44ac1590-d500-4447-a838-ccccffd88e00"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","There are 12 Lexicon Models\n","  Lexicon Model #0: sentimentr_sentimentr\n","  Lexicon Model #1: pysentimentr_jockersrinker\n","  Lexicon Model #2: pysentimentr_huliu\n","  Lexicon Model #3: pysentimentr_nrc\n","  Lexicon Model #4: pysentimentr_sentiword\n","  Lexicon Model #5: pysentimentr_senticnet\n","  Lexicon Model #6: pysentimentr_lmcd\n","  Lexicon Model #7: syuzhetr_afinn\n","  Lexicon Model #8: syuzhetr_bing\n","  Lexicon Model #9: syuzhetr_nrc\n","  Lexicon Model #10: syuzhetr_syuzhetr\n","  Lexicon Model #11: afinn\n","\n","There are 9 Heuristic Models\n","  Heuristic Model #0: pattern\n","  Heuristic Model #1: sentimentr_jockersrinker\n","  Heuristic Model #2: sentimentr_jockers\n","  Heuristic Model #3: sentimentr_bing\n","  Heuristic Model #4: sentimentr_nrc\n","  Heuristic Model #5: sentimentr_sentiword\n","  Heuristic Model #6: sentimentr_senticnet\n","  Heuristic Model #7: sentimentr_lmcd\n","  Heuristic Model #8: vader\n","\n","There are 8 Traditional ML Models\n","  Traditional ML Model #0: autogluon\n","  Traditional ML Model #1: flaml\n","  Traditional ML Model #2: logreg\n","  Traditional ML Model #3: logreg_cv\n","  Traditional ML Model #4: multinb\n","  Traditional ML Model #5: rf\n","  Traditional ML Model #6: textblob\n","  Traditional ML Model #7: xgb\n","\n","There are 5 DNN Models\n","  DNN Model #0: cnn\n","  DNN Model #1: fcn\n","  DNN Model #2: flair\n","  DNN Model #3: lstm\n","  DNN Model #4: stanza\n","\n","There are 8 Transformer Models\n","  Transformer Model #0: imdb2way\n","  Transformer Model #1: hinglish\n","  Transformer Model #2: nlptown\n","  Transformer Model #3: yelp\n","  Transformer Model #4: huggingface\n","  Transformer Model #5: roberta15lg\n","  Transformer Model #6: robertaxml8lang\n","  Transformer Model #7: t5imdb50k\n","\n","There are 5 Total Models:\n","  Model # 0: lexicon\n","  Model # 1: heuristic\n","  Model # 2: ml\n","  Model # 3: dnn\n","  Model # 4: transformer\n","\n","There are 5 Total Models (+1 for Ensemble Mean)\n","\n","Test: Lexicon Family of Models:\n"]},{"output_type":"execute_result","data":{"text/plain":["['sentimentr_sentimentr',\n"," 'pysentimentr_jockersrinker',\n"," 'pysentimentr_huliu',\n"," 'pysentimentr_nrc',\n"," 'pysentimentr_sentiword',\n"," 'pysentimentr_senticnet',\n"," 'pysentimentr_lmcd',\n"," 'syuzhetr_afinn',\n"," 'syuzhetr_bing',\n"," 'syuzhetr_nrc',\n"," 'syuzhetr_syuzhetr',\n"," 'afinn']"]},"metadata":{},"execution_count":84}],"source":["# Get Model Families of Ensemble\n","\n","from utils.get_model_families import get_ensemble_model_famalies\n","\n","global_vars.models_ensemble_dt = get_ensemble_model_famalies(global_vars.models_titles_dt)\n","\n","print('\\nTest: Lexicon Family of Models:')\n","global_vars.models_ensemble_dt['lexicon']"]},{"cell_type":"markdown","metadata":{"id":"pjQBAoLjOzDO"},"source":["### Text Cleaning "]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1649189569965,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"qpchjKtfy2H4","outputId":"d26984d4-d26d-406a-cbcd-19c772ad07bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<function texthero.preprocessing.fillna>,\n"," <function texthero.preprocessing.lowercase>,\n"," <function texthero.preprocessing.remove_digits>,\n"," <function texthero.preprocessing.remove_punctuation>,\n"," <function texthero.preprocessing.remove_diacritics>,\n"," <function texthero.preprocessing.remove_stopwords>,\n"," <function texthero.preprocessing.remove_whitespace>]"]},"metadata":{},"execution_count":53}],"source":["# [VERIFY]: Texthero preprocessing pipeline\n","\n","hero.preprocessing.get_default_pipeline()\n","\n","\n","\n","# Create Default and Custom Stemming TextHero pipeline\n","\n","# Create a custom cleaning pipeline\n","def_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                # , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace]\n","\n","# Create a custom cleaning pipeline\n","stem_pipeline = [preprocessing.fillna\n","                , preprocessing.lowercase\n","                , preprocessing.remove_digits\n","                , preprocessing.remove_punctuation\n","                , preprocessing.remove_diacritics\n","                , preprocessing.remove_stopwords\n","                , preprocessing.remove_whitespace\n","                , preprocessing.stem]\n","                   \n","# Test: pass the custom_pipeline to the pipeline argument\n","# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":568,"status":"ok","timestamp":1649189570707,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"J2DLzn1TJ12C","outputId":"4711f227-f001-435e-f788-7bb7a68522bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Test #1:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i be run late for a meeting with all the many people .',\n"," 'what time be it when you fall down run away from a grow problem ?',\n"," 'you have get to be kid me - you be joke right ?']"]},"metadata":{},"execution_count":54},{"output_type":"stream","name":"stdout","text":["\n","Test #2:\n","\n","['I', 'will', 'not', 'go', 'and', 'you', 'can', 'not', 'make', 'me', '.']\n","['Billy', 'be', 'run', 'really', 'quickly', 'and', 'with', 'great', 'haste', '.']\n","['Eating', 'freshly', 'catch', 'seafood', '.']\n","\n","Test #3:\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['i will not go and you can not make me .',\n"," 'billy be run really quickly and with great haste .',\n"," 'eating freshly catch seafood .']"]},"metadata":{},"execution_count":54}],"source":["# Test Text Cleaning Functions\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/text_cleaners.py'\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# test_suite_ls = []\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1649189570882,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"FNiSgwZqElpJ","outputId":"96817d2b-c49f-4f19-b79b-9acd7dd316af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i be go to start study much often and work hard .'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n","BEFORE stripping out headings len: 96\n","   Parag count before processing sents: 2\n","pysbd found 3 Sentences in Paragraph #0\n","      3 Sentences remain after cleaning\n","pysbd found 3 Sentences in Paragraph #1\n","      3 Sentences remain after cleaning\n","Processing asent: Hello.\n","Processing asent: You are a great dude!\n","Processing asent: WTF?\n","Processing asent: You are a goat.\n","Processing asent: What is a goat?!? A big lazy GOAT...\n","Processing asent: No way-\n","About to return sents_ls with len = 7\n"]},{"output_type":"execute_result","data":{"text/plain":["['Hello.',\n"," 'You are a great dude!',\n"," 'WTF?',\n"," 'You are a goat.',\n"," 'What is a goat?!?',\n"," 'A big lazy GOAT...',\n"," 'No way-']"]},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","test_str: [Hilarious face with tears of joy. The feeling of making a sale smiling face with sunglasses, The feeling of actually ;) fulfilling orders unamused face]\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'Hilarious face with tears of joy. The feeling Surprise of making a sale smiling face with sunglasses, The feeling Frown sad andry or pouting of actually Wink or smirk fulfilling orders unamused face'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'i do not know laughing out loud you suck!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0    the rain in spain\n","1      wtf do you know\n","Name: text_dirty, dtype: object"]},"metadata":{},"execution_count":55},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Test: lemma_pipe()\\nif \\'lemma_pipe\\' in test_suite_ls:\\n  print(\\'\\nTest #1:\\n\\')\\n  test_ls = [\\'I am running late for a meetings with all the many people.\\',\\n            \\'What time is it when you fall down running away from a growing problem?\\',\\n            \"You\\'ve got to be kidding me - you\\'re joking right?\"]\\n  lemma_pipe(test_ls)\\n  print(\\'\\nTest #2:\\n\\')\\n  texts = pd.Series([\"I won\\'t go and you can\\'t make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\\n  for doc in nlp.pipe(texts):\\n    print([tok.lemma_ for tok in doc])\\n  print(\\'\\nTest #3:\\n\\')\\n  lemma_pipe(texts)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}],"source":["# Test Text Cleaning Functions\n","\n","%run -i './utils/text_cleaners.py'\n","# from utils.text_cleaners import text2lemmas, text_str2sents, emojis2text, expand_slang, clean_text, lemma_pipe\n","\n","test_suite_ls = ['text2lemmas',\n","                 'text_str2sents',\n","                 'textfile2df',\n","                 'emojis2text',\n","                 'all_emos2text',\n","                 'expand_slang',\n","                 'clean_text',\n","                 'lemma_pipe'\n","                 ]\n","\n","# Comment out this line to active tests above\n","# test_suite_ls = []\n","\n","\n","# Test: text2lemmas()\n","if 'text2lemmas' in test_suite_ls:\n","  text2lemmas('I am going to start studying more often and working harder.', lowercase=True, remove_stopwords=False)\n","  print('\\n')\n","\n","# Test: text_str2sents()\n","if 'text_str2sents' in test_suite_ls:\n","  text_str2sents('Hello. You are a great dude! WTF?\\n\\n You are a goat. What is a goat?!? A big lazy GOAT... No way-', pysbd_only=False) # !?! Dr. and Mrs. Elipses...', pysbd_only=True)\n","  print('\\n')\n","\n","# Test: textfile2df()\n","if 'textfile2df' in test_suite_ls:\n","  # ???\n","  print('\\n')\n","\n","# Test: emojis2text()\n","if 'emojis2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling of making a sale 😎, The feeling of actually ;) fulfilling orders 😒\"\n","  test_str = emojis2text(test_str)\n","  print(f'test_str: [{test_str}]')\n","  print('\\n')\n","\n","# Test: all_emos2text()\n","if 'all_emos2text' in test_suite_ls:\n","  test_str = \"Hilarious 😂. The feeling :o of making a sale 😎, The feeling :( of actually ;) fulfilling orders 😒\"\n","  all_emos2text(test_str)\n","  print('\\n')\n","\n","# Test: expand_slang():\n","if 'expand_slang' in test_suite_ls:\n","  expand_slang('idk LOL you suck!')\n","  print('\\n')\n","\n","# Test: clean_text()\n","if 'clean_text' in test_suite_ls:\n","  test_df = pd.DataFrame({'text_dirty':['The RAin in SPain','WTF?!?! Do you KnoW...']})\n","  clean_text(test_df, 'text_dirty', text_type='formal')\n","  print('\\n')\n","\"\"\"\n","# Test: lemma_pipe()\n","if 'lemma_pipe' in test_suite_ls:\n","  print('\\nTest #1:\\n')\n","  test_ls = ['I am running late for a meetings with all the many people.',\n","            'What time is it when you fall down running away from a growing problem?',\n","            \"You've got to be kidding me - you're joking right?\"]\n","  lemma_pipe(test_ls)\n","  print('\\nTest #2:\\n')\n","  texts = pd.Series([\"I won't go and you can't make me.\", \"Billy is running really quickly and with great haste.\", \"Eating freshly caught seafood.\"])\n","  for doc in nlp.pipe(texts):\n","    print([tok.lemma_ for tok in doc])\n","  print('\\nTest #3:\\n')\n","  lemma_pipe(texts)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WlfujTpEKhCp"},"source":["### File Functions"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"yOX-fpiuApL4","executionInfo":{"status":"ok","timestamp":1649189571381,"user_tz":240,"elapsed":147,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","%run -i './utils/file_utils.py'\n","# from utils.file_utils import *\n","\n","# %run -i './utils/file_utils.py'\n","\n","# TODO: Not used? Delete?\n","# get_fullpath(text_title_str, ftype='data_clean', fig_no='', first_note = '',last_note='', plot_ext='png', no_date=False)"]},{"cell_type":"markdown","metadata":{"id":"Ilz5X9AEbP8r"},"source":["# **[STEP 2] Read in Corpus and Clean**"]},{"cell_type":"markdown","metadata":{"id":"1a-1wyeiGt4Z"},"source":["## Create List of Raw Textfiles"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1649189574720,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"Tcl8BtfGfvgZ","outputId":"ca187b21-ec38-402f-e99c-fcf26d1b3348"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/gdrive/MyDrive/sentimentarcs_notebooks/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["global_vars.SUBDIR_SENTIMENTARCS"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649189574857,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"cHXNIMtPfZ1j","outputId":"04456698-0817-44fe-9ae4-fd6df401ebcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["path_text_raw: ./text_raw/text_raw_finance_reference\n","\n","Full Path to Corpus text_raw: ./text_raw/./text_raw/text_raw_finance_reference/\n"]}],"source":["# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","path_text_raw = './' + '/'.join(global_vars.SUBDIR_TEXT_RAW.split('/')[1:-1])\n","print(f'path_text_raw: {path_text_raw}\\n')\n","# SUBDIR_TEXT_RAW = path_text_raw + '/'\n","print(f'Full Path to Corpus text_raw: ./text_raw/{global_vars.SUBDIR_TEXT_RAW}')"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1649189575037,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"eyDzL7zJfYZf","outputId":"8bf1bf9b-e27d-4e4e-b923-2b194c470aa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/sentimentarcs_notebooks\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1649189575037,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"iXN4pdZReL4Y","outputId":"136f1fab-a82c-45d5-dbf8-986c1dcd76d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Corpus_Genre: finance\n","Corpus_Type: reference\n","\n","Corpus Subdir: ./text_raw/./text_raw/text_raw_finance_reference/\n","\n","texts_raw_root_ls:\n","  []\n","\n","\n","There are 0 Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]\n"]}],"source":["# Get a list of all the Textfile filename roots in Subdir text_raw\n","\n","# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","corpus_titles_ls = list(global_vars.corpus_titles_dt.keys())\n","\n","print(f'Corpus_Genre: {global_vars.Corpus_Genre}')\n","print(f'Corpus_Type: {global_vars.Corpus_Type}\\n')\n","\n","# Build path to Corpus Subdir\n","# TODO: Temp fix until print(f'Original: {SUBDIR_TEXT_RAW}\\n')\n","# path_text_raw = './' + '/'.join(SUBDIR_TEXT_RAW.split('/')[1:-1]) + '/' + SUBDIR_TEXT_RAW\n","path_text_raw = './text_raw/' + global_vars.SUBDIR_TEXT_RAW\n","print(f'Corpus Subdir: {path_text_raw}')\n","\n","# Create a List (preprocessed_ls) of all preprocessed text files\n","try:\n","  # texts_raw_ls = glob.glob(f'{SUBDIR_TEXT_RAW}*.txt')\n","  texts_raw_root_ls = glob.glob(f'{path_text_raw}/*.txt')\n","  texts_raw_root_ls = [x.split('/')[-1] for x in texts_raw_root_ls]\n","  texts_raw_root_ls = [x.split('.')[0] for x in texts_raw_root_ls]\n","except IndexError:\n","  raise RuntimeError('No *.txt files found')\n","\n","print(f'\\ntexts_raw_root_ls:\\n  {texts_raw_root_ls}\\n')\n","\n","text_ct = 0\n","for afile_root in texts_raw_root_ls:\n","  # file_root = file_fullpath.split('/')[-1].split('.')[0]\n","  text_ct += 1\n","  print(f'{afile_root}: ') # {corpus_titles_dt[afile_root]}')\n","\n","print(f'\\nThere are {text_ct} Texts defined in SentmentArcs [corpus_dt] and found in the subdir: [SUBDIR_TEXT_RAW]')"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1649189575190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"lKpausmwhXJU","outputId":"5e9c3bd3-704a-4479-ec8c-8e559d989ae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access './text_raw/./text_raw/text_raw_finance_reference/': No such file or directory\n"]}],"source":["!ls -altr $path_text_raw"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649189575190,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"EvqS1TQthfxF","outputId":"ede4e56d-e9fa-4e25-c022-d9f19ab340b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":62}],"source":["glob.glob(f'{path_text_raw}/*.txt')"]},{"cell_type":"markdown","metadata":{"id":"KkXBipRrGoCQ"},"source":["## Read and Segment into Sentences"]},{"cell_type":"code","source":["corpus_titles_ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jP4fbmMuqwe","executionInfo":{"status":"ok","timestamp":1649189577800,"user_tz":240,"elapsed":146,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"5f089a64-df36-48f0-9258-0349af67dabb"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['federalreserve_speech_bog', 'european_central_bank']"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":835694,"status":"ok","timestamp":1649191245522,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"},"user_tz":240},"id":"eq50LyAMHKYX","outputId":"d4fb3186-cad2-43c1-82f1-f0b5706ebcb4"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m./utils/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Verify First Text is Segmented into text_raw Sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mglobal_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_texts_dt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_titles_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 13min 48s, sys: 2.57 s, total: 13min 51s\n","Wall time: 13min 55s\n"]}],"source":["%%time\n","%%capture\n","\n","# Read all Corpus Textfiles and Segment each into Sentences\n","\n","# NOTE:   3m30s Entire Corpus of 25 \n","#         7m30s Ref Corpus 32 Novels\n","#         7m24s Ref Corpus 32 Novels\n","#         1m00s New Corpus 2 Novels\n","\n","#        13m55s Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","\n","# Read all novel files into a Dictionary of DataFrames\n","#   Dict.keys() are novel names\n","#   Dict.values() are DataFrames with one row per Sentence\n","\n","# Continue here ONLY if last cell completed WITHOUT ERROR\n","\n","# anovel_df = pd.DataFrame()\n","\n","for i, file_root in enumerate(corpus_titles_ls):\n","  file_fullpath = f'{global_vars.SUBDIR_TEXT_RAW}{file_root}.txt'\n","  # print(f'Processing Novel #{i}: {file_fullpath}') # {file_root}')\n","  # fullpath_str = novels_subdir + asubdir + '/' + asubdir + '.txt'\n","  # print(f\"  Size: {os.path.getsize(file_fullpath)}\")\n","\n","  global_vars.corpus_texts_dt[file_root] = textfile2df(file_fullpath)\n","  \n","# corpus_dt.keys()\n","\n","# Verify First Text is Segmented into text_raw Sentences\n","print('\\n\\n')\n","\n","# global_vars.corpus_texts_dt[corpus_titles_ls[0]].head()\n","text_no = 0\n","print(f'Verify sample segmented Text: \\n    {corpus_texts_ls[text_no]}\\n')\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].head()\n"]},{"cell_type":"markdown","metadata":{"id":"tw-Ll-fdI_yb"},"source":["## Clean Sentences"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPrpL1wyNPva","outputId":"024fd4f7-88e8-46dc-83cd-6a2082e43bdb","executionInfo":{"status":"ok","timestamp":1649192046316,"user_tz":240,"elapsed":250576,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Novel #0: bogfederalreserve_speech_1997-2022...\n","  shape: (219966, 2)\n","Processing Novel #1: eucentralbank_speeches_1998-2022...\n","  shape: (298570, 2)\n","CPU times: user 3min 57s, sys: 2.26 s, total: 3min 59s\n","Wall time: 4min 10s\n"]}],"source":["%%time\n","\n","# NOTE: (no stem) 4m09s (24 Novels)\n","#       (w/ stem) 4m24s (24 Novels)\n","\n","\n","#         4m10s Finance FedBoard Gov Speeches 32M + EU Cent Bank SPeeches 38M\n","\n","i = 0\n","\n","for key_novel, atext_df in global_vars.corpus_texts_dt.items():\n","\n","  print(f'Processing Novel #{i}: {key_novel}...')\n","\n","  atext_df['text_clean'] = clean_text(atext_df, 'text_raw', text_type='formal')\n","  atext_df['text_clean'] = lemma_pipe(atext_df['text_clean'])\n","  atext_df['text_clean'] = atext_df['text_clean'].astype('string')\n","\n","  # TODO: Fill in all blank 'text_clean' rows with filler semaphore\n","  atext_df.text_clean = atext_df.text_clean.fillna('empty_placeholder')\n","\n","  atext_df.head(2)\n","\n","  print(f'  shape: {atext_df.shape}')\n","\n","  i += 1"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":871},"id":"RXuwwg2_XgZu","outputId":"d91a8fc0-6add-4023-a6ff-3cabb00853fc","executionInfo":{"status":"ok","timestamp":1649193144061,"user_tz":240,"elapsed":155,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            text_raw  \\\n","0  Mr. Duisenberg reports on the outcome of the s...   \n","1  The Governing Council first assessed current e...   \n","2  The general picture is one of continued econom...   \n","3  Several forecasts made during spring 1998 have...   \n","4  As far as price developments are concerned, in...   \n","5  Output growth has remained strong in recent qu...   \n","6  Economic growth has been driven increasingly b...   \n","7  Private consumption and stockbuilding have bee...   \n","8  The favourable conjunctural situation has star...   \n","9  It is evident, however, that economic growth a...   \n","\n","                                          text_clean  \n","0  mr duisenberg report on the outcome of the 2 m...  \n","1  the govern council ﻿1 assess current economic ...  \n","2  the general picture be one of continue economi...  \n","3  several forecast make during spring have even ...  \n","4  a far a price development be concern inflation...  \n","5  output growth have remain strong in recent qua...  \n","6  economic growth have be drive increasingly by ...  \n","7  private consumption and stockbuilding have be ...  \n","8  the favourable conjunctural situation have sta...  \n","9  it be evident however that economic growth alo...  "],"text/html":["\n","  <div id=\"df-ac648074-c40d-404d-bb5f-d1e1504a7706\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_raw</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Mr. Duisenberg reports on the outcome of the s...</td>\n","      <td>mr duisenberg report on the outcome of the 2 m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Governing Council first assessed current e...</td>\n","      <td>the govern council ﻿1 assess current economic ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The general picture is one of continued econom...</td>\n","      <td>the general picture be one of continue economi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Several forecasts made during spring 1998 have...</td>\n","      <td>several forecast make during spring have even ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>As far as price developments are concerned, in...</td>\n","      <td>a far a price development be concern inflation...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Output growth has remained strong in recent qu...</td>\n","      <td>output growth have remain strong in recent qua...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Economic growth has been driven increasingly b...</td>\n","      <td>economic growth have be drive increasingly by ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Private consumption and stockbuilding have bee...</td>\n","      <td>private consumption and stockbuilding have be ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The favourable conjunctural situation has star...</td>\n","      <td>the favourable conjunctural situation have sta...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>It is evident, however, that economic growth a...</td>\n","      <td>it be evident however that economic growth alo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac648074-c40d-404d-bb5f-d1e1504a7706')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ac648074-c40d-404d-bb5f-d1e1504a7706 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ac648074-c40d-404d-bb5f-d1e1504a7706');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":112},{"output_type":"execute_result","data":{"text/plain":["                                                 text_raw  \\\n","298560  @87ABCD6 )$1113631E21)2!191235192F913\"62606133...   \n","298561                          [5!612316261E1 269213121!   \n","298562  362!09251!31111\" B)91291123161!9231\\91]359I612...   \n","298563  151!632269131912!313632512626!2626163 99236166...   \n","298564                                       =36128132(2\"   \n","298565                      _abcdeafdbhbibafjkellmicfdein   \n","298566    $612321 BK >32\"32I2J1320 p ABAK C XX 1!2q113621   \n","298567                       24185152458824158788714516 !   \n","298568                                              #$%&!   \n","298569                                   012345678 412084   \n","\n","                                               text_clean  \n","298560  87abcd6 1113631e21 191235192f913 21325 g 11h21...  \n","298561                                        612316261e1  \n","298562                                    b \\ 359i61230 \\  \n","298563                                        y1365321131  \n","298564                                                     \n","298565                       abcdeafdbhbibafjkellmicfdein  \n","298566                  bk 32i2j1320 p abak c xx 2q113621  \n","298567                                                     \n","298568                                                     \n","298569                                                     "],"text/html":["\n","  <div id=\"df-31d1980f-6136-481e-9539-561496bb217a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_raw</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>298560</th>\n","      <td>@87ABCD6 )$1113631E21)2!191235192F913\"62606133...</td>\n","      <td>87abcd6 1113631e21 191235192f913 21325 g 11h21...</td>\n","    </tr>\n","    <tr>\n","      <th>298561</th>\n","      <td>[5!612316261E1 269213121!</td>\n","      <td>612316261e1</td>\n","    </tr>\n","    <tr>\n","      <th>298562</th>\n","      <td>362!09251!31111\" B)91291123161!9231\\91]359I612...</td>\n","      <td>b \\ 359i61230 \\</td>\n","    </tr>\n","    <tr>\n","      <th>298563</th>\n","      <td>151!632269131912!313632512626!2626163 99236166...</td>\n","      <td>y1365321131</td>\n","    </tr>\n","    <tr>\n","      <th>298564</th>\n","      <td>=36128132(2\"</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>298565</th>\n","      <td>_abcdeafdbhbibafjkellmicfdein</td>\n","      <td>abcdeafdbhbibafjkellmicfdein</td>\n","    </tr>\n","    <tr>\n","      <th>298566</th>\n","      <td>$612321 BK &gt;32\"32I2J1320 p ABAK C XX 1!2q113621</td>\n","      <td>bk 32i2j1320 p abak c xx 2q113621</td>\n","    </tr>\n","    <tr>\n","      <th>298567</th>\n","      <td>24185152458824158788714516 !</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>298568</th>\n","      <td>#$%&amp;!</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>298569</th>\n","      <td>012345678 412084</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31d1980f-6136-481e-9539-561496bb217a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-31d1980f-6136-481e-9539-561496bb217a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-31d1980f-6136-481e-9539-561496bb217a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":112},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 298570 entries, 0 to 298569\n","Data columns (total 2 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   text_raw    298570 non-null  object\n"," 1   text_clean  298570 non-null  string\n","dtypes: object(1), string(1)\n","memory usage: 4.6+ MB\n"]}],"source":["# Verify the first Text in Corpus is cleaned\n","\n","text_no = 1\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].head(10)\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].tail(10)\n","global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].info()"]},{"cell_type":"markdown","source":["## Detect Language and Filter out Noise\n","\n","To deal with noisy text:\n","\n","* ftfy: fix bad encodings\n","* chardet: detect encoding\n","* langdet: detect language\n","* (custom): skip non-sense sentences\n"," \n","References:\n","\n","* http://cs229.stanford.edu/proj2014/Ian%20Tenney,%20A%20General-Purpose%20Sentence-Level%20Nonsense%20Detector.pdf"],"metadata":{"id":"KkdYuF1Okz_E"}},{"cell_type":"code","source":["!pip install ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsF6Su0Bpu1-","executionInfo":{"status":"ok","timestamp":1649194505187,"user_tz":240,"elapsed":2739,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"41c1d900-1eb5-4e14-8e9d-cb2fcf5bd47e"},"execution_count":140,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n"]}]},{"cell_type":"code","source":["from ftfy import fix_and_explain, apply_plan, fix_text"],"metadata":{"id":"CVnZm8I5puv9","executionInfo":{"status":"ok","timestamp":1649194785037,"user_tz":240,"elapsed":140,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":150,"outputs":[]},{"cell_type":"code","source":["sent_no = -2\n","\n","sample_str = global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].iloc[sent_no]['text_raw']\n","sample_str = \"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\"\n","\n","fixed, explanation = fix_and_explain(sample_str)\n","fixed\n","explanation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"7VyM6b8Gpux-","executionInfo":{"status":"ok","timestamp":1649194614252,"user_tz":240,"elapsed":248,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0469fb78-3949-4531-fbf6-f68503b7308e"},"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'LóPEZ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":149},{"output_type":"execute_result","data":{"text/plain":["[('apply', 'unescape_html'),\n"," ('apply', 'unescape_html'),\n"," ('apply', 'unescape_html'),\n"," ('encode', 'latin-1'),\n"," ('decode', 'utf-8')]"]},"metadata":{},"execution_count":149}]},{"cell_type":"code","source":["tail_indx_ls = list(range(-20, 0))\n","tail_indx_ls.reverse()\n","tail_indx_ls\n","\n","head_indx_ls = list(range(1, 10))\n","\n","# Pick one\n","# Option (a)\n","indx_ls = head_indx_ls\n","# Option (b)\n","indx_ls = tail_indx_ls\n","\n","for sent_no in indx_ls:\n","\n","  print(f'Sentence Index = {sent_no}:')\n","  sample_str = global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].iloc[sent_no]['text_raw']\n","  # sample_str = \"L&AMP;AMP;ATILDE;&AMP;AMP;SUP3;PEZ\"\n","\n","  fixed = fix_text(sample_str)\n","\n","  print(f'  Original Text:\\n    [{sample_str}]')\n","\n","  print(f'  Fixed Text:\\n    [{fixed}]')\n","\n","  fixed_alpha= re.sub('[^a-zA-Z]','',fixed)\n","\n","  print(f'  Fixed Text w/o punct or numbers:\\n    [{fixed_alpha}]\\n\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvobIE0Aq3H4","executionInfo":{"status":"ok","timestamp":1649195922458,"user_tz":240,"elapsed":216,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"56fad095-01f7-4c02-d652-f993be600052"},"execution_count":191,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-1,\n"," -2,\n"," -3,\n"," -4,\n"," -5,\n"," -6,\n"," -7,\n"," -8,\n"," -9,\n"," -10,\n"," -11,\n"," -12,\n"," -13,\n"," -14,\n"," -15,\n"," -16,\n"," -17,\n"," -18,\n"," -19,\n"," -20]"]},"metadata":{},"execution_count":191},{"output_type":"stream","name":"stdout","text":["Sentence Index = -1:\n","  Original Text:\n","    [012345678 412084]\n","  Fixed Text:\n","    [012345678 412084]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -2:\n","  Original Text:\n","    [#$%&!]\n","  Fixed Text:\n","    [#$%&!]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -3:\n","  Original Text:\n","    [24185152458824158788714516 !]\n","  Fixed Text:\n","    [24185152458824158788714516 !]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -4:\n","  Original Text:\n","    [$612321 BK >32\"32I2J1320 p ABAK C XX 1!2q113621]\n","  Fixed Text:\n","    [$612321 BK >32\"32I2J1320 p ABAK C XX 1!2q113621]\n","  Fixed Text w/o punct or numbers:\n","    [BKIJpABAKCXXq]\n","\n","\n","Sentence Index = -5:\n","  Original Text:\n","    [_abcdeafdbhbibafjkellmicfdein]\n","  Fixed Text:\n","    [_abcdeafdbhbibafjkellmicfdein]\n","  Fixed Text w/o punct or numbers:\n","    [abcdeafdbhbibafjkellmicfdein]\n","\n","\n","Sentence Index = -6:\n","  Original Text:\n","    [=36128132(2\"]\n","  Fixed Text:\n","    [=36128132(2\"]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -7:\n","  Original Text:\n","    [151!632269131912!313632512626!2626163 992361662!Y1365321131!151!]\n","  Fixed Text:\n","    [151!632269131912!313632512626!2626163 992361662!Y1365321131!151!]\n","  Fixed Text w/o punct or numbers:\n","    [Y]\n","\n","\n","Sentence Index = -8:\n","  Original Text:\n","    [362!09251!31111\" B)91291123161!9231\\91]359I61230?322 6\\ !]\n","  Fixed Text:\n","    [362!09251!31111\" B)91291123161!9231\\91]359I61230?322 6\\ !]\n","  Fixed Text w/o punct or numbers:\n","    [BI]\n","\n","\n","Sentence Index = -9:\n","  Original Text:\n","    [[5!612316261E1 269213121!]\n","  Fixed Text:\n","    [[5!612316261E1 269213121!]\n","  Fixed Text w/o punct or numbers:\n","    [E]\n","\n","\n","Sentence Index = -10:\n","  Original Text:\n","    [@87ABCD6 )$1113631E21)2!191235192F913\"6260613312 6231 921392913\"660156666366906312 2!9163211236 21325G$11H21 2>7F*0133\"2013312 6^612306091 2!1G119291115691=8(I610I23\"1862 J36$1113 K$L!133L #7FI61230H609#132H232113GMN2OP569065Q056R,S:65/,7 TP,6,U0P4V6 W6 X X ?9226112312!Y1!633\"312296313963Z6 X)91#1!$212!91#1!]\n","  Fixed Text:\n","    [@87ABCD6 )$1113631E21)2!191235192F913\"6260613312 6231 921392913\"660156666366906312 2!9163211236 21325G$11H21 2>7F*0133\"2013312 6^612306091 2!1G119291115691=8(I610I23\"1862 J36$1113 K$L!133L #7FI61230H609#132H232113GMN2OP569065Q056R,S:65/,7 TP,6,U0P4V6 W6 X X ?9226112312!Y1!633\"312296313963Z6 X)91#1!$212!91#1!]\n","  Fixed Text w/o punct or numbers:\n","    [ABCDEFGHFGIIJKLLFIHHGMNOPQRSTPUPVWXXYZX]\n","\n","\n","Sentence Index = -11:\n","  Original Text:\n","    [91213269692 23\"1131662959266 6136152063 91#1!$21663911362312?91131799121912362163 1310130959631306265913162666311 2656913 959131!06911322\"2!61230603151 (3223!]\n","  Fixed Text:\n","    [91213269692 23\"1131662959266 6136152063 91#1!$21663911362312?91131799121912362163 1310130959631306265913162666311 2656913 959131!06911322\"2!61230603151 (3223!]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -12:\n","  Original Text:\n","    ['1'7+,-/012/30456067/580,69045482:;16<6!6$ 9666= 66>13230 ?]\n","  Fixed Text:\n","    ['1'7+,-/012/30456067/580,69045482:;16<6!6$ 9666= 66>13230 ?]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -13:\n","  Original Text:\n","    [012325678923 1219125365136612302201126252  9602236636115262619235178923 35921 11913519631326311311239220591661!3621 6211!21921631239113623122022\"62!1262303291 31913126391#1!$21916613\"95913$11%913&'2(23)2*2!]\n","  Fixed Text:\n","    [012325678923 1219125365136612302201126252  9602236636115262619235178923 35921 11913519631326311311239220591661!3621 6211!21921631239113623122022\"62!1262303291 31913126391#1!$21916613\"95913$11%913&'2(23)2*2!]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -14:\n","  Original Text:\n","    [*6869#'7D?,'8?4%*D?864J&'P878698&9486&'4&657&*P878690 00'(& *7&(*6A94868'484I?4%&8&*57&'(&58%E(*6GA94868'484I?4%&8&*44'86&G\" *F46A&46D4I?4%&8&*8648*54%*DFD*64%*%4&68&486*7&'40C4)4%*D?864J&'8]\n","  Fixed Text:\n","    [*6869#'7D?,'8?4%*D?864J&'P878698&9486&'4&657&*P878690 00'(& *7&(*6A94868'484I?4%&8&*57&'(&58%E(*6GA94868'484I?4%&8&*44'86&G\" *F46A&46D4I?4%&8&*8648*54%*DFD*64%*%4&68&486*7&'40C4)4%*D?864J&'8]\n","  Fixed Text w/o punct or numbers:\n","    [DDJPPAIEGAIGFADIDFDCDJ]\n","\n","\n","Sentence Index = -15:\n","  Original Text:\n","    [*68&4?8&8D 86?%&764(*6444F8)468F4J8F4%6484 &*64(6*D86*70C&'464%4&?8&&*GC*6D*64&'94868&'*7F'J&'F(%8&9'F'46 68&4*(J8F4F6*J&'*D44%&*6J&'85*76'*6&8F444864PL*66M8M*D8L 0 00\"#L8(F(6*D&'4O64%4&%*&8%&J&'*A(8%8%*D?84,85979245 QRSS/T4774 1H'48)468F468&4*(4F*&8&4J8F4F6*J&'J8>C0 08(&461C0 0 88J'*4 54*J*76U4%4D546?6*K4%&* *7D46O4I?4%&8&*J'%'86F7859(44D*&64%&9&*J8F44F*&8&*'*J8&%& 57&&4D?]\n","  Fixed Text:\n","    [*68&4?8&8D 86?%&764(*6444F8)468F4J8F4%6484 &*64(6*D86*70C&'464%4&?8&&*GC*6D*64&'94868&'*7F'J&'F(%8&9'F'46 68&4*(J8F4F6*J&'*D44%&*6J&'85*76'*6&8F444864PL*66M8M*D8L 0 00\"#L8(F(6*D&'4O64%4&%*&8%&J&'*A(8%8%*D?84,85979245 QRSS/T4774 1H'48)468F468&4*(4F*&8&4J8F4F6*J&'J8>C0 08(&461C0 0 88J'*4 54*J*76U4%4D546?6*K4%&* *7D46O4I?4%&8&*J'%'86F7859(44D*&64%&9&*J8F44F*&8&*'*J8&%& 57&&4D?]\n","  Fixed Text w/o punct or numbers:\n","    [DFFJFDCGCDFJFFJFFJDJFPLMMDLLFDOJADQRSSTHFFJFFJJCCJJUDKDOIJFDJFFJD]\n","\n","\n","Sentence Index = -16:\n","  Original Text:\n","    [*6&46*(J'48& !H'4O76)49*(86F4%*6?]\n","  Fixed Text:\n","    [*6&46*(J'48& !H'4O76)49*(86F4%*6?]\n","  Fixed Text w/o punct or numbers:\n","    [JHOF]\n","\n","\n","Sentence Index = -17:\n","  Original Text:\n","    [*&8'$E684?6*7%46864F84 444&*?6*7%4846744DA%*7%&*6?6*7%&*8M788$E68486486F4 4I?]\n","  Fixed Text:\n","    [*&8'$E684?6*7%46864F84 444&*?6*7%4846744DA%*7%&*6?6*7%&*8M788$E68486486F4 4I?]\n","  Fixed Text w/o punct or numbers:\n","    [EFDAMEFI]\n","\n","\n","Sentence Index = -18:\n","  Original Text:\n","    [*&A?84D%*JA(*6A*F,J*6,0 B?6 3*6&8%4M7884867?6*7%486F4N78&&4*(?]\n","  Fixed Text:\n","    [*&A?84D%*JA(*6A*F,J*6,0 B?6 3*6&8%4M7884867?6*7%486F4N78&&4*(?]\n","  Fixed Text w/o punct or numbers:\n","    [ADJAAFJBMFN]\n","\n","\n","Sentence Index = -19:\n","  Original Text:\n","    [*)468#?]\n","  Fixed Text:\n","    [*)468#?]\n","  Fixed Text w/o punct or numbers:\n","    []\n","\n","\n","Sentence Index = -20:\n","  Original Text:\n","    [?9%8?8%&9*(&'44%**D9 H'4?64A%6F6*J&'?8&'&'464(*648648*854&886+84&&830 05\"#L*4&869 87&**D98F*584J*6,J4%*D48648&&'4K*&*8L3%*(464%4* #?]\n","  Fixed Text:\n","    [?9%8?8%&9*(&'44%**D9 H'4?64A%6F6*J&'?8&'&'464(*648648*854&886+84&&830 05\"#L*4&869 87&**D98F*584J*6,J4%*D48648&&'4K*&*8L3%*(464%4* #?]\n","  Fixed Text w/o punct or numbers:\n","    [DHAFJLDFJJDKL]\n","\n","\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"3fi5jah_q3Ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_kacrnaPpurq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install chardet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taoxvNmCnLsw","executionInfo":{"status":"ok","timestamp":1649193834346,"user_tz":240,"elapsed":2622,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"80ed3ee0-1de3-424e-886c-ae31cf9f3977"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"]}]},{"cell_type":"code","source":["import chardet\n"],"metadata":{"id":"7uDx5YTmnTfj","executionInfo":{"status":"ok","timestamp":1649193879550,"user_tz":240,"elapsed":205,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["sent_no = 1\n","\n","sample_str = global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].iloc[sent_no]['text_raw']\n","\n","sample_bin = bytearray(sample_str, encoding ='utf-8')\n","\n","print(f'Sentence #{sent_no}:\\n\\n    {sample_str}\\n\\n')\n","\n","try:\n","  alang = chardet.detect(sample_bin)\n","  # print(f'LANGUAGE: {detect(sample_str)}')\n","except:\n","  print('ERROR: Unrecognized language')\n","\n","print(f'LANGUAGE: {chardet.detect(sample_bin)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"LsC6AwqCnajB","executionInfo":{"status":"error","timestamp":1649194327500,"user_tz":240,"elapsed":204,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"a857fd49-9100-4fc4-865c-b4354b82172a"},"execution_count":139,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/gdrive/MyDrive/sentimentarcs_notebooks/utils/file_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_texts_dt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_texts_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msent_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msample_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sentence #{sent_no}:\\n\\n    {sample_str}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: string argument without an encoding"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"v1MFqb1Tnae7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwVb_cHyk4iO","executionInfo":{"status":"ok","timestamp":1649193233078,"user_tz":240,"elapsed":4179,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"814a9f9e-92a4-455c-d5c0-2620119c378a"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 981 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=9c4892bc044e8d97d07ee85325d510cfc6cac9d57b947e30d1ee135467e2bb1c\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["# from langdetect import detect\n","import langdetect"],"metadata":{"id":"yiYqxuQ4kyBO","executionInfo":{"status":"ok","timestamp":1649193950292,"user_tz":240,"elapsed":2,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["sent_no = 1\n","\n","sample_str = global_vars.corpus_texts_dt[corpus_texts_ls[text_no]].iloc[sent_no]['text_raw']\n","\n","print(f'Sentence #{sent_no}:\\n\\n    {sample_str}\\n\\n')\n","\n","try:\n","  alang = langdetect.detect(sample_str)\n","  # print(f'LANGUAGE: {detect(sample_str)}')\n","except:\n","  print('ERROR: Unrecognized language')\n","\n","print(f'LANGUAGE: {langdetect.detect(sample_str)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4jzrVzDk6_R","executionInfo":{"status":"ok","timestamp":1649193955553,"user_tz":240,"elapsed":3,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"1aba264d-0cc6-43e0-c641-68ead27ecd2c"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence #1:\n","\n","    The Governing Council first assessed current economic developments in the euro area.\n","\n","\n","LANGUAGE: en\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"BOil2qUAqs3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAjjOEFx7F5J"},"source":["## Save Cleaned Corpus"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CrH24Dv7YwK","outputId":"e71a5d4a-a573-4a12-d947-d94edb72c970","executionInfo":{"status":"ok","timestamp":1649192121501,"user_tz":240,"elapsed":418,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Currently in SentimentArcs root directory:\n","/gdrive/MyDrive/sentimentarcs_notebooks\n","\n","Saving Clean Texts to Subdir: ./text_clean/text_clean_finance_ref\n","\n","Saving these Texts:\n","  dict_keys(['bogfederalreserve_speech_1997-2022', 'eucentralbank_speeches_1998-2022'])\n"]}],"source":["# Verify in SentimentArcs Root Directory\n","os.chdir(Path_to_SentimentArcs)\n","\n","print('Currently in SentimentArcs root directory:')\n","!pwd\n","\n","# Verify Subdir to save Cleaned Texts and Texts into..\n","\n","print(f'\\nSaving Clean Texts to Subdir: {SUBDIR_TEXT_CLEAN}')\n","print(f'\\nSaving these Texts:\\n  {global_vars.corpus_texts_dt.keys()}')"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgcmvfbvXqfy","outputId":"949e3ef1-c8a4-4ac5-e518-7c1d22c83cbb","executionInfo":{"status":"ok","timestamp":1649192312127,"user_tz":240,"elapsed":4553,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving Novel #0 to ./text_clean/text_clean_finance_reference/bogfederalreserve_speech_1997-2022.csv\n","Saving Novel #1 to ./text_clean/text_clean_finance_reference/eucentralbank_speeches_1998-2022.csv\n"]}],"source":["# Save the cleaned Textfiles\n","\n","i = 0\n","for key_novel, anovel_df in global_vars.corpus_texts_dt.items():\n","  anovel_fname = f'{key_novel}.csv'\n","\n","  anovel_fullpath = f'{SUBDIR_TEXT_CLEAN}/{anovel_fname}'\n","  print(f'Saving Novel #{i} to {anovel_fullpath}')\n","  global_vars.corpus_texts_dt[key_novel].to_csv(anovel_fullpath)\n","  i += 1"]},{"cell_type":"code","source":["%whos str\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHn_5eFFhDLf","executionInfo":{"status":"ok","timestamp":1649192225501,"user_tz":240,"elapsed":183,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"7031aab4-6908-4cfa-9f2d-b9aba8e83768"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Variable                Type    Data/Info\n","-----------------------------------------\n","Corpus_Genre            str     finance\n","Corpus_Type             str     reference\n","PATH_TEXT_CLEAN         str     ./text_clean/text_clean_finance_ref\n","PATH_TEXT_RAW           str     ./text_raw/text_raw_finance_ref\n","PATH_UTILS              str     /gdrive/MyDrive/sentimentarcs_notebooks/utils\n","Path_to_SentimentArcs   str     /gdrive/MyDrive/sentimentarcs_notebooks/\n","SUBDIR_TEXT_CLEAN       str     ./text_clean/text_clean_finance_ref\n","SUBDIR_TEXT_RAW         str     text_raw_finance_ref\n","akey                    str     European Central Bank Spe<...>hes (Jul 1998 - Mar 2022)\n","anovel_fname            str     eucentralbank_speeches_1998-2022.csv\n","anovel_fullpath         str     ./text_clean/text_clean_f<...>nk_speeches_1998-2022.csv\n","file_fullpath           str     ./text_raw/text_raw_finan<...>nk_speeches_1998-2022.txt\n","file_root               str     eucentralbank_speeches_1998-2022\n","key_novel               str     eucentralbank_speeches_1998-2022\n","path_text_raw           str     ./text_raw/./text_raw/text_raw_finance_reference/\n","test_str                str     Hilarious 😂. The feeling <...>ly ;) fulfilling orders 😒\n","text                    str     I love python ☮ 🙂 ❤ :-) :-( :-)))\n"]}]},{"cell_type":"code","source":["SUBDIR_TEXT_CLEAN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vvmmN7zvhV6U","executionInfo":{"status":"ok","timestamp":1649192299635,"user_tz":240,"elapsed":224,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"0b7e4efa-184b-4682-eb55-eb55c14518e8"},"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./text_clean/text_clean_finance_reference'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["PATH_TEXT_CLEAN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bWqfKN2uhFkD","executionInfo":{"status":"ok","timestamp":1649192266099,"user_tz":240,"elapsed":151,"user":{"displayName":"Jon Chun","userId":"14430240466678867548"}},"outputId":"ea4cc567-5f5c-4738-998b-06e2f580f8ee"},"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./text_clean/text_clean_finance_reference'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"_348z09gQKe3"},"source":["# **[END OF NOTEBOOK]**"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["o5GqEXyRkPjj","ajD8hCbzkStO","umZfB0YCqajW","ZD8Qe-H12p6j","EA1yTaY_9Qod","7dPPrZwyIIze","pjQBAoLjOzDO","WlfujTpEKhCp","1a-1wyeiGt4Z"],"name":"sentiment_arcs_part1_text_preprocessing.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}